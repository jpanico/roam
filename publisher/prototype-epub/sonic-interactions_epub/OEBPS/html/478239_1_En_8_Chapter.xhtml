<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops"><head><title>Supporting Sonic Interaction in Creative, Shared Virtual Environments</title><meta content="text/html; charset=utf-8" http-equiv="content-type"/><link href="../css/springer_epub.css" rel="styleSheet" type="text/css"/></head><body><div epub:type="chapter" role="doc-chapter"><div class="ChapterContextInformation"><div class="ContextInformation" id="Chap8"><div class="ChapterCopyright">© The Author(s) 2023</div><span class="ContextInformationAuthorEditorNames">M. Geronazzo, S. Serafin<span class="CollaboratorDesignation"> (eds.)</span></span><span class="ContextInformationBookTitles"><span class="BookTitle">Sonic Interactions in Virtual Environments</span></span><span class="ContextInformationSeries"><span class="SeriesTitle" lang="en">Human–Computer Interaction Series</span></span><span class="ChapterDOI"><a href="https://doi.org/10.1007/978-3-031-04021-4_8">https://doi.org/10.1007/978-3-031-04021-4_8</a></span></div></div><!--Begin Abstract--><div class="MainTitleSection"><h1 class="ChapterTitle" lang="en">8. Supporting Sonic Interaction in Creative, Shared Virtual Environments</h1></div><div class="AuthorGroup"><div class="AuthorNames"><span class="Author"><span class="AuthorName">Liang Men</span><sup><a href="#Aff34">1</a> <a aria-label="Contact information for this author" href="#ContactOfAuthor1"><span class="ContactIcon"> </span></a></sup> and </span><span class="Author"><span class="AuthorName">Nick Bryan-Kinns</span><sup><a href="#Aff35">2</a> <a aria-label="Contact information for this author" href="#ContactOfAuthor2"><span class="ContactIcon"> </span></a></sup></span></div><div class="Affiliations"><div class="Affiliation" id="Aff34"><span class="AffiliationNumber">(1)</span><div class="AffiliationText">Liverpool John Moores University, Liverpool, United Kingdom</div></div><div class="Affiliation" id="Aff35"><span class="AffiliationNumber">(2)</span><div class="AffiliationText">Queen Mary University of London, London, United Kingdom</div></div><div class="ClearBoth"> </div></div><div class="Contacts"><div class="Contact" id="ContactOfAuthor1"><div class="ContactIcon"> </div><div class="ContactAuthorLine"><span class="AuthorName">Liang Men</span> (Corresponding author)</div><div class="ContactAdditionalLine"><span class="ContactType">Email: </span><a href="mailto:l.men@ljmu.ac.uk">l.men@ljmu.ac.uk</a></div></div><div class="Contact" id="ContactOfAuthor2"><div class="ContactIcon"> </div><div class="ContactAuthorLine"><span class="AuthorName">Nick Bryan-Kinns</span></div><div class="ContactAdditionalLine"><span class="ContactType">Email: </span><a href="mailto:n.bryan-kinns@qmul.ac.uk">n.bryan-kinns@qmul.ac.uk</a></div></div></div></div><section class="Abstract" id="Abs1" lang="en" role="doc-abstract"><h2 class="Heading">Abstract</h2><p class="Para" id="Par1">This chapter examines user experience design for collaborative music making in shared virtual environments (SVEs). Whilst SVEs have been extensively researched for many application domains including education, entertainment, work and training, there is limited research on the creative aspects. This results in many unanswered design questions such as how to design the user experience without being detrimental to the creative output, and how to design spatial configurations to support both individual creativity and collaboration. Here, we explore multi-modal approaches to supporting creativity in collaborative music making in SVEs. We outline an SVE, LeMo, which allows two people to create music collaboratively. We then present two studies; the first explores how free-form visual 3D annotations instead of spoken communication can support collaborative composition processes and human–human interaction. Five classes of use of annotation were identified in the study, three of which are particularly relevant to the future design of sonic interactions in virtual environments. The second study used a modified version of LeMo to test the support for a creative collaboration of two different spatial audio settings, which according to the results, changed participants’ behaviour and affected their collaboration. Finally, design implications for the auditory design of SVEs focusing on supporting creative collaboration are given.</p></section><!--End Abstract--><div class="Fulltext"><section class="Section1 RenderAsSection1" id="Sec1"><h2 class="Heading"><span class="HeadingNumber">8.1 </span>Introduction</h2><p class="Para" id="Par2">Music has long been produced in social and collaborative ways [<span class="CitationRef"><a epub:type="biblioref" href="#CR16" role="doc-biblioref">16</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR67" role="doc-biblioref">67</a></span>], being inherently multi-modal, music making includes not only the produced sound itself but also other presentations such as body posture[<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>], physical activation of the instrument [<span class="CitationRef"><a epub:type="biblioref" href="#CR7" role="doc-biblioref">7</a></span>], and written symbols and sketches [<span class="CitationRef"><a epub:type="biblioref" href="#CR40" role="doc-biblioref">40</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR66" role="doc-biblioref">66</a></span>] to manage the joint creation and production of music. Many of these modalities such as body position are promoted by the physical proximity of musicians. Immersive virtual environments (VEs) provide a great opportunity to mimic these multi-modal <span id="ITerm1">experiences</span> and to explore radical sonic interaction design spaces for collaborative music making (CMM) [<span class="CitationRef"><a epub:type="biblioref" href="#CR17" role="doc-biblioref">17</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR70" role="doc-biblioref">70</a></span>]<span id="ITerm2"/>, such as telepresence for networked performance and composition. Indeed, whilst many screen-based collaborative systems treat users as outsiders looking in [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>], VEs offer an opportunity to truly immerse people into interactions. Compared to traditional media, VEs may provide a greater sense of community and more intuitive interactions [<span class="CitationRef"><a epub:type="biblioref" href="#CR68" role="doc-biblioref">68</a></span>], and offer new forms of human–computer interaction [<span class="CitationRef"><a epub:type="biblioref" href="#CR36" role="doc-biblioref">36</a></span>] and interpersonal interaction [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>]. Furthermore, VEs have some unique advantages over other media to simulate multi-modal senses and enable people to interact in a natural way that is similar to the real world.</p><p class="Para" id="Par3">However, although VEs have become a hot topic and have been researched in depth and the potential of multi-user immersive virtual reality to promote social activities has been well established (see AlterspaceVR,<sup><a epub:type="noteref" href="#Fn1" id="Fn1_source" role="doc-noteref">1</a></sup> Venues from Oculus<sup><a epub:type="noteref" href="#Fn2" id="Fn2_source" role="doc-noteref">2</a></sup>), little attention is paid to interpersonal interactions in creativity, which includes collaborative sonic interactions, e.g. CMM. This raises many open research questions on how to design user experiences in VEs to support collaborative sonic <span id="ITerm3">interactions</span>, such as CMM. In this chapter we will explore two design features of SVEs, trying to understand their roles in supporting collaborative sonic interaction: i) visual annotation and ii) acoustic attenuation.</p><p class="Para" id="Par6">We will start by reviewing the related work in related areas. Then two studies will be presented, with each exploring one of the two features. Finally, the findings of the two studies will be compared and implications for supporting collaborative sonic interaction in SVEs will be proposed.</p></section><section class="Section1 RenderAsSection1" id="Sec2"><h2 class="Heading"><span class="HeadingNumber">8.2 </span>Shared Virtual Environments</h2><p class="Para" id="Par7">The term VE can be traced back to the early 1990s [<span class="CitationRef"><a epub:type="biblioref" href="#CR12" role="doc-biblioref">12</a></span>], and it emerged as a competitive term to virtual reality (VR). Both are usually equally used to refer to the world created entirely by computer simulation [<span class="CitationRef"><a epub:type="biblioref" href="#CR32" role="doc-biblioref">32</a></span>]. In the mid-1990s, the development of network technology made it possible to connect many users in the same VE, prompting the shared virtual environments (SVEs) [<span class="CitationRef"><a epub:type="biblioref" href="#CR53" role="doc-biblioref">53</a></span>]<span id="ITerm4"/>. In addition to “SVEs”, other similar terms being used include multi-user virtual environments, multi-user virtual reality [<span class="CitationRef"><a epub:type="biblioref" href="#CR18" role="doc-biblioref">18</a></span>], collaborative virtual environments (CVEs) [<span class="CitationRef"><a epub:type="biblioref" href="#CR75" role="doc-biblioref">75</a></span>] and social virtual reality (SVR) [<span class="CitationRef"><a epub:type="biblioref" href="#CR19" role="doc-biblioref">19</a></span>]<span id="ITerm5"/>. To stay consistent, we will herein use the term SVEs to refer to VE systems in which users experience other participants as being mutually present in the same environment and can interact inter-personally [<span class="CitationRef"><a epub:type="biblioref" href="#CR53" role="doc-biblioref">53</a></span>]. Whilst single-person VEs concern how to create detailed (visual) simulations, the design of SVEs usually prioritises enabling collaboration between users [<span class="CitationRef"><a epub:type="biblioref" href="#CR41" role="doc-biblioref">41</a></span>]. By providing a natural medium for three-dimensional collaborative work [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>] and allowing multiple people to interact with each other, SVEs are considered emerging tools for a variety of purposes, including community activities [<span class="CitationRef"><a epub:type="biblioref" href="#CR31" role="doc-biblioref">31</a></span>], online education [<span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>], distributed work and training [<span class="CitationRef"><a epub:type="biblioref" href="#CR42" role="doc-biblioref">42</a></span>], and gaming and entertainment [<span class="CitationRef"><a epub:type="biblioref" href="#CR45" role="doc-biblioref">45</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR47" role="doc-biblioref">47</a></span>]. Despite this, there is little research in the field of supporting collaborative creativity (such as CMMs), which presents the necessity to explore the design space to support the rich forms of interpersonal interaction inherent in CMMs, and leaves many open questions: whether collaborative creativity in SVEs follows a similar pattern with real-world collaborative creativity or not; how to design the virtual environments support creative collaboration is also unclear, see [<span class="CitationRef"><a epub:type="biblioref" href="#CR2" role="doc-biblioref">2</a></span>]. For further discussions on these issues, refer also to Chap. <span class="ExternalRef"><a href="478239_1_En_6_Chapter.xhtml"><span class="RefSource">6</span></a></span>.</p><section class="Section2 RenderAsSection2" id="Sec3"><h3 class="Heading"><span class="HeadingNumber">8.2.1 </span>Embodiment in Collaborative Virtual Environments</h3><p class="Para" id="Par8">Our bodies provide continuous and immediate information about our presence, activity, attention, availability, mood, status, location, identity, capabilities and many other factors to ourselves and others, hence using body language explicitly to facilitate communication is recommended [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>]. Questions have been voiced in regard to embodiment, including the impact of <span id="ITerm6">embodiment</span> on users’ social communication and behaviour [<span class="CitationRef"><a epub:type="biblioref" href="#CR68" role="doc-biblioref">68</a></span>], how the avatars’ appearances and behaviours impact users’ sense of presence [<span class="CitationRef"><a epub:type="biblioref" href="#CR20" role="doc-biblioref">20</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR38" role="doc-biblioref">38</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR57" role="doc-biblioref">57</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>] and co-presence [<span class="CitationRef"><a epub:type="biblioref" href="#CR48" role="doc-biblioref">48</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR59" role="doc-biblioref">59</a></span>]. Research suggests that the embodiment plays an important role in conveying presence, location and identity[<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR4" role="doc-biblioref">4</a></span>], all of which are crucial to the success of collaboration [<span class="CitationRef"><a epub:type="biblioref" href="#CR16" role="doc-biblioref">16</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR21" role="doc-biblioref">21</a></span>]. Social interactions in the real world and in virtual environments are regulated by the same social norms [<span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>]<span id="ITerm7"/>. An appropriate use of embodiment can enhance the sense of telepresence [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>], the sense of social <span id="ITerm8">presence</span> (the feeling that others are present with the user in the mediated environment) [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>] and promote the sense of community [<span class="CitationRef"><a epub:type="biblioref" href="#CR52" role="doc-biblioref">52</a></span>]. Having embodiment is also beneficial to achieve a better sense of co-worker’s locations, actions, intention and construction of workspace awareness, see [<span class="CitationRef"><a epub:type="biblioref" href="#CR24" role="doc-biblioref">24</a></span>]. The embodiment can also create a strong sense of identification, which is essential in collaboration since it is a fundamental component in creating workspace <span id="ITerm9">awareness</span> [<span class="CitationRef"><a epub:type="biblioref" href="#CR24" role="doc-biblioref">24</a></span>], and it can influence collaboration both positively and negatively in group work situations [<span class="CitationRef"><a epub:type="biblioref" href="#CR21" role="doc-biblioref">21</a></span>]. Mutually engaging interactions can be significantly increased with proper awareness of the identity of others[<span class="CitationRef"><a epub:type="biblioref" href="#CR16" role="doc-biblioref">16</a></span>], and in VEs, to a large extent, the identification is shaped by the embodiment. As a result, embodiment decisions are critical and can influence the quality and scope of collaboration in VR [<span class="CitationRef"><a epub:type="biblioref" href="#CR68" role="doc-biblioref">68</a></span>]. The <span id="ITerm10">avatar</span> might be as basic as a T-shape with eyes to indicate orientation and viewing direction, or as sophisticated as a full 3D body scan of the user [<span class="CitationRef"><a epub:type="biblioref" href="#CR58" role="doc-biblioref">58</a></span>].</p></section><section class="Section2 RenderAsSection2" id="Sec4"><h3 class="Heading"><span class="HeadingNumber">8.2.2 </span>Collaborative Music Making</h3><div class="Para" id="Par9">As previously discussed, music making, as a collaborative activity that relies on common goals, understanding and good interpersonal communication, has long been a key form of collaborative creativity (cf. [<span class="CitationRef"><a epub:type="biblioref" href="#CR16" role="doc-biblioref">16</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR67" role="doc-biblioref">67</a></span>]). Although music making tools for multiple users have become more and more popular with the aid of digital technologies, this field remains fairly unexplored [<span class="CitationRef"><a epub:type="biblioref" href="#CR29" role="doc-biblioref">29</a></span>]. In 2003, Blaine and Fels [<span class="CitationRef"><a epub:type="biblioref" href="#CR8" role="doc-biblioref">8</a></span>] explored the design criteria of CMM systems and pointed out the main features including the media used, player interaction, learning curve of systems, physical interfaces and so on. In the same year, inspired by Rodden’s Classification Space for collaborative software [<span class="CitationRef"><a epub:type="biblioref" href="#CR49" role="doc-biblioref">49</a></span>], otherwise known as groupware, Barbosa developed the Networked Music Systems Classification Space [<span class="CitationRef"><a epub:type="biblioref" href="#CR1" role="doc-biblioref">1</a></span>], which classifies CMM systems in terms of the time dimension (synchronous/asynchronous) and space dimension (remote/co-located). Examples based on tangible user <span id="ITerm11">interfaces</span> include reacTable, where multiple users can construct and play the instrument by moving the tangible objects on the table [<span class="CitationRef"><a epub:type="biblioref" href="#CR29" role="doc-biblioref">29</a></span>], and Jam-O-Drum [<span class="CitationRef"><a epub:type="biblioref" href="#CR9" role="doc-biblioref">9</a></span>], which enables participants to join collaborative, musical improvisation. The Music Room provides a room-scale experience, allowing people without music expertise to compose original music inside an interactive space [<span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>]. Sync’n’Move enables two users to explore a multi-channel pre-recorded music piece and users can generate an audio content by synchronising their movements using mobile phones as a collaborative interface. Another phone-based system is Daisyphone [<span class="CitationRef"><a epub:type="biblioref" href="#CR13" role="doc-biblioref">13</a></span>], which provides shared editing of short musical loops. Other examples include BilliArT [<span class="CitationRef"><a epub:type="biblioref" href="#CR11" role="doc-biblioref">11</a></span>], which offers a co-located music-making experience, and Ocarina [<span class="CitationRef"><a epub:type="biblioref" href="#CR69" role="doc-biblioref">69</a></span>], which provides a distributed experience. Though many CMMs have been developed, most of them rely on users to be in a relatively fixed position, e.g. in front of a computer [<span class="CitationRef"><a epub:type="biblioref" href="#CR72" role="doc-biblioref">72</a></span>]. Potentially, the head tracking and spatialised audio provided by VEs can be applied to break this chain and free users. However, this research area is little explored, especially for the collaborative aspect.<figure class="Figure" id="Fig1"><div class="MediaObject" id="MO1"><img alt="" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Fig1_HTML.png" style="width:34.05em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 8.1</span><p class="SimplePara">LeMos enable two players to work together on a music loop in VR </p><div class="Credit"><p class="SimplePara">(reproduced from [<span class="CitationRef"><a epub:type="biblioref" href="#CR36" role="doc-biblioref">36</a></span>] and [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>])</p></div></div></figcaption></figure>
</div></section></section><section class="Section1 RenderAsSection1" id="Sec5"><h2 class="Heading"><span class="HeadingNumber">8.3 </span>LeMo: An SVE Supporting CMM</h2><div class="Para" id="Par10">To build a basis for exploring CMM in SVEs, we created Let’s Move (LeMo<sup><a epub:type="noteref" href="#Fn3" id="Fn3_source" role="doc-noteref">3</a></sup>), which enables two users to manipulate virtual music interfaces together in an SVE to create a music loop, see Fig. <span class="InternalRef"><a href="#Fig1">8.1</a></span>. LeMo was programmed in Unity, and models and textures were made in Cinema 4D and Adobe Photoshop, respectively. The run-time environment includes two HTC Vive headsets (each with one Leap Motion mounted, see Fig. <span class="InternalRef"><a href="#Fig1">8.1</a></span>c) and two PCs connected and synchronised via a LAN cable. LeMo currently has two major versions: LeMo I and LeMo II (together referred to as LeMos). Both LeMos have three key elements:<figure class="Figure" id="Fig2"><div class="MediaObject" id="MO2"><img alt="" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Fig2_HTML.png" style="width:33.95em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 8.2</span><p class="SimplePara">The interfaces of LeMo I and LeMo II</p></div></figcaption></figure>
<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par13">Music interface—For producing music. As shown in Fig. <span class="InternalRef"><a href="#Fig2">8.2</a></span>, the <em class="EmphasisTypeItalic ">matrix</em> interface contains a grid of grids/dots. Each row represents the same pitch, forming an octave from bottom to top, see Fig. <span class="InternalRef"><a href="#Fig2">8.2</a></span>. Users can edit notes by tapping the grids/dots. A vertical play-head repeatedly moves from left to right playing corresponding activated notes. In this way, each interface generates a music loop.</p></li><li><p class="Para" id="Par14">Avatars—Each user has an avatar, including a head and both hands, check Fig. <span class="InternalRef"><a href="#Fig1">8.1</a></span>. Avatars are synchronised with users’ real movements in real time, including position and rotation of heads, as well as gestures. So users can not only see their own embodiment but also their collaborator’s.</p></li><li><p class="Para" id="Par15">A virtual space in which users co-present. LeMos provide visual aids for collaboration by synchronising the virtual environment (virtual space and music interfaces) and avatars across a network, providing participants the sense of being in the same virtual environment and manipulating the same set of interfaces.</p></li></ul></div>
</div><div class="Para" id="Par16">LeMo I and II have three major differences, which are mainly because LeMo II was built later on the basis of LeMo I, and thus provides more and possibly better functionalities. These differences are:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par17">Size of interface matrix of LeMo I is 8*7 while that for LeMo II is 16*8. So participants can create an 8-beat loop in LeMo I and can create 16-beat loops in LeMo II, see Fig. <span class="InternalRef"><a href="#Fig2">8.2</a></span>.</p></li><li><p class="Para" id="Par18">While LeMo I only provides one stationary music interface, LeMo II allows users to generate, remove, position and edit up to eight virtual music interfaces. Music interfaces in LeMo II have two modes: <em class="EmphasisTypeItalic ">sphere</em> and <em class="EmphasisTypeItalic ">matrix</em> (Fig. <span class="InternalRef"><a href="#Fig3">8.3</a></span>b), with <em class="EmphasisTypeItalic ">sphere</em> mainly for storage and positioning, and <em class="EmphasisTypeItalic ">matrix</em> for music editing. Users can generate <em class="EmphasisTypeItalic ">sphere</em>s with pinch and stretch gesture, see Fig. <span class="InternalRef"><a href="#Fig3">8.3</a></span>a. The <em class="EmphasisTypeItalic ">sphere</em> and the <em class="EmphasisTypeItalic ">matrix</em> form can be switched in between using the pop button at the central bottom of the interface, see Fig. <span class="InternalRef"><a href="#Fig3">8.3</a></span>b. Users can have up to eight music interfaces at the same time,<sup><a epub:type="noteref" href="#Fn4" id="Fn4_source" role="doc-noteref">4</a></sup> which means they can have eight music loops at the maximum at the same time.</p></li><li><p class="Para" id="Par20">Compared with LeMo I, LeMo II allows users to control more music features; users can now use sliders to control tempo, volume and pitch, and use “erase” button and “switch” button to erase or switch among four different instruments, including piano, drum, marimba and guitar, see bottom part of Figs. <span class="InternalRef"><a href="#Fig2">8.2</a></span> and  <span class="InternalRef"><a href="#Fig3">8.3</a></span>b.</p></li></ul></div>
<figure class="Figure" id="Fig3"><div class="MediaObject" id="MO3"><img alt="" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Fig3_HTML.png" style="width:33.98em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 8.3</span><p class="SimplePara"><strong class="EmphasisTypeBold ">a</strong> The gesture to generate a new interface; <strong class="EmphasisTypeBold ">b</strong> <em class="EmphasisTypeItalic ">Matrix</em> (opened interface) and <em class="EmphasisTypeItalic ">sphere</em> (packed interface), double-click the pop button to switch in between </p><div class="Credit"><p class="SimplePara">(reproduced from [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>])</p></div></div></figcaption></figure>
</div></section><section class="Section1 RenderAsSection1" id="Sec6"><h2 class="Heading"><span class="HeadingNumber">8.4 </span>Study I—Visual Approach: 3D Annotation</h2><p class="Para" id="Par21">Writing and sketching are often used in collaboration to exchange ideas, acting as a memory aid, conveying approval, ideas, doubts and so on. In the CMM systems Daisyphone and Daisyfield in [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>], people are given a shared annotation mechanism, which enables collaborators to draw lines that are publicly visible. This has been suggested as an advantage to music making. Taking inspiration from this, the goal of this study is to explore how similar visual <span id="ITerm12">cues</span> (e.g. 3D annotations) might impact the creative collaboration when it comes to VR setting. We are interested in exploring how this capability may be used in an SVE to allow collaborative sonic interactions (CMM in this case). <span id="ITerm13"/>
</p><p class="Para" id="Par22">To explore this, LeMo I enables users to draw 3D lines (annotations) by pinching their thumb and index finger together and moving their hands, see left part of Fig. <span class="InternalRef"><a href="#Fig4">8.4</a></span>. These 3D lines are shared and visible to both collaborators, and can therefore potentially be used for communication. To avoid clutter or confusion, users can flip both hands downward to discard all the 3D lines. Users can add or discard lines at any time as they wish.</p><section class="Section2 RenderAsSection2" id="Sec7"><h3 class="Heading"><span class="HeadingNumber">8.4.1 </span>Participants and Procedure</h3><p class="Para" id="Par23">Thirty-two participants (16 pairs) were recruited via group emails at the authors’ university and the authors’ social media for this study.<sup><a epub:type="noteref" href="#Fn5" id="Fn5_source" role="doc-noteref">5</a></sup> Of the participants, 25% had not used VR before, 37.5% of them had tried it only once, nearly a third (28.5%) of them played 2–5 times and nearly 10% played VR frequently. Only two rated themselves as music experts, with the majority rating themselves as novices in musical field. Twelve pairs of participants were familiar with their study partner prior to the study. It took each pair of participants roughly 1 h to finish the experiment, participants received no compensation.</p><div class="Para" id="Par25">After reading and signing informed consent forms, each pair of participants first received a tutorial on how to use LeMo I and then undertook a task-free trial of LeMo I for 5 min, during which they could change music notes and make annotations, helping them get familiar with LeMo. After that, each pair undertook four sessions of composing music, each lasting 5 min. They were asked to create a music loop that sounds nice to them together. Note only two of these sessions were set for this study, in which participants could make annotations. Participants’ annotations were recorded and are highlighted for better readability—see an example in Fig. <span class="InternalRef"><a href="#Fig4">8.4</a></span>. The study ended with a semi-structured interview (around 5 min). Although participants are physically co-located during the experiment, we purposefully did not support nor allow spoken communication. This is because the creative content is in the sound domain and we are interested in how to design systems which foreground the creative uses of sound whilst using complementary modalities to manage the creative process.<figure class="Figure" id="Fig4"><div class="MediaObject" id="MO4"><img alt="" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Fig4_HTML.png" style="width:33.98em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 8.4</span><p class="SimplePara">All annotations in subsequent figures have been emphasised by darkening the background and brightening the annotation lines to enhance their legibility outside of VR (from [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>])</p></div></figcaption></figure>
</div></section><section class="Section2 RenderAsSection2" id="Sec8"><h3 class="Heading"><span class="HeadingNumber">8.4.2 </span>Annotation Categories</h3><div class="Para" id="Par26">Seventy-eight annotations were post-hoc identified and categorised by the researchers according to the annotations for Mutual Engagement classification scheme (referred to as aME classification) in distributed music making: presence, making it happen, quality, social and localisation [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>]. We use aME classification scheme as a starting point for understanding the use of annotations in LeMo I. The following subsections report on the kinds of annotations participants used when making music together in LeMo I, and later sections reflect on these annotations and the utility of the aME classification scheme for SVEs.<figure class="Figure" id="Fig5"><div class="MediaObject" id="MO5"><img alt="" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Fig5_HTML.png" style="width:33.98em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 8.5</span><p class="SimplePara">Presence annotation: <em class="EmphasisTypeItalic ">“XiaoB”</em> (<strong class="EmphasisTypeBold ">a</strong>) and <em class="EmphasisTypeItalic ">“it me”</em> (<strong class="EmphasisTypeBold ">b</strong>), from [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>]</p></div></figcaption></figure>
</div><section class="Section3 RenderAsSection3" id="Sec9"><h4 class="Heading"><span class="HeadingNumber">8.4.2.1 </span>Presence</h4><p class="Para" id="Par27">The concept of <span id="ITerm14">presence</span> has been defined and interpreted in different ways, e.g. [<span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR62" role="doc-biblioref">62</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR63" role="doc-biblioref">63</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR71" role="doc-biblioref">71</a></span>]. Presence is a subjective experience [<span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR61" role="doc-biblioref">61</a></span>] which can greatly affect collaboration [<span class="CitationRef"><a epub:type="biblioref" href="#CR22" role="doc-biblioref">22</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR50" role="doc-biblioref">50</a></span>]—having knowledge of oneself and those we are working with is important in collaboration. An earlier study found many participants in distributed music making used annotations as a way to express and query presence, helping participants know about each other’s existence [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>]. In this study only two users used annotations to convey presence. One wrote “<em class="EmphasisTypeItalic ">XiaoB</em>” (the participant’s name) and the other wrote ‘<em class="EmphasisTypeItalic ">it me</em>” to tell the collaborators their presence and identity, see Fig. <span class="InternalRef"><a href="#Fig5">8.5</a></span>. The reason that much fewer people used annotations to convey presence could be that the avatars provided a sense of presence and identity not available in the original Daisy studies in [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>]. Avatars intuitively show the collaborators where they are, what they are doing and where they are looking. Another reason might be because the collaborators were co-located and that they had previously met in the real world before entering the virtual realm.</p></section><section class="Section3 RenderAsSection3" id="Sec10"><h4 class="Heading"><span class="HeadingNumber">8.4.2.2 </span>Making It Happen</h4><div class="Para" id="Par28">Annotations were also used to support the process of collaborative music <span id="ITerm15">making</span> in four ways explored below:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par29"><em class="EmphasisTypeItalic ">Turn Taking:</em> Although LeMo I allows simultaneous editing of the shared musical loop, at some points participants took turns to contribute the musical notes and used annotations to manage the process. As shown in Fig. <span class="InternalRef"><a href="#Fig6">8.6</a></span>, participants wrote “<em class="EmphasisTypeItalic ">Let me</em>” or “<em class="EmphasisTypeItalic ">you do</em>” to switch who had the active role. By doing so, the active person could either require or give away full control of the music interface until they agree to a turn change—note that there was no explicit ownership control of the musical interface, so in these cases participants were self-managing their access to the shared musical loop.</p></li><li><p class="Para" id="Par30"><em class="EmphasisTypeItalic ">Composition Thoughts:</em> Some annotations emerged that were expressing composition ideas at different levels, covering the highest level—music style, the medium level—patterns formed of notes, and the most specific level- single notes. By drawing lines aligning with possible notes on the grid, Fig. <span class="InternalRef"><a href="#Fig7">8.7</a></span>b, c, d, e sketch out participants’ composition ideas. These are more specific communication compared with annotations revealing musical ideas (e.g. “<em class="EmphasisTypeItalic ">Chinese style</em>?” in Fig. <span class="InternalRef"><a href="#Fig7">8.7</a></span>a). These annotations were usually drawn before activating the corresponding buttons to make and share a plan, possibly so that the partner could help to construct the sequence of notes. Occasionally, these compositional sketches were drawn afterwards (e.g. Fig. <span class="InternalRef"><a href="#Fig7">8.7</a></span>e) and were used to demonstrate a musical idea. In both cases, this kind of annotation may have helped participants to better formulate and understand the collaborative music plan/idea. More directed use of annotations in composition is illustrated in Fig. <span class="InternalRef"><a href="#Fig7">8.7</a></span>f where the participant made three dot markers near the column reference system (B, G and D specifically), asking the partner to make notes in these three columns, which resulted in the partner adding these notes to the shared musical loop. A similar case is shown in 7 h, in which the partner was asked to make notes in rows C, E and G. Participants also directly wrote the reference to ask partners to change specific notes, see Fig. <span class="InternalRef"><a href="#Fig7">8.7</a></span> h, i, j, k.</p></li><li><p class="Para" id="Par31"><em class="EmphasisTypeItalic ">Area and Position Arrangement:</em> Annotations were also used to divide the working area and to manage participants’ work focus in the VE. Fig. <span class="InternalRef"><a href="#Fig8">8.8</a></span>a shows an example in which participants drew a horizontal line to divide the music interface into two parts, each for one participant. The pair was composed within their own working area after the line was drawn, and later on, a word “<em class="EmphasisTypeItalic ">Switch</em>” was written to ask to switch positions (i.e. to swap from top to bottom and vice versa), see Fig. <span class="InternalRef"><a href="#Fig8">8.8</a></span>b. These annotations may have contributed to participants’ working areas and space management.</p></li><li><p class="Para" id="Par32"><em class="EmphasisTypeItalic ">Confusion Expressions:</em> Participants used annotation to write “<em class="EmphasisTypeItalic ">what</em>” or to draw a question mark to presumably express confusion about their partners’ activities given that such annotations were made directly after their partners changed notes, drew, wrote or made gestures. Fig. <span class="InternalRef"><a href="#Fig10">8.10</a></span> illustrates typical indicators of confusion.</p></li></ul></div>
<figure class="Figure" id="Fig6"><div class="MediaObject" id="MO6"><img alt="" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Fig6_HTML.png" style="width:33.98em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 8.6</span><p class="SimplePara">Turn taking annotations: “<em class="EmphasisTypeItalic ">You go ahead</em>” (<strong class="EmphasisTypeBold ">a</strong>); “<em class="EmphasisTypeItalic ">you make</em>” (<strong class="EmphasisTypeBold ">b</strong>); “<em class="EmphasisTypeItalic ">I make</em>” written in Chinese (<strong class="EmphasisTypeBold ">c</strong>); “<em class="EmphasisTypeItalic ">you do</em>” (<strong class="EmphasisTypeBold ">d</strong>) (reproduced from [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>])</p></div></figcaption></figure>
<figure class="Figure" id="Fig7"><div class="MediaObject" id="MO7"><img alt="" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Fig7_HTML.png" style="width:33.95em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 8.7</span><p class="SimplePara">“<em class="EmphasisTypeItalic ">Chinese style</em>?” written in Chinese (<strong class="EmphasisTypeBold ">a</strong>); Patterns formed of notes (<strong class="EmphasisTypeBold ">b</strong>, <strong class="EmphasisTypeBold ">c</strong>, <strong class="EmphasisTypeBold ">d</strong>, <strong class="EmphasisTypeBold ">e</strong>); Note markers (<strong class="EmphasisTypeBold ">f</strong>); References of notes (<strong class="EmphasisTypeBold ">g</strong>, <strong class="EmphasisTypeBold ">h</strong>, <strong class="EmphasisTypeBold ">i</strong>, <strong class="EmphasisTypeBold ">j</strong>, <strong class="EmphasisTypeBold ">k</strong>) (from [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>])</p></div></figcaption></figure>
<figure class="Figure" id="Fig8"><div class="MediaObject" id="MO8"><img alt="" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Fig8_HTML.png" style="width:33.95em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 8.8</span><p class="SimplePara">Annotations for working area arrangement (from [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>])</p></div></figcaption></figure>
<figure class="Figure" id="Fig9"><div class="MediaObject" id="MO9"><img alt="" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Fig9_HTML.png" style="width:33.95em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 8.9</span><p class="SimplePara">Quality Annotations (from [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>])</p></div></figcaption></figure>
</div></section><section class="Section3 RenderAsSection3" id="Sec11"><h4 class="Heading"><span class="HeadingNumber">8.4.2.3 </span>Quality</h4><div class="Para" id="Par33">When creating the music loop, reflecting and exchanging the ideas of the quality of the piece is crucial to smooth the cooperation and ensure a final output with good quality. In LeMo I, participants used annotations to express and exchange their judgments of the quality. These annotations are usually short words or simple shapes, either positive (e.g. “<em class="EmphasisTypeItalic ">OK</em>”, “<em class="EmphasisTypeItalic ">Nice</em>”, “<em class="EmphasisTypeItalic ">Cool</em>”, “<em class="EmphasisTypeItalic ">Good</em>” and heart shape) or negative (e.g. “<em class="EmphasisTypeItalic ">No</em>”), as illustrated in Fig. <span class="InternalRef"><a href="#Fig9">8.9</a></span>. Some of the confusion expressions such as “<em class="EmphasisTypeItalic ">?</em>” were probably indicators of queries of quality, not just queries about the process. It is also interesting to note that positive words may convey different meanings when temporal relationships change. For example, a “<em class="EmphasisTypeItalic ">yes</em>” written shortly after a note addition means the writer’s satisfaction with the addition while an “<em class="EmphasisTypeItalic ">OK</em>” write much later with a certain addition has fewer relation with the addition and means more satisfaction about the whole piece. These emerging annotation-based judgments help collaborators exchange feelings about the piece being made, reduce the idea variation and strengthen the cooperation on the activity.<figure class="Figure" id="Fig10"><div class="MediaObject" id="MO10"><img alt="" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Fig10_HTML.png" style="width:33.98em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 8.10</span><p class="SimplePara">Confusion annotations (from [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>])</p></div></figcaption></figure>
<figure class="Figure" id="Fig11"><div class="MediaObject" id="MO11"><img alt="" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Fig11_HTML.png" style="width:33.95em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 8.11</span><p class="SimplePara">Annotations for social purposes (from [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>])</p></div></figcaption></figure>
</div></section><section class="Section3 RenderAsSection3" id="Sec12"><h4 class="Heading"><span class="HeadingNumber">8.4.2.4 </span>Social</h4><div class="Para" id="Par34">Beyond music making and process management, annotations were also used for non-task-related purposes, as illustrated in Figs. <span class="InternalRef"><a href="#Fig11">8.11</a></span> and <span class="InternalRef"><a href="#Fig12">8.12</a></span>. As shown in Fig. <span class="InternalRef"><a href="#Fig12">8.12</a></span>, one participant started detailed steps of a social drawing activity, their partner then saw this and joined in with the drawing activity and they finished the drawing together. It is interesting to note that in total five human doodles appeared, two of which were drawn collaboratively. The possible reasons for its frequent emergence could be that participants were inspired unknowingly by the kinetic avatar or people just naturally love to draw faces. Although social annotations did not contribute to the music directly, making these lighthearted drawings, as a social interaction, contributes to a close relationship between the collaborators.<figure class="Figure" id="Fig12"><div class="MediaObject" id="MO12"><img alt="" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Fig12_HTML.png" style="width:33.95em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 8.12</span><p class="SimplePara">Annotations for social purposes (reproduced from [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>])</p></div></figcaption></figure>
</div></section><section class="Section3 RenderAsSection3" id="Sec13"><h4 class="Heading"><span class="HeadingNumber">8.4.2.5 </span>Localisation</h4><div class="Para" id="Par35">Bryan-Kinns [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>] identified the frequent use of annotation as a <span id="ITerm16">localisation</span> cue (mainly by drawing arrows), but in LeMo I we only found one similar case, in which the participant drew an arrow, and from the review of the interaction successfully obtained their partner’s attention, as illustrated in Fig. <span class="InternalRef"><a href="#Fig13">8.13</a></span>. However, in this case the arrow may have been more to attract attention to the activity rather than to highlight a specific part of the joint creation. The reason that annotations are not used for localisation in LeMo I could be that participants could simply draw each other’s attention to a certain location by waving their hands and then pointing to that location.<figure class="Figure" id="Fig13"><div class="MediaObject" id="MO13"><img alt="" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Fig13_HTML.png" style="width:33.95em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 8.13</span><p class="SimplePara">A participant drew an arrow (<strong class="EmphasisTypeBold ">a</strong>), and this successfully drew their partner’s attention to the intended area (<strong class="EmphasisTypeBold ">b</strong>) (from [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>])</p></div></figcaption></figure>
</div></section></section><section class="Section2 RenderAsSection2" id="Sec14"><h3 class="Heading"><span class="HeadingNumber">8.4.3 </span>Interviews</h3><p class="Para" id="Par36">Post-task interviews with participants revealed more reflective insights into the use of the annotations. The interviews were transcribed (around 5,000 words) and a thematic analysis was undertaken, see more information about thematic analysis in [<span class="CitationRef"><a epub:type="biblioref" href="#CR10" role="doc-biblioref">10</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR74" role="doc-biblioref">74</a></span>]. The thematic analysis started with a reading through of the transcript, then an inductive analysis of the data was performed, and relevant patterns were collapsed into codes. Next, these codes were combined into overarching themes, which were then reviewed and adjusted until they were appropriate for the codes. In total, 41 codes and 4 overarching themes emerged from the thematic analysis. Two themes were directly related to annotation: (i) annotation’s usefulness, and (ii) annotation’s problems.</p><p class="Para" id="Par37">Many participants described that they had a positive feeling when they could write something to support their <span id="ITerm17">communication</span>. They reported annotations were used to make “signs and symbols” to support composition, or to “create drawing together [...] like a physical warm up”. Participants also reported that annotations exceeded vocal communication in some ways, “with the lines, [they] could just circle the notes to say that was [note] G and go back to [note] C, from that perspective, drawing was more effective”. Many participants reported that they successfully understood each other’s intentions via the annotations, e.g. one participant drew a line and “used the line to affect the partner”, guiding their partner to move notes to lower positions, the partner fully understood and reported they “did the changes”. Other examples mentioned are showing satisfaction by “writing an OK” or using “Hi” for greetings.</p><p class="Para" id="Par38">Meanwhile, writing and reading in 3D space were reported by participants to be quite different from the real world and these differences caused inconveniences and problems. For instance, the 3D nature of the annotations reduced their readability, it only “makes sense to [them] from [their] perspective[s], because it was 3D”. For ease of identifying the annotations, “[they] need to stand where the person wrote it stood”. Furthermore, making annotations was reported as being time-consuming, and “when [they] finish[ed] it, it [did] not make sense” anymore. Also, the low accuracy of movement tracking led to annotations being drawn at quite a large size, which then led to a limitation of “how much [they] [could] write”. Finally, participants reported that it was hard to notice each others’ annotation activities, a participant “waved hands to [their partner], but [the partner] did not see”, the participant “had to wave hands [closer], directly in front of [the partner]” to draw their attention to the annotations so as to get the annotations read. This was probably due to the narrower field of view (FOV) in VR <em class="EmphasisTypeItalic ">vs</em> real life as the FOV is about 100 horizontal degrees with HTC Vive <em class="EmphasisTypeItalic ">vs</em> about 200 degrees binocular FOV in real life, see [<span class="CitationRef"><a epub:type="biblioref" href="#CR28" role="doc-biblioref">28</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>].</p></section><section class="Section2 RenderAsSection2" id="Sec15"><h3 class="Heading"><span class="HeadingNumber">8.4.4 </span>Reflection of Study I</h3><p class="Para" id="Par39">Similar to Bryan-Kinns’ findings [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>], most of the annotations that emerged in the use of LeMo I fall into three types: making it happen, quality and social. However, unlike the aME classification, presence and localisation appear to be well managed through avatar interaction. This similarity suggests that 3D annotations can function similarly in an immersive collaborative music-making system as they can in a 2D non-immersive CMM system. However, much fewer annotations are used to convey presence compared with the findings of Bryan-Kinns [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>] which may be because avatars already support this well, or it may also have been due to the physical collocation of participants with LeMo I compared to the Daisy* studies which were distributed remotely. The length of the musical loop in LeMo I is 8 beats, whereas in the Daisy* studies the length was 48 beats which may have affected the kinds of annotation produced as the LeMo I loop was simpler and required less temporal organisation. Regardless of these issues, the use of aME to classify annotations in a study of CMM indicates that the annotation classification scheme applies to media beyond the Daisy* systems it was previously used to evaluate [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>].</p><p class="Para" id="Par40">For sonic interaction design of VEs, the findings of this exploratory study indicate that 3D graphical <span id="ITerm18">annotations</span> of a virtual environment can support a music making as a tool for communication where the co-produced sound is prioritised over other modalities—CMM in our case. We specifically prevented conversation during the creative process to allow us to explore how to support collaboration without interrupting or interfering with the music being created by collaborators. The step sequencer used in LeMo I was intentionally simple to allow initial exploration of the role of annotations without conflating this with the complexity of an interface. For richer and more complex sonic creation and exploration in VR, we suggest that annotations could usefully support communication about the process, quality and also social aspects of interaction without compromising the joint product being produced. It may facilitate a foregrounding of the creative sound product to such an extent that the sounds created are able to use the full width of the sound domain at the exclusion of all other parts of the human–human interaction necessary for collaboration.</p><p class="Para" id="Par41">Whilst the annotations of LeMo I supported co-creation of music, they did generate some issues. More specifically, making annotations and viewing them were reported to be very different from real life, daily experiences. Participants needed to get used to controlling strokes by pinching and releasing fingers. Besides, compared with writing or drawing with a real pen, the LeMo I has a less accuracy in supporting these. To increase the readability of written contents and sketches, participants tended to write or draw in a bigger size, which resulted in a limitation of how much they could write/draw. But on the positive side, the larger size made it possible to write and draw together, which expanded the range of annotating action, making it less personal but more social-friendly and more accommodating to multiple people. Another unexpected problem found in this study was that 3D annotations can, of course, be viewed from many angles, so written text is often reversed for a participant’s collaborator, especially if they write in the space between themselves. This clearly decreases the readability of the annotations. Some participants wrote in reverse to try to compensate for this issue, see an example shown in Fig. <span class="InternalRef"><a href="#Fig9">8.9</a></span>h and i. Future development of the use of annotations in VR would need to explore how this mirroring issue could be addressed.</p></section></section><section class="Section1 RenderAsSection1" id="Sec16"><h2 class="Heading"><span class="HeadingNumber">8.5 </span>Study II—Audio Approach: Augmented Acoustic Attenuation</h2><p class="Para" id="Par42">Sound attenuates as a result of diminishing intensity when travelling through a medium. Acoustic attenuation is one of the primary cues for sound localisation of distance; it enables humans to use their innate spatial abilities to retrieve and localise information and to aid performance, see [<span class="CitationRef"><a epub:type="biblioref" href="#CR5" role="doc-biblioref">5</a></span>]<span id="ITerm19"/>. Whilst augmenting the acoustic attenuation of a real medium (e.g. the air) is difficult, this can be easily done in VEs with the aid of audio simulation (refer to Chap. <span class="ExternalRef"><a href="478239_1_En_3_Chapter.xhtml"><span class="RefSource">3</span></a></span> for modularity in the auralisation)<span id="ITerm20"/>. Research has begun to investigate the impacts of spatialised sounds on user experience in VR, see [<span class="CitationRef"><a epub:type="biblioref" href="#CR27" role="doc-biblioref">27</a></span>]. However, little research explores how the spatialisation of sound may affect or aid collaboration in a VR context. Considering sound is both the primary medium and the final output of the creative task [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>], by affecting sound, different settings of acoustic attenuation can possibly affect the collaboration differently. With the ability to modify the simulated acoustic attenuation in an immersive virtual environment, we can possibly create sonic privacy by augmenting acoustic attenuation, and then use sonic privacy as personal space to support individual creativity in <span id="ITerm21">CMM</span>. Supporting individual creativity is important as it contributes to the group creativity [<span class="CitationRef"><a epub:type="biblioref" href="#CR37" role="doc-biblioref">37</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR44" role="doc-biblioref">44</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR46" role="doc-biblioref">46</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR60" role="doc-biblioref">60</a></span>].</p><section class="Section2 RenderAsSection2" id="Sec17"><h3 class="Heading"><span class="HeadingNumber">8.5.1 </span>Hypotheses</h3><p class="Para" id="Par43">Research has suggested users should be allowed to work individually in their personal spaces at their own pace, cooperatively work together in the shared space and smoothly transition between both of the spaces during collaboration [<span class="CitationRef"><a epub:type="biblioref" href="#CR23" role="doc-biblioref">23</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR56" role="doc-biblioref">56</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR65" role="doc-biblioref">65</a></span>]. <span id="ITerm22"> In</span> a previous study [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>], following this implication, we built three different spatial configurations (public space only, public space + publicly visible personal space, public space + publicly invisible personal space), and tested different impacts of these spatial configurations on collaborative music making in SVEs. The results show adding personal space to be helpful in supporting collaborative music making in SVE, since it provides a chance to explore individual ideas, and provides higher efficiency in making notes. However, several negative impacts also showed up along with the addition of personal space, e.g. longer average distance between participants, reduced group territory and group edits [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>]. We believe this might due to: (i) the separated stationary locations of the personal spaces forced users to leave each other to access, causing a longer distance between participants and less collaboration; (ii) the rigid boundary between public space and personal space made users more isolated, resulting in a higher sense of isolation. Thus allowing users to access personal space without leaving each other far away might eliminate these disadvantages.</p><p class="Para" id="Par44">To make the shift between personal and public spaces more fluid, inspired by the implication that the separation between public and personal workspace should be gradual rather than too rigid [<span class="CitationRef"><a epub:type="biblioref" href="#CR23" role="doc-biblioref">23</a></span>], the attenuation feature can possibly be applied to form a gradual personal space, enabling a fluid transition between personal space and public space. This is because sound is both the primary medium of collaborative tasks and the final work of CMM [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>], thus by manipulating acoustic attenuation, we can produce sonic privacy. Thus H1 was developed.</p><p class="Para" id="Par45"><strong class="EmphasisTypeBold ">H1</strong>: Attenuation can play a similar role to personal space with rigid form in CMM in SVE, providing collaborators a personal space and supporting individual creativity during the collaboration.</p><p class="Para" id="Par46">Additionally, an acoustic attenuation, rather than a personal space with rigid separation from public space, enables a gradual shift between personal and public workspace, which may possibly increase the fluidity of the experience and support collaboration better, cf. [<span class="CitationRef"><a epub:type="biblioref" href="#CR23" role="doc-biblioref">23</a></span>]. Thus we developed H2.</p><p class="Para" id="Par47"><strong class="EmphasisTypeBold ">H2</strong>: Acoustic attenuation provides a fluid transition (no hard borders nor rigid forms) between personal and public spaces, which introduces less negative impacts on collaboration compared with personal space with rigid form in [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>].</p></section><section class="Section2 RenderAsSection2" id="Sec18"><h3 class="Heading"><span class="HeadingNumber">8.5.2 </span>Independent Variable</h3><div class="Para" id="Par48">Spatial configuration is an independent variable in this experiment. Two spatial configurations were designed as the independent variable levels, as shown in Fig. <span class="InternalRef"><a href="#Fig14">8.14</a></span>, including the following:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par49"><strong class="EmphasisTypeBold ">C</strong>ondition 1: <strong class="EmphasisTypeBold ">Pub</strong>lic space only (referred to as <span class="InlineEquation" id="IEq1"><img alt="$$\mathrm {{\textbf {C}}}_\mathrm {{\textbf {pub}}}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq1.png" style="width:2.37em"/></span>): where players can generate, remove or manipulate music interfaces, and have equal access to all of the space and the music interfaces. As no personal space is provided, a shift between public and personal space does not exist, i.e. users cannot shift to personal space.</p></li><li><p class="Para" id="Par50"><strong class="EmphasisTypeBold ">C</strong>ondition 2: Public space + <strong class="EmphasisTypeBold ">Aug</strong>mented Attenuation Personal Space (referred to as <span class="InlineEquation" id="IEq2"><img alt="$$\mathrm {{\textbf {C}}}_\mathrm {{\textbf {aug}}}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq2.png" style="width:2.25em"/></span>). In addition to <span class="InlineEquation" id="IEq3"><img alt="$$\mathrm {{\textbf {C}}}_\mathrm {{\textbf {pub}}}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq3.png" style="width:2.37em"/></span>), the sound attenuation is augmented. The volume of audio drops much faster, creating a sonic privacy, which can be seen as a personal space. As the volume changes gradually with the changes of distance, the shift between personal space and public space is gradual.</p></li></ul></div>
<figure class="Figure" id="Fig14"><div class="MediaObject" id="MO14"><img alt="" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Fig14_HTML.png" style="width:24.17em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 8.14</span><p class="SimplePara">Top view of the two experimental condition settings</p></div></figcaption></figure>
</div></section><section class="Section2 RenderAsSection2" id="Sec19"><h3 class="Heading"><span class="HeadingNumber">8.5.3 </span>Dependent Variables</h3><div class="Para" id="Par51">To identify how users use the space and the effect of adding augmented sound attenuation as personal space, dependent variables were developed. The Igroup Presence Questionnaire (IPQ) was used to inform the design of questions about sense of collaborator’s presence [<span class="CitationRef"><a epub:type="biblioref" href="#CR54" role="doc-biblioref">54</a></span>]. The IPQ measures the sense of presence using one general measurement—sense of being there, plus three sub-scales covering spatial presence, involvement and experience realism. Questions about output quality, communication and contribution were adapted from the Mutual Engagement Questionnaire (MEQ) [<span class="CitationRef"><a epub:type="biblioref" href="#CR15" role="doc-biblioref">15</a></span>]. The MEQ is formed of two parts: (i) participant ratings of the quality of the musical outcome and their interaction with musical interface; (ii) participant choices between different conditions when being provided a series of statements covering the music quality, enjoyment, involvement and frustration. The rest of the questions were designed to question people’s preference for conditions. The questionnaire included measures on:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par52"><em class="EmphasisTypeItalic ">Presence</em>: (i) Sense of self-presence, (ii) sense of co-worker’s <span id="ITerm23">presence</span> and (iii) sense of collaborator’s activities.</p></li><li><p class="Para" id="Par53"><em class="EmphasisTypeItalic ">Communication</em>: quality of communication, which may vary as the visibility of spaces can possibly affect the embodiment and nonverbal communication.</p></li><li><p class="Para" id="Par54"><em class="EmphasisTypeItalic ">Content assessment</em>: the satisfaction of the final music created reflects the quality of collaboration, cf. [<span class="CitationRef"><a epub:type="biblioref" href="#CR15" role="doc-biblioref">15</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR16" role="doc-biblioref">16</a></span>].</p></li><li><p class="Para" id="Par55"><em class="EmphasisTypeItalic ">Preference</em>: preference of the conditions, to see if users have subjective preferences towards the settings.</p></li><li><p class="Para" id="Par56"><em class="EmphasisTypeItalic ">Contribution</em>: (i) the feeling of self’s contribution; (ii) the feeling of others’ contribution.</p></li></ul></div>
</div><div class="Para" id="Par57">These measures are grouped into a Post-Session Questionnaire (PSQ, see items in Table <span class="InternalRef"><a href="#Tab1">8.1</a></span>).<div class="Table" id="Tab1"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 8.1</span><p class="SimplePara">Results of Post-Session Questionnaire and the results of Wilcoxon Rank-Sum Test (two-tailed)<span class="InlineEquation" id="IEq4"><img alt="$$^\textrm{a}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq4.png" style="width:0.5em"/></span></p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1 align-left"/><col class="tcol2 align-left"/><col class="tcol3 align-left"/><col class="tcol4 align-left"/><col class="tcol5 align-left"/><col class="tcol6 align-left"/><col class="tcol7 align-left"/></colgroup><thead><tr><th rowspan="2" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Questions (Measure)</p></th><th colspan="2" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InlineEquation" id="IEq5"><img alt="$$\textrm{C}_\textrm{pub}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq5.png" style="width:2.07em"/></span></p></th><th colspan="2" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InlineEquation" id="IEq6"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq6.png" style="width:2em"/></span></p></th><th colspan="2" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><span class="InlineEquation" id="IEq7"><img alt="$$\textrm{C}_\textrm{pub}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq7.png" style="width:2.07em"/></span> <em class="EmphasisTypeItalic ">vs</em> <span class="InlineEquation" id="IEq8"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq8.png" style="width:2em"/></span></p></th></tr><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">   M  </em></p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">   SD  </em></p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">   M  </em></p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">   SD  </em></p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">   p  </em></p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">   W  </em></p></th></tr></thead><tbody><tr><td colspan="7" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PSQ1 (support for creativity)—I think the space setting in this session was extremely helpful</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">      for creativity</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">8.55</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1.44</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">8.77</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1.34</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.5695</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">259</p></td></tr><tr><td colspan="7" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PSQ2 (support for creativity)—I feel like the space setting in this session was extremely helpful</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">      to support the development of my own ideas</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">7.82</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1.92</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">8.35</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1.50</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.5211</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">255.5</p></td></tr><tr><td colspan="7" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PSQ3 (preference)—I enjoyed the space setting of this virtual world very much</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">8.27</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1.61</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">8.65</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1.60</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.2622</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">233</p></td></tr><tr><td colspan="7" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PSQ4 (sense of collaborator’s presence)—I always had strong feeling that my collaborator was there,</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">      collaborating with me together, all the time</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">8.91</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.92</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">8.54</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1.68</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.7961</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">298.5</p></td></tr><tr><td colspan="7" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PSQ5 (content assessments)—How satisfied are you with the final piece of loop music you two</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">       created in this session</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">8.64</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1.09</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">8.50</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1.36</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.7644</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">300.5</p></td></tr><tr><td colspan="7" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PSQ6 (communication quality)—How would you rate the quality of communication between</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">       you and your collaborator during the session</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">8.68</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1.09</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">8.50</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1.36</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.7644</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">300.5</p></td></tr><tr><td colspan="7" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PSQ7 (sense of collaborator’s activity)—I had a clear sense what my collaborator was doing</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">8.73</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1.20</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">7.96</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1.54</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.08094</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">368.5</p></td></tr><tr><td colspan="7" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PSQ8 (amount of contribution)—The amount of your contribution to the joint</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">       piece of music is</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">8.41</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1.44</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">8.15</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1.46</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.4776</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">320</p></td></tr><tr><td colspan="7" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PSQ9 (amount of contribution)—The amount of your collaborator’s contribution to the joint</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">       piece of music is</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">8.18</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1.26</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">8.23</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1.39</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.8486</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">276.5</p></td></tr><tr><td colspan="7" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PSQ10 (quality of contribution)—What do you think of the quality of your contribution to the</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">       joint piece of music is</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">8.05</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1.70</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">7.81</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1.41</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.319</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">333.5</p></td></tr><tr><td colspan="7" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">PSQ11 (quality of contribution)—What do you think of the quality of your collaborator’s</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">       contribution to the joint piece of music is</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">7.73</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">1.52</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">8.19</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">1.20</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.3496</p></td><td style="text-align: left;"><p class="SimplePara">241.5</p></td></tr></tbody></table><div class="TableFooter"><p class="SimplePara"><span class="InlineEquation" id="IEq9"><img alt="$$^\textrm{a}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq9.png" style="width:0.5em"/></span> Note: statistics in this table are calculated based on the data collected from third and fourth session to better counterbalance the learning effect</p></div></div>
</div></section><section class="Section2 RenderAsSection2" id="Sec20"><h3 class="Heading"><span class="HeadingNumber">8.5.4 </span>Participants and Procedure</h3><p class="Para" id="Par58">Fifty-two participants (26 pairs) were recruited through group emails at the authors’ university for this study.<sup><a epub:type="noteref" href="#Fn6" id="Fn6_source" role="doc-noteref">6</a></sup> Each participant was compensated 10 GBP for their time (roughly 1 h). Participants’ rating of musical theory knowledge is 3.92 (SD = 2.50) on a 10-point Likert scale, where higher values indicate increased knowledge; 24 participants play one or more instruments, and the remaining 28 do not. Twenty participants had tried VR 2–5 times before, 20 had only tried once and the remaining 12 had no VR experience previously. Thirty-seven participants knew their collaborators very well prior to the experiment; three met their collaborators several times, and the remaining 12 did not know their collaborators at all prior to the experiment.</p><p class="Para" id="Par60">The experiment started with participants reading the information form and signing the consent form. Then they first received an explanation of the music interface of LeMo II (see Fig. <span class="InternalRef"><a href="#Fig2">8.2</a></span>), with all of the interaction gestures supported in LeMo II demonstrated by an experimenter. Next, a trial (roughly 5–15 min) session was carried out, where participants could try all of the possible interactions. The trial ended once participants were confident enough of all available interactions. The length of time of the tutorial session was flexible to ensure participants with diverse musical knowledge could grasp LeMo II. Participants were then asked to have four sessions of collaboratively composing music that was mutually satisfying and compliments an animation loop. Two of these sessions were set for this study; each covered a condition (<span class="InlineEquation" id="IEq10"><img alt="$$\textrm{C}_\textrm{pub}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq10.png" style="width:2.07em"/></span>/<span class="InlineEquation" id="IEq11"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq11.png" style="width:2em"/></span>), and the sequence of conditions was fully randomised to counterbalance the learning effect. We set each session as 7 min because based on our pilot study and a previous study [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>], we found 7–8 min were sufficient for the task. In total, four visual, silent animation loops were introduced to trigger participants’ creativity; each to be played in one experimental session on four virtual screens surrounding the virtual stage. These clips were played in an independently randomised sequence to counterbalance impacts on the study. Each session ended with a Post-Session Questionnaire (PSQ, see Table <span class="InternalRef"><a href="#Tab1">8.1</a></span>). After all the four sessions finished, a short interview was carried out.</p></section><section class="Section2 RenderAsSection2" id="Sec21"><h3 class="Heading"><span class="HeadingNumber">8.5.5 </span>Results</h3><div class="Para" id="Par61">Wilcoxon Rank-Sum tests were run to compare the ratings of <span class="InlineEquation" id="IEq12"><img alt="$$\textrm{C}_\textrm{pub}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq12.png" style="width:2.07em"/></span> with <span class="InlineEquation" id="IEq13"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq13.png" style="width:2em"/></span> collected by PSQ, see results in Table <span class="InternalRef"><a href="#Tab1">8.1</a></span>. No significant effect was found between <span class="InlineEquation" id="IEq14"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq14.png" style="width:2em"/></span> and <span class="InlineEquation" id="IEq15"><img alt="$$\textrm{C}_\textrm{pub}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq15.png" style="width:2.07em"/></span>. Post-task interviews revealed more reflective insights. Around 41,000 words of audio recorded interview responses were transcribed and a thematic analysis of the transcription was undertaken (more details about the thematic analysis in Sect. <span class="InternalRef"><a href="#Sec14">8.4.3</a></span>). As shown in Fig. <span class="InternalRef"><a href="#Fig15">8.15</a></span>, in total, 439 coded segments, 15 codes and 3 overarching themes emerged from the thematic analysis: (i) learning effects; (ii) preferences, advantages and disadvantages of conditions; and (iii) advantages, disadvantages of LeMo II and suggestions for improvements. Next, we will only cover the former two themes as the final one is not directly related to the scope of this chapter.<figure class="Figure" id="Fig15"><div class="MediaObject" id="MO15"><img alt="" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Fig15_HTML.png" style="width:34.18em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 8.15</span><p class="SimplePara">Ingredients of all the coded segments of the interview; number of coded segments are shown in the bars</p></div></figcaption></figure>
</div><section class="Section3 RenderAsSection3" id="Sec22"><h4 class="Heading"><span class="HeadingNumber">8.5.5.1 </span>Learning Effects</h4><p class="Para" id="Par62">Members of 18 groups mentioned the effect of the session sequence. Specifically, 43 coded segments contributed by 27 participants were related to learning effects. For example, Participant 15A (participant A in group 15, referred to as <span class="InlineEquation" id="IEq16"><img alt="$$\textrm{P}_\textrm{15A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq16.png" style="width:2.07em"/></span>) reported the sequence is an “important factor”. The first session was felt to be hard as they were “just being introduced to [the system and they were] still adjusting” to it (<span class="InlineEquation" id="IEq17"><img alt="$$\textrm{P}_\textrm{5A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq17.png" style="width:1.69em"/></span>), trying to “[figure] out how the system was working” (<span class="InlineEquation" id="IEq18"><img alt="$$\textrm{P}_\textrm{16A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq18.png" style="width:2.07em"/></span>), as they “were progressing into latter sessions, [they] felt easier to communicate and use gestures to manipulate the sound, being able to collaborate more, more used to the system” (<span class="InlineEquation" id="IEq19"><img alt="$$\textrm{P}_\textrm{5B}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq19.png" style="width:1.69em"/></span>), these changes led to a higher level of satisfaction and more enjoyment in later conditions. To better counterbalance the impact of sequence, Table <span class="InternalRef"><a href="#Tab1">8.1</a></span> only includes data collected from the latter two sessions (note: as aforementioned, there were four sessions that were randomly sequenced, and two of which were related to this study).</p></section><section class="Section3 RenderAsSection3" id="Sec23"><h4 class="Heading"><span class="HeadingNumber">8.5.5.2 </span><span class="InlineEquation" id="IEq20"><img alt="$$\mathrm {{\textbf {C}}}_\mathrm {{\textbf {pub}}}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq20.png" style="width:2.37em"/></span>—Simple but can be Chaotic</h4><p class="Para" id="Par63">With no personal space, participants had to hear all the interfaces throughout the session. In total, 16 coded segments are about the disadvantages of <span class="InlineEquation" id="IEq21"><img alt="$$\textrm{C}_\textrm{pub}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq21.png" style="width:2.07em"/></span>; some exemplars are: “a bit troubling”—<span class="InlineEquation" id="IEq22"><img alt="$$\textrm{P}_\textrm{11B}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq22.png" style="width:2.07em"/></span>,<sup><a epub:type="noteref" href="#Fn7" id="Fn7_source" role="doc-noteref">7</a></sup> “music always very loud”—<span class="InlineEquation" id="IEq25"><img alt="$$\textrm{P}_\textrm{9A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq25.png" style="width:1.69em"/></span>, “it was global music, and there was someone annoying”—<span class="InlineEquation" id="IEq26"><img alt="$$\textrm{P}_\textrm{2A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq26.png" style="width:1.69em"/></span>, “you are not going to say anything” to avoid being “rude”—(<span class="InlineEquation" id="IEq27"><img alt="$$\textrm{P}_\textrm{2A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq27.png" style="width:1.69em"/></span>). It was easier if there is something helpful “to perceive what I was doing, and not get confused with what [the collaborator] was doing” (<span class="InlineEquation" id="IEq28"><img alt="$$\textrm{P}_\textrm{15B}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq28.png" style="width:2.07em"/></span>), it was too “chaotic” (<span class="InlineEquation" id="IEq29"><img alt="$$\textrm{P}_\textrm{20A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq29.png" style="width:2.07em"/></span>), “too confusing” (<span class="InlineEquation" id="IEq30"><img alt="$$\textrm{P}_\textrm{22A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq30.png" style="width:2.07em"/></span> and <span class="InlineEquation" id="IEq31"><img alt="$$\textrm{P}_\textrm{22B}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq31.png" style="width:2.07em"/></span>), “annoying” (<span class="InlineEquation" id="IEq32"><img alt="$$\textrm{P}_\textrm{25B}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq32.png" style="width:2.07em"/></span>). They “can not concentrate” (<span class="InlineEquation" id="IEq33"><img alt="$$\textrm{P}_\textrm{25B}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq33.png" style="width:2.07em"/></span>) while “everything [is] open and quite noisy” (<span class="InlineEquation" id="IEq34"><img alt="$$\textrm{P}_\textrm{26B}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq34.png" style="width:2.07em"/></span>), and they “don’t have the tranquillity to operating [their] sounds or the everything’s come mixed, which is difficult to manage” (<span class="InlineEquation" id="IEq35"><img alt="$$\textrm{P}_\textrm{22A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq35.png" style="width:2.07em"/></span>).</p><p class="Para" id="Par65">There were 25 coded segments from 14 participants reporting the positive side of the <span class="InlineEquation" id="IEq36"><img alt="$$\textrm{C}_\textrm{pub}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq36.png" style="width:2.07em"/></span>; some examples are: (i) pieces created in “personal space” might clash in a musical way (<span class="InlineEquation" id="IEq37"><img alt="$$\textrm{P}_\textrm{1A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq37.png" style="width:1.69em"/></span>), “better to work when knowing how it sounds all together” (<span class="InlineEquation" id="IEq38"><img alt="$$\textrm{P}_\textrm{17B}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq38.png" style="width:2.07em"/></span>), music pieces might match better; (ii) better for providing help to the other collaborator, as reported by P4A, saying that they needed someone to lead them and thus the ability to hear all the work all the time was helpful; (iii) “space wise”, compared with having to work closer to “hear the sound well” (<span class="InlineEquation" id="IEq39"><img alt="$$\textrm{P}_\textrm{12A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq39.png" style="width:2.07em"/></span>) in <span class="InlineEquation" id="IEq40"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq40.png" style="width:2em"/></span>, <span class="InlineEquation" id="IEq41"><img alt="$$\textrm{C}_\textrm{pub}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq41.png" style="width:2.07em"/></span> does not have this space constraint, they could chose to work “anywhere” (<span class="InlineEquation" id="IEq42"><img alt="$$\textrm{P}_\textrm{24A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq42.png" style="width:2.07em"/></span>); iv) “easier” to understand the condition (<span class="InlineEquation" id="IEq43"><img alt="$$\textrm{P}_\textrm{6B}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq43.png" style="width:1.69em"/></span>), fewer confusions when simply being able to hear all the things all the time (<span class="InlineEquation" id="IEq44"><img alt="$$\textrm{P}_\textrm{13A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq44.png" style="width:2.07em"/></span>); v) “collaborative wise” (<span class="InlineEquation" id="IEq45"><img alt="$$\textrm{P}_\textrm{13A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq45.png" style="width:2.07em"/></span>), less separation, better collaboration compared with “personal space” was provided (<span class="InlineEquation" id="IEq46"><img alt="$$\textrm{P}_\textrm{3B}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq46.png" style="width:1.69em"/></span>, <span class="InlineEquation" id="IEq47"><img alt="$$\textrm{P}_\textrm{18A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq47.png" style="width:2.07em"/></span> and <span class="InlineEquation" id="IEq48"><img alt="$$\textrm{P}_\textrm{18B}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq48.png" style="width:2.07em"/></span>).</p></section><section class="Section3 RenderAsSection3" id="Sec24"><h4 class="Heading"><span class="HeadingNumber">8.5.5.3 </span>Preference on <span class="InlineEquation" id="IEq49"><img alt="$$\mathrm {{\textbf {C}}}_\mathrm {{\textbf {aug}}}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq49.png" style="width:2.25em"/></span></h4><div class="Para" id="Par66">There were 35 coded segments contributed by 24 participants favouring condition <span class="InlineEquation" id="IEq50"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq50.png" style="width:2em"/></span>, higher than 12 segments contributed by 11 participants for <span class="InlineEquation" id="IEq51"><img alt="$$\textrm{C}_\textrm{pub}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq51.png" style="width:2.07em"/></span>. There are 111 coded segments contributed by 33 participants from 25 groups reporting the advantages of this condition, much higher than the number of segments reporting other conditions’ advantages. These reports reveal some insights behind the popularity of <span class="InlineEquation" id="IEq52"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq52.png" style="width:2em"/></span>. <span class="InlineEquation" id="IEq53"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq53.png" style="width:2em"/></span>’s advantages reported by participants can be grouped into 4 groups:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par67"><em class="EmphasisTypeItalic ">Higher team cohesiveness and lower sense of separation.</em> Participants reported that, without the rigid personal space, they had to “work with the other person” (<span class="InlineEquation" id="IEq54"><img alt="$$\textrm{P}_\textrm{6A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq54.png" style="width:1.69em"/></span>). With no rigid personal space, <span class="InlineEquation" id="IEq55"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq55.png" style="width:2em"/></span>’s “forces [them] to collaborate more the most because [they] had to stay very close to compose music ” (<span class="InlineEquation" id="IEq56"><img alt="$$\textrm{P}_\textrm{9B}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq56.png" style="width:1.69em"/></span>).</p></li><li><p class="Para" id="Par68"><em class="EmphasisTypeItalic ">An appropriate environment for creativity, more consistency and convenience.</em> As described by participants, it was “a middle point between personal space and no personal space” (<span class="InlineEquation" id="IEq57"><img alt="$$\textrm{P}_\textrm{6A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq57.png" style="width:1.69em"/></span>), without even triggering something, “[they] could decide in a continuous way if [they] were able to listen to the other sound sources or not”, and “to what extent [they] want to isolate [themselves]” (<span class="InlineEquation" id="IEq58"><img alt="$$\textrm{P}_\textrm{16A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq58.png" style="width:2.07em"/></span>). Compared with having to hear all sounds in <span class="InlineEquation" id="IEq59"><img alt="$$\textrm{C}_\textrm{pub}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq59.png" style="width:2.07em"/></span>, this provided them a “less stressing” (<span class="InlineEquation" id="IEq60"><img alt="$$\textrm{P}_\textrm{4A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq60.png" style="width:1.69em"/></span>) context, and they can selectively move away to avoid “getting interrupted with the other” (<span class="InlineEquation" id="IEq61"><img alt="$$\textrm{P}_\textrm{5B}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq61.png" style="width:1.69em"/></span>) and overlapping music. Being able to still “hear a bit of it in the background but not completely” (<span class="InlineEquation" id="IEq62"><img alt="$$\textrm{P}_\textrm{20A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq62.png" style="width:2.07em"/></span>) was reported good as this kept them “up to date” (<span class="InlineEquation" id="IEq63"><img alt="$$\textrm{P}_\textrm{9A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq63.png" style="width:1.69em"/></span>) and helped them to “tailor what [the participant] was making” (<span class="InlineEquation" id="IEq64"><img alt="$$\textrm{P}_\textrm{22B}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq64.png" style="width:2.07em"/></span>) to match the co-created music and to make something new and see if it “fit with” (<span class="InlineEquation" id="IEq65"><img alt="$$\textrm{P}_\textrm{20A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq65.png" style="width:2.07em"/></span>) the old. <span class="InlineEquation" id="IEq66"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq66.png" style="width:2em"/></span> provided them with “a little bit of personal space” although not a quite a “defined thing” (<span class="InlineEquation" id="IEq67"><img alt="$$\textrm{P}_\textrm{6A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq67.png" style="width:1.69em"/></span>), which provided the possibility “to work on something individually” but also being able to “share work quite easily” (<span class="InlineEquation" id="IEq68"><img alt="$$\textrm{P}_\textrm{20A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq68.png" style="width:2.07em"/></span>).</p></li><li><p class="Para" id="Par69"><em class="EmphasisTypeItalic ">Easier to identify sounds.</em> Participants reported it was easier to “locate the source of the sound” (<span class="InlineEquation" id="IEq69"><img alt="$$\textrm{P}_\textrm{16A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq69.png" style="width:2.07em"/></span>) and “perceive what [they were] doing” (<span class="InlineEquation" id="IEq70"><img alt="$$\textrm{P}_\textrm{15B}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq70.png" style="width:2.07em"/></span>), which helped them “understand instruments better” (<span class="InlineEquation" id="IEq71"><img alt="$$\textrm{P}_\textrm{7B}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq71.png" style="width:1.69em"/></span>) and “not get confused” (<span class="InlineEquation" id="IEq72"><img alt="$$\textrm{P}_\textrm{15B}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq72.png" style="width:2.07em"/></span>);</p></li><li><p class="Para" id="Par70"><em class="EmphasisTypeItalic ">More real.</em> Interestingly, instead of <span class="InlineEquation" id="IEq73"><img alt="$$\textrm{C}_\textrm{pub}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq73.png" style="width:2.07em"/></span>, which simulates the real-world sound attenuation, <span class="InlineEquation" id="IEq74"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq74.png" style="width:2em"/></span> was reported to be “real”. “If you want to hear something, you just come closer, like in the real world” (<span class="InlineEquation" id="IEq75"><img alt="$$\textrm{P}_\textrm{11B}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq75.png" style="width:2.07em"/></span> and <span class="InlineEquation" id="IEq76"><img alt="$$\textrm{P}_\textrm{11B}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq76.png" style="width:2.07em"/></span>), “it was good like we were feeling like the real-time experience (<span class="InlineEquation" id="IEq77"><img alt="$$\textrm{P}_\textrm{26B}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq77.png" style="width:2.07em"/></span>)”.</p></li></ul></div>
</div><p class="Para" id="Par71">It should also be noted that, along with these reports about advantage, there are 19 segments reporting <span class="InlineEquation" id="IEq78"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq78.png" style="width:2em"/></span>’s limitations, including: (i) a preference “to hear all the instruments all the time” in <span class="InlineEquation" id="IEq79"><img alt="$$\textrm{C}_\textrm{pub}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq79.png" style="width:2.07em"/></span> (<span class="InlineEquation" id="IEq80"><img alt="$$\textrm{P}_\textrm{26B}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq80.png" style="width:2.07em"/></span>), (ii) <span class="InlineEquation" id="IEq81"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq81.png" style="width:2em"/></span> might lead to “another type of compositions” and “influence the piece” (<span class="InlineEquation" id="IEq82"><img alt="$$\textrm{P}_\textrm{16B}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq82.png" style="width:2.07em"/></span>) and (iii) without being able to hear all sounds led to a feeling of separation (<span class="InlineEquation" id="IEq83"><img alt="$$\textrm{P}_\textrm{18A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq83.png" style="width:2.07em"/></span>).</p></section></section><section class="Section2 RenderAsSection2" id="Sec25"><h3 class="Heading"><span class="HeadingNumber">8.5.6 </span>Discussion</h3><p class="Para" id="Par72">The issues from having no personal space are clear. Especially for the music-making task in this study, participants reported that without personal space, the auditory background could be messy to develop own ideas, and their creativity required a quieter and more controllable environment, which could be provided by personal space. Providing such an environment is crucial considering individual creativity is an important part of the collaborative creativity. Having personal space was reported to be “an added advantage” because it promoted their own creativity, which can then be combined and contributed to the joint piece. This matches the findings in [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>], that providing personal spaces is helpful as it provides a chance to explore individual ideas freely, which then added an interesting dynamic to the collaborative work. However, adding personal space indeed brought a few impacts, next we discuss the impacts of using acoustic attenuation as personal space and its characteristics.</p><section class="Section3 RenderAsSection3" id="Sec26"><h4 class="Heading"><span class="HeadingNumber">8.5.6.1 </span>Impacts of Adding Acoustic Attenuation as Personal Space</h4><p class="Para" id="Par73">As mentioned above, in the previous study [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>], we found the addition of personal space located on the opposite side of the public space led to a shrunken size of group territory, fewer group note edits, a larger size of personal territory, more personal note edits, a larger average distance between collaborators and fewer times of paying attention to collaborator. We argued that these negative impacts are mainly due to the personal spaces distributed on the opposite side of the group space resulting in a larger distance between participants. So we proposed personal space with different features (e.g. gradual boundary—<span class="InlineEquation" id="IEq84"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq84.png" style="width:2em"/></span>) might reduce these negative effects. In many ways, <span class="InlineEquation" id="IEq85"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq85.png" style="width:2em"/></span> is quite similar to <span class="InlineEquation" id="IEq86"><img alt="$$\textrm{C}_\textrm{pub}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq86.png" style="width:2.07em"/></span>, e.g. both do not have a visual boundary for spaces, so not surprisingly, no significant differences were found in most of the statistical measures, see Table <span class="InternalRef"><a href="#Tab1">8.1</a></span>, and most previously identified disadvantages brought by adding rigid personal spaces have been successfully eliminated; more detailed results are available in [<span class="CitationRef"><a epub:type="biblioref" href="#CR35" role="doc-biblioref">35</a></span>]. By making the personal space invisible and gradual, the isolation and difficulty of coordinating that introduced by the additional personal space was minimised. For example, in the interview, participants reported <span class="InlineEquation" id="IEq87"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq87.png" style="width:2em"/></span> provided a proper level of group work as working context, making easier to create new that matches the old.</p></section><section class="Section3 RenderAsSection3" id="Sec27"><h4 class="Heading"><span class="HeadingNumber">8.5.6.2 </span>Providing Personal Space with Fluid Boundary</h4><p class="Para" id="Par74">Although no significant differences were found in PSQ2, see Table <span class="InternalRef"><a href="#Tab1">8.1</a></span>, which questioned the support each condition gave to individual creativity, <span class="InlineEquation" id="IEq88"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq88.png" style="width:2em"/></span> has a higher mean rating. The thematic analysis revealed more insights. <span class="InlineEquation" id="IEq89"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq89.png" style="width:2em"/></span> provides both “an appropriate background” with which participants felt “less stressed” and were able to “tailor” the individual composing to match the co-work, and a space personal enough to “work on something individually”. No major differences were found between <span class="InlineEquation" id="IEq90"><img alt="$$\textrm{C}_\textrm{pub}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq90.png" style="width:2.07em"/></span> and <span class="InlineEquation" id="IEq91"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq91.png" style="width:2em"/></span> in PSQ, indicating <span class="InlineEquation" id="IEq92"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq92.png" style="width:2em"/></span> provides a very mild solution, with limited impacts on people’s collaborative behaviour introduced, whilst still providing sufficient support for individual creativity during collaboration, thus H1 is validated.</p><p class="Para" id="Par75">Compared with natural attenuation in <span class="InlineEquation" id="IEq93"><img alt="$$\textrm{C}_\textrm{pub}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq93.png" style="width:2.07em"/></span>, <span class="InlineEquation" id="IEq94"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq94.png" style="width:2em"/></span>’s augmented sound attenuation setting forced or prompted people to work more closely in order to hear each other’s work, as reported by some participants. Compared with adding personal space with visible rigid boundary, by enabling participants to “decide in a continuous way” (<span class="InlineEquation" id="IEq95"><img alt="$$\textrm{P}_\textrm{16A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq95.png" style="width:2.07em"/></span>) if they want to hear other’s work, an invisible gradual boundary in <span class="InlineEquation" id="IEq96"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq96.png" style="width:2em"/></span> led to less separation, and higher consistency between personal and public space. H2 is therefore supported. This finding also echos the implication proposed in [<span class="CitationRef"><a epub:type="biblioref" href="#CR23" role="doc-biblioref">23</a></span>] that there should be many gradations between personal and public space to enable people fluidly shift in between. Popularity—the code “advantage of <span class="InlineEquation" id="IEq97"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq97.png" style="width:2em"/></span>” has 111 coded segments, and the code “most favourite—<span class="InlineEquation" id="IEq98"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq98.png" style="width:2em"/></span>” has 35 coded segments, both are greater than what <span class="InlineEquation" id="IEq99"><img alt="$$\textrm{C}_\textrm{pub}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq99.png" style="width:2.07em"/></span> gets. All indicate <span class="InlineEquation" id="IEq100"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq100.png" style="width:2em"/></span> is the most popular condition. The popularity is also partially verified by that <span class="InlineEquation" id="IEq101"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq101.png" style="width:2em"/></span> has the highest mean in preference measure (PSQ3 in Table <span class="InternalRef"><a href="#Tab1">8.1</a></span>). We believe the reasons behind this popularity are mainly due to its unique advantages, which as reported by participants, include: (i) an appropriate environment for creativity, (ii) easier to identify sounds and (iii) perceived as more “real” (although it should be noted that <span class="InlineEquation" id="IEq102"><img alt="$$\textrm{C}_\textrm{pub}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq102.png" style="width:2.07em"/></span> is more similar to real-world audio attenuation). These features of <span class="InlineEquation" id="IEq103"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq103.png" style="width:2em"/></span> made it provide better support for collaborative creativity and therefore led to its popularity.</p></section></section></section><section class="Section1 RenderAsSection1" id="Sec28"><h2 class="Heading"><span class="HeadingNumber">8.6 </span>General Discussion</h2><div class="Para" id="Par76">The two studies have explored 3D annotation and augmented acoustic attenuation’s role in CMM. This section compares the two approaches against each other, seeking the potential differences and finding out the usage scenarios. The comparison results are summarised in Table <span class="InternalRef"><a href="#Tab2">8.2</a></span>.<div class="Table" id="Tab2"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 8.2</span><p class="SimplePara">Comparison between the two routes</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1 align-left"/><col class="tcol2 align-left"/><col class="tcol3 align-left"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"> </th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">3D Annotation</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Augmented Attenuation</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Modality</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Visual</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Auditory</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Type of interaction</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Explicit interaction—active drawing</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Implicit interaction—passive body movement</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Supports for collaboration</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Supporting communication between users</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Supporting development of individual creativity</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Characteristic</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">No influence introduced on audio channel, users hear roughly the same audio<span class="InlineEquation" id="IEq104"><img alt="$$^\textrm{a}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq104.png" style="width:0.5em"/></span></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Influence introduced on audio and composition, users do not hear the same audio</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Applications</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Wider range of application, not restricted to audio tasks, audio tasks requiring precise audio output, or users with hearing/speech impairment</p></td><td style="text-align: left;"><p class="SimplePara">Application restricted to auditory tasks with no requirement for precise audio outputs</p></td></tr></tbody></table><div class="TableFooter"><p class="SimplePara"><span class="InlineEquation" id="IEq105"><img alt="$$^\textrm{a}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq105.png" style="width:0.5em"/></span>Strictly speaking, what users hear still slightly differs unless the realistic spatialisation of audio is disabled</p></div></div>
</div><section class="Section2 RenderAsSection2" id="Sec29"><h3 class="Heading"><span class="HeadingNumber">8.6.1 </span>Modality and Interaction Type</h3><p class="Para" id="Par77">3D annotation is a visual approach, while augmented attenuation is an audio approach. This fundamental difference led to their unique advantages and disadvantages, which then determine their scope of usage scenarios. Specifically, the visual approach can fully avoid influencing the audio channel, leaving that modality purely for composers to hear the project they are working on. While on the contrary, the audio approach imposes unavoidable effects on how the audio sounds, as the privacy is produced by augmenting the acoustic attenuation of the medium of the sound.</p><p class="Para" id="Par78">Unlike 3D annotation, which requires explicit interaction to make 3D lines, the augmented attenuation in Study II only relies on users’ passive listening and active physical locating in space. Explicit interaction is consciously deciding to interact, e.g. clicking a button. It is what we normally think about when we’re interacting with a computer [<span class="CitationRef"><a epub:type="biblioref" href="#CR55" role="doc-biblioref">55</a></span>]. Compared with explicit interaction, implicit interaction does not require users to perform conscious actions; the interaction is mainly the <span id="ITerm24">movement</span> (e.g. head movement and eye movement) of the user. As a result, the 3D annotation introduced a higher learning cost.</p></section><section class="Section2 RenderAsSection2" id="Sec30"><h3 class="Heading"><span class="HeadingNumber">8.6.2 </span>Key Support for Collaboration</h3><p class="Para" id="Par79">The 3D annotation helps people to warm up at the beginning, supports the non-vocal communication and provides help for collaborators to understand each other’s attention. In other words, it supports the social aspects of the collaboration by intensifying the links between collaborators. While the augmented attenuation gives collaborators the choice to be separated, hence provides support for individual creation. With this flexibility, users have the choice to develop their own work and to switch fluidly between working on own and teamwork.</p></section><section class="Section2 RenderAsSection2" id="Sec31"><h3 class="Heading"><span class="HeadingNumber">8.6.3 </span>Characteristic and Application</h3><p class="Para" id="Par80">3D annotation completely avoids impacts on the auditory channel. This supportive measure suits where the sonic output comes with stringent requirements, and users must be able to hear exactly the same final output during their working. Its application is not limited to sonic task because it provides support to communication, which is required by many collaborative tasks in SVEs. In contrast, the augmented attenuation has a narrower application range. It provides better support for individual activity, with still enough context of group work and the cost of hearing (slightly) differed output, making it only appropriate to audio related-tasks with no rigid requirements, e.g. people are improvising music for fun.</p><p class="Para" id="Par81">These two supportive features do not necessarily contradict each other, and could be applied simultaneously. To manage the simultaneous use, a manipulation system might be needed. For example, the transparency of the visual 3D annotation and the degree of augmentation of attenuation can be adjusted to modify their impacts (visibility/audibility), fitting collaborators’ needs during different stages of the collaborative composing. When only one feature is needed, the other can be adjusted to zero, wiping out its impacts entirely.</p></section></section><section class="Section1 RenderAsSection1" id="Sec32"><h2 class="Heading"><span class="HeadingNumber">8.7 </span>Conclusions and Future Work</h2><div class="Para" id="Par82">In this chapter, two different approaches to support collaborative sonic interaction in SVEs have been presented, one exploited visual modality and the other exploited audio modality. The results of both studies have been presented and reflected upon. A comparison between the two approaches has been made. Next, following the findings and discussion above, we propose six implications for supporting collaborative sonic interaction in SVEs, e.g. CMM.<span id="ITerm25"/>
<div class="OrderedList"><ol><li class="ListItem"><div class="ItemNumber">1.</div><div class="ItemContent"><p class="Para" id="Par83">Adding a system that supports 3D annotation may be considered to aid collaborator’s communication, especially if co-produced sound has to be prioritised over other modalities to avoid any impacts.</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">2.</div><div class="ItemContent"><p class="Para" id="Par84">For audio-related tasks in SVEs, adding personal space should be considered, as it provides sonic privacy and essential support for the development of individual creativity, which forms a key part of the collaborative creativity. This is especially essential when the output of the task is vulnerable (e.g. audio), and co-workers need a space where they can think of own ideas and develop own work.</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">3.</div><div class="ItemContent"><p class="Para" id="Par85">For audio-related tasks (e.g. collaborative music making), manipulating acoustic attenuation as personal space can be an effective way to allow users to shift between personal and public working space continuously by adjusting their relative distance. With light-weight form, it introduces mild impacts compared with the prominent negative impacts introduced by rigid personal space [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>].</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">4.</div><div class="ItemContent"><p class="Para" id="Par86">The level of privacy can be adjusted by manipulating the level of augmentation. For instance, in <span class="InlineEquation" id="IEq106"><img alt="$$\textrm{C}_\textrm{aug}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq106.png" style="width:2em"/></span> of Study II, participants adjusted their distance between themselves and collaborators to achieve different levels of being personal (herein referred to as “personalness”). Instead of changing positions, adjusting the sound attenuation rate with distance can impact the level of “personalness” and therefore producing a varied level of personalness. Potentially, adding a method allowing users to adjust the level might be useful so users can shift between having a “very personal and isolated” space and a “very public” space.</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">5.</div><div class="ItemContent"><p class="Para" id="Par87">Augmented attenuation can be exploited for creative audio privacy, which can be then used to promote individual creativity during the collaboration. However, augmented attenuation introduces differences in what collaborators hear, making it only applicable to contexts with no rigid requirements on audio outputs.</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">6.</div><div class="ItemContent"><p class="Para" id="Par88">We suggest that augmented attenuation and 3D annotation could be applied together or chosen with a flexible switch so that users can choose the feature fitting their needs during different stages of the collaborative composition.</p></div><div class="ClearBoth"> </div></li></ol></div>
</div><p class="Para" id="Par89">Future works concern an exploration of how multi-modal approaches can be applied simultaneously, and designing and applying tools based on other modalities to support collaborative sonic interaction in SVEs, such as visual modality. For each modality, it could be interesting to test how that sensory cue can be augmented/depressed to adjust the level of its influence.</p></section><div class="Acknowledgments" epub:type="acknowledgments" role="doc-acknowledgments"><div class="Heading">Acknowledgements</div><p class="SimplePara">This work is partially supported by EPSRC and AHRC Centre for Doctoral Training in Media and Arts Technology (EP/ L01632X/1). We would also like to thank Ms Louise Bryce and Ms Danqi Zhao for their help in proofreading this chapter.</p></div><div class="License LicenseSubType-cc-by"><a href="https://creativecommons.org/licenses/by/4.0"><img alt="Creative Commons" src="../css/cc-by.png"/></a><p class="SimplePara"><strong class="EmphasisTypeBold ">Open Access</strong> This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (<span class="ExternalRef"><a href="http://creativecommons.org/licenses/by/4.0/"><span class="RefSource">http://​creativecommons.​org/​licenses/​by/​4.​0/​</span></a></span>), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p><p class="SimplePara">The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p></div><aside aria-labelledby="Bib1Heading" class="Bibliography" id="Bib1"><div epub:type="bibliography" role="doc-bibliography"><div class="Heading" id="Bib1Heading">References</div><ol class="BibliographyWrapper"><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">1.</div><div class="CitationContent" id="CR1">Barbosa, Á.: Displaced soundscapes: A survey of network systems for music and sonic art creation. Leonardo Music Journal <strong class="EmphasisTypeBold ">13</strong>, 53–59 (2003).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1162/096112104322750791"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">2.</div><div class="CitationContent" id="CR2">Basdogan, C., Ho, C.-H., Srinivasan, M. A., Slater, M.: An experimental study on the role of touch in shared virtual environments. ACM Transactions on Computer-Human Interaction (TOCHI) <strong class="EmphasisTypeBold ">7</strong>, 443–460 (2000).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1145/365058.365082"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">3.</div><div class="CitationContent" id="CR3">Benford, S., Bowers, J., Fahlén, L. E., Greenhalgh, C., Snowdon, D.: User embodiment in collaborative virtual environments in Proceedings of the SIGCHI conference on Human factors in computing systems (1995), 242–249.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">4.</div><div class="CitationContent" id="CR4">Benford, S., Greenhalgh, C., Rodden, T., Pycock, J.: Collaborative virtual environments. Communications of the ACM <strong class="EmphasisTypeBold ">44</strong>, 79–85 (2001).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1145/379300.379322"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">5.</div><div class="CitationContent" id="CR5">Billinghurst, M., Kato, H.: Collaborative augmented reality. Communications of the ACM <strong class="EmphasisTypeBold ">45</strong>, 64–70 (2002).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1145/514236.514265"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">6.</div><div class="CitationContent" id="CR6">Billinghurst, M., Poupyrev, I., Kato, H., May, R.: Mixing realities in shared space: An augmented reality interface for collaborative computing in Multimedia and Expo, 2000. ICME 2000. 2000 IEEE International Conference on <strong class="EmphasisTypeBold ">3</strong> (2000), 1641–1644.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">7.</div><div class="CitationContent" id="CR7">Bin, S. A., Bryan-Kinns, N., McPherson, A., et al.: Hands where we can see them! investigating the impact of gesture size on audience perception. International Computer Music Conference (2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">8.</div><div class="CitationContent" id="CR8">Blaine, T., Fels, S.: Contexts of collaborative musical experiences in Proceedings of the 2003 conference on New interfaces for musical expression (2003), 129–134.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">9.</div><div class="CitationContent" id="CR9">Blaine, T., Perkis, T.: The Jam-O-Drum Interactive Music System: A Study in Interaction Design in Proceedings of the 3rd Conference on Designing Interactive Systems: Processes, Practices, Methods, and Techniques (Association for Computing Machinery, New York City, New York, USA, 2000), 165–173.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">10.</div><div class="CitationContent" id="CR10">Braun, V., Clarke, V.: Using thematic analysis in psychology. Qualitative research in psychology <strong class="EmphasisTypeBold ">3</strong>, 77–101 (2006).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1191/1478088706qp063oa"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">11.</div><div class="CitationContent" id="CR11">Bressan, F., Vets, T., Leman, M.: A multimodal interactive installation for collaborative music making: From preservation to enhanced user design in Proceedings of the European Society for Cognitive Sciences Of Music (ESCOM) Conference, Ghent University (2017), 23–26.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">12.</div><div class="CitationContent" id="CR12">Brooks Jr, F. et al.: Research directions in virtual environments. Computer Graphics <strong class="EmphasisTypeBold ">26</strong>, 153 (1992).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1145/142413.142416"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">13.</div><div class="CitationContent" id="CR13">Bryan-Kinns, N.: Daisyphone: the design and impact of a novel environment for remote group music improvisation in Proceedings of the 5th conference on Designing interactive systems: processes, practices, methods, and techniques (2004), 135–144.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">14.</div><div class="CitationContent" id="CR14">Bryan-Kinns, N.: Annotating distributed scores for mutual engagement in daisyphone and beyond. Leonardo Music Journal <strong class="EmphasisTypeBold ">21</strong>, 51–55 (2011).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1162/LMJ_a_00061"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">15.</div><div class="CitationContent" id="CR15">Bryan-Kinns, N.: Mutual engagement and collocation with shared representations. International Journal of Human-Computer Studies <strong class="EmphasisTypeBold ">71</strong>, 76–90 (2013).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1016/j.ijhcs.2012.02.004"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">16.</div><div class="CitationContent" id="CR16">Bryan-Kinns, N., Hamilton, F.: Identifying mutual engagement. Behaviour &amp; Information Technology <strong class="EmphasisTypeBold ">31</strong>, 101–125 (2012).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1080/01449290903377103"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">17.</div><div class="CitationContent" id="CR17">Bryan-Kinns, N., Healey, P. G.: Decay in collaborative music making in Proceedings of the 2006 conference on new interfaces for musical expression (2006), 114–117.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">18.</div><div class="CitationContent" id="CR18">Carlsson, C., Hagsand, O.: DIVE A multi-user virtual reality system in Proceedings of IEEE Virtual Reality Annual International Symposium (1993), 394–400.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">19.</div><div class="CitationContent" id="CR19">De Simone, F. et al.: Watching Videos Together in Social Virtual Reality: An Experimental Study on User’s QoE in 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR) (2019), 890–891.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">20.</div><div class="CitationContent" id="CR20">Draper, J. V., Kaber, D. B., Usher, J. M.: Telepresence. Human Factors <strong class="EmphasisTypeBold ">40</strong>, 354–375 (1998).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1518/001872098779591386"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">21.</div><div class="CitationContent" id="CR21">Ellemers, N., Rink, F. in Social Identification in Groups 1–41 (Emerald Group Publishing Limited, 2005).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">22.</div><div class="CitationContent" id="CR22">Frécon, E., Greenhalgh, C., Stenius, M.: The DiveBone-an application-level network architecture for Internet-based CVEs in Proceedings of the ACM symposium on Virtual reality software and technology (1999), 58–65.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">23.</div><div class="CitationContent" id="CR23">Greenberg, S., Boyle, M., LaBerge, J.: PDAs and shared public displays: Making personal information public, and public information personal. Personal Technologies <strong class="EmphasisTypeBold ">3</strong>, 54–64 (1999).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1007/BF01305320"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">24.</div><div class="CitationContent" id="CR24">Gutwin, C., Greenberg, S. in Team cognition: Understanding the Factors that Drive Process and Performance (eds Salas, E., Fiore, S. M.) 177–201 (Washington, DC, US: American Psychological Association, 2004).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">25.</div><div class="CitationContent" id="CR25">Healey, P. G., Leach, J., Bryan-Kinns, N.: Inter-play: Understanding group music improvisation as a form of everyday interaction. Proceedings of Less is More-Simple Computing in an Age of Complexity (2005).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">26.</div><div class="CitationContent" id="CR26">Heeter, C.: Being there: The subjective experience of presence. Presence: Teleoperators &amp; Virtual Environments <strong class="EmphasisTypeBold ">1</strong>, 262–271 (1992).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">27.</div><div class="CitationContent" id="CR27">Hendrix, C., Barfield, W.: The sense of presence within auditory virtual environments. Presence: Teleoperators &amp; Virtual Environments <strong class="EmphasisTypeBold ">5</strong>, 290–301 (1996).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">28.</div><div class="CitationContent" id="CR28">Hunt, C.: Field of view face-off: Rift vs Vive vs Gear VR vs PSVR <span class="ExternalRef"><a href="https://www.vrheads.com/field-view-faceoff-rift-vs-vive-vsgear-vr-vs-psvr"><span class="RefSource">https://​www.​vrheads.​com/​field-view-faceoff-rift-vs-vive-vsgear-vr-vs-psvr</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">29.</div><div class="CitationContent" id="CR29">Jordá, S.: On stage: the reactable and other musical tangibles go real. Int. J. Arts Technol. <strong class="EmphasisTypeBold ">1</strong>, 268–287 (2008).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1504/IJART.2008.022363"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">30.</div><div class="CitationContent" id="CR30">Lab, R. L.: Field of View for Virtual Reality Headsets Explained <span class="ExternalRef"><a href="https://vr-lens-lab.com/field-of-view-for-virtual-reality-headsets/"><span class="RefSource">https://​vr-lens-lab.​com/​field-of-view-for-virtual-reality-headsets/​</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">31.</div><div class="CitationContent" id="CR31">Lea, R., Honda, Y., Matsuda, K.: Virtual society: Collaboration in 3D spaces on the Internet. Computer Supported CooperativeWork (CSCW) <strong class="EmphasisTypeBold ">6</strong>, 227–250 (1997).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1023/A:1008672915465"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">32.</div><div class="CitationContent" id="CR32">Luciani, A. in Enaction and enactive interfaces : a handbook of terms 299–300 (Enactive Systems Book, 2007).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">33.</div><div class="CitationContent" id="CR33">Men, L., Bryan-Kinns, N.: LeMo: Supporting Collaborative Music Making in Virtual Reality in 2018 IEEE 4th VR Workshop on Sonic Interactions for Virtual Environments (SIVE) (2018), 1–6.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">34.</div><div class="CitationContent" id="CR34">Men, L., Bryan-Kinns, N.: LeMo: Exploring Virtual Space for Collaborative Creativity in Proceedings of ACM conference on Creativity and cognition 2019 (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">35.</div><div class="CitationContent" id="CR35">Men, L., Bryan-Kinns, N., Bryce, L.: Designing spaces to support collaborative creativity in shared virtual environments. PeerJ Computer Science <strong class="EmphasisTypeBold ">5</strong>, e229 (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">36.</div><div class="CitationContent" id="CR36">Men, L., Bryan-Kinns, N., Hassard, A. S., Ma, Z.: The impact of transitions on user experience in virtual reality in Virtual Reality (VR), 2017 IEEE (2017), 285–286.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">37.</div><div class="CitationContent" id="CR37">Men, L., Zhao, D.: Designing Privacy for Collaborative Music Making in Virtual Reality in Proceedings of the 16th International Conference on Audio Mostly (Trento, Italy, 2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">38.</div><div class="CitationContent" id="CR38">Minsky, M.: Telepresence. Omni, 45–51 (1980).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">39.</div><div class="CitationContent" id="CR39">Morreale, F., Angeli, A. D., O’Modhrain, S.: Musical Interface Design: An Experience-oriented Framework in Proceedings of the International Conference on New Interfaces for Musical Expression (NIME) (2014), 467–472.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">40.</div><div class="CitationContent" id="CR40">Nabavian, S., Bryan-Kinns, N.: Analysing group creativity: A distributed cognitive study of joint music composition. Proc. of Cognitive Science, 1856–1861 (2006).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">41.</div><div class="CitationContent" id="CR41">Nassiri, N., Powell, N., Moore, D.: Human interactions and personal space in collaborative virtual environments. Virtual reality <strong class="EmphasisTypeBold ">14</strong>, 229–240 (2010).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1007/s10055-010-0169-3"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">42.</div><div class="CitationContent" id="CR42">Nedel, L. et al.: Using Immersive Virtual Reality to Reduce Work Accidents in Developing Countries. IEEE Computer Graphics and Applications <strong class="EmphasisTypeBold ">36</strong>, 36–46 (2016).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1109/MCG.2016.19"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">43.</div><div class="CitationContent" id="CR43">Nowak, K. L., Biocca, F.: The effect of the agency and anthropomorphism on users’ sense of telepresence, copresence, and social presence in virtual environments. Presence: Teleoperators &amp; Virtual Environments <strong class="EmphasisTypeBold ">12</strong>, 481–494 (2003).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">44.</div><div class="CitationContent" id="CR44">Ochse, R., Ochse, R.: Before the gates of excellence: The determinants of creative genius (CUP Archive, 1990).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">45.</div><div class="CitationContent" id="CR45">Oculus: Toybox Demo for Oculus Touch <span class="ExternalRef"><a href="https://www.youtube.com/watch?v=iFEMiyGMa58"><span class="RefSource">https://​www.​youtube.​com/​watch?​v=​iFEMiyGMa58</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">46.</div><div class="CitationContent" id="CR46">Pirola-Merlo, A., Mann, L.: The relationship between individual creativity and team creativity: Aggregating across people and time. Journal of Organizational behavior <strong class="EmphasisTypeBold ">25</strong>, 235–257 (2004).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1002/job.240"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">47.</div><div class="CitationContent" id="CR47">Plante, C.: A PlayStation VR demo made me cry from laughing <span class="ExternalRef"><a href="https://www.theverge.com/2016/3/16/11246334/playstation-virtualreality-social-vr-demo"><span class="RefSource">https://​www.​theverge.​com/​2016/​3/​16/​11246334/​playstation-virtualreality-social-vr-demo</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">48.</div><div class="CitationContent" id="CR48">Rice, R. E.: Media appropriateness: Using social presence theory to compare traditional and new organizational media. Human Communication Research <strong class="EmphasisTypeBold ">19</strong>, 451–484 (1993).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1111/j.1468-2958.1993.tb00309.x"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">49.</div><div class="CitationContent" id="CR49">Rodden, T.: A survey of CSCW systems. Interacting with computers <strong class="EmphasisTypeBold ">3</strong>, 319–353 (1991).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1016/0953-5438(91)90020-3"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">50.</div><div class="CitationContent" id="CR50">Romano, D. M., Brna, P., Self, J. A.: Collaborative decision-making and presence in shared dynamic virtual environments in Proceedings of the Workshop on Presence in SharedVirtual Environments.BTLabs, Martlesham Heath (1998).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">51.</div><div class="CitationContent" id="CR51">Roussos, M. et al.: NICE: combining constructionism, narrative and collaboration in a virtual learning environment. Computer Graphics-New York- Association for Computing Machinery <strong class="EmphasisTypeBold ">31</strong>, 62–63 (1997).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">52.</div><div class="CitationContent" id="CR52">Rovai, A. P.: Building sense of community at a distance. The International Review of Research in Open and Distributed Learning <strong class="EmphasisTypeBold ">3</strong> (2002).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">53.</div><div class="CitationContent" id="CR53">Schroeder, R.: The social life of avatars: Presence and interaction in shared virtual environments (Springer Science &amp; Business Media, 2012).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">54.</div><div class="CitationContent" id="CR54">Schubert, T., Friedmann, F., Regenbrecht, H.: The experience of presence: Factor analytic insights. Presence: Teleoperators &amp; Virtual Environments <strong class="EmphasisTypeBold ">10</strong>, 266–281 (2001).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">55.</div><div class="CitationContent" id="CR55">Serim, B., Jacucci, G.: Explicating" Implicit Interaction": An Examination of the Concept and Challenges for Research in Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (2019), 417.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">56.</div><div class="CitationContent" id="CR56">Shen, C., Everitt, K., Ryall, K.: UbiTable: Impromptu face-to-face collaboration on horizontal interactive surfaces in International Conference on Ubiquitous Computing (2003), 281–288.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">57.</div><div class="CitationContent" id="CR57">Sheridan, T. B.: Musings on telepresence and virtual presence. Presence: Teleoperators &amp; Virtual Environments <strong class="EmphasisTypeBold ">1</strong>, 120–126 (1992).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">58.</div><div class="CitationContent" id="CR58">Sherman, W. R., Craig, A. B.: Understanding Virtual reality: Interface, Application, and Design (Morgan Kaufmann, 2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">59.</div><div class="CitationContent" id="CR59">Short, J., Williams, E., Christie, B.: The social psychology of telecommunications. The Social Psychology of Telecommunications (1976).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">60.</div><div class="CitationContent" id="CR60">Simonton, D. K.: Creative productivity: A predictive and explanatory model of career trajectories and landmarks. Psychological review <strong class="EmphasisTypeBold ">104</strong>, 66 (1997).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1037/0033-295X.104.1.66"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">61.</div><div class="CitationContent" id="CR61">Slater, M., Usoh, M., Steed, A.: Depth of Presence in Virtual Environments. Presence <strong class="EmphasisTypeBold ">3</strong>, 130–144 (1994).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1162/pres.1994.3.2.130"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">62.</div><div class="CitationContent" id="CR62">Slater, M.: Place illusion and plausibility can lead to realistic behaviour in immersive virtual environments. Phil. Trans. R. Soc. B <strong class="EmphasisTypeBold ">364</strong>, 3549–3557 (2009).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1098/rstb.2009.0138"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">63.</div><div class="CitationContent" id="CR63">Slater, M., Wilbur, S.: A framework for immersive virtual environments (FIVE): Speculations on the role of presence in virtual environments. Presence: Teleoperators &amp; Virtual Environments <strong class="EmphasisTypeBold ">6</strong>, 603–616 (1997).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">64.</div><div class="CitationContent" id="CR64">Steuer, J.: Defining virtual reality: Dimensions determining telepresence. Journal of Communication <strong class="EmphasisTypeBold ">42</strong>, 73–93 (1992).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1111/j.1460-2466.1992.tb00812.x"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">65.</div><div class="CitationContent" id="CR65">Sugimoto, M., Hosoi, K., Hashizume, H.: Caretta: a system for supporting face-to-face collaboration by integrating personal and shared spaces in Proceedings of the SIGCHI conference on Human factors in computing systems (2004), 41–48.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">66.</div><div class="CitationContent" id="CR66">Thiebaut, J.-B., Healey, P. G., Bryan-Kinns, N.: Drawing Electroacoustic Music. in ICMC (2008).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">67.</div><div class="CitationContent" id="CR67">Titon, J. T., Slobin, M.: The music-culture as a world of music. Worlds of music: an introduction to the music of the world’s peoples. New York: Schirmer Books (1996).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">68.</div><div class="CitationContent" id="CR68">Wallace, P., Maryott, J.: The impact of avatar self-representation on collaboration in virtual worlds. Innovate: Journal of Online Education <strong class="EmphasisTypeBold ">5</strong>, 3 (2009).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">69.</div><div class="CitationContent" id="CR69">Wang, G.: Designing Smule’s Ocarina: The iPhone’s Magic Flute. in NIME (2009), 303–307.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">70.</div><div class="CitationContent" id="CR70">Weinberg, G.: Interconnected musical networks-bringing expression and thoughtfulness to collaborative music making in Massachusetts Institute of Technology Media Laboratory (2003).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">71.</div><div class="CitationContent" id="CR71">Witmer, B. G., Singer, M. J.: Measuring presence in virtual environments: A presence questionnaire. Presence <strong class="EmphasisTypeBold ">7</strong>, 225–240 (1998).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1162/105474698565686"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">72.</div><div class="CitationContent" id="CR72">Wozniewski, M., Bouillot, N., Settel, Z., Cooperstock, J. R.: Large-Scale Mobile Audio Environments for Collaborative Musical Interaction. in NIME (2008), 13–18.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">73.</div><div class="CitationContent" id="CR73">Yee, N., Bailenson, J. N., Urbanek, M., Chang, F., Merget, D.: The unbearable likeness of being digital: the persistence of nonverbal social norms in online virtual environments. CyberPsychology &amp; Behavior <strong class="EmphasisTypeBold ">10</strong>, 115–121 (2007).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1089/cpb.2006.9984"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">74.</div><div class="CitationContent" id="CR74">Yin, R. K.: Case study research and applications: Design and methods (Sage publications, 2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">75.</div><div class="CitationContent" id="CR75">Zhang, X., Furnas, G. W.: The effectiveness of multiscale collaboration in virtual environments in CHI’03 Extended Abstracts on Human Factors in Computing Systems (2003), 790–791.</div></li></ol></div></aside><aside aria-label="Footnotes" class="FootnoteSection" epub:type="footnotes"><div class="Heading">Footnotes</div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn1_source">1</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn1" role="doc-footnote"><p class="Para" id="Par4">AltSpaceVR: <span class="ExternalRef"><a href="https://altvr.com"><span class="RefSource">https://​altvr.​com</span></a></span>.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn2_source">2</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn2" role="doc-footnote"><p class="Para" id="Par5">Venues: <span class="ExternalRef"><a href="https://www.oculus.com/experiences/quest/3002729676463989/"><span class="RefSource">https://​www.​oculus.​com/​experiences/​quest/​3002729676463989​/​</span></a></span></p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn3_source">3</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn3" role="doc-footnote"><p class="Para" id="Par11">More information is available at:</p><p class="Para" id="Par12"><span class="ExternalRef"><a href="https://sites.google.com/view/liangmen/projects/LeMo"><span class="RefSource">https://​sites.​google.​com/​view/​liangmen/​projects/​LeMo</span></a></span></p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn4_source">4</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn4" role="doc-footnote"><p class="Para" id="Par19">We limit the number to 8 to achieve a proper frame rate.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn5_source">5</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn5" role="doc-footnote"><p class="Para" id="Par24">The Queen Mary Research Ethics Committee granted ethical approval to carry out the study within its facilities (Ethical Application Ref: QMREC1592).</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn6_source">6</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn6" role="doc-footnote"><p class="Para" id="Par59">The Queen Mary Research Ethics Committee granted ethical approval to carry out the study within its facilities (Ethical Application Ref: QMREC2005).</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn7_source">7</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn7" role="doc-footnote"><p class="Para" id="Par64"><span class="InlineEquation" id="IEq23"><img alt="$$\textrm{P}_\textrm{11B}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq23.png" style="width:2.07em"/></span> refers to participant B in group 11, similarly, <span class="InlineEquation" id="IEq24"><img alt="$$\textrm{P}_\textrm{9A}$$" src="../images/478239_1_En_8_Chapter/478239_1_En_8_Chapter_TeX_IEq24.png" style="width:1.69em"/></span> indicates participant A in group 9.</p></div><div class="ClearBoth"> </div></div></aside></div></div></body></html>