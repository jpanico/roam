<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops"><head><title>Spatial Design Considerations for Interactive Audio in Virtual Reality</title><meta content="text/html; charset=utf-8" http-equiv="content-type"/><link href="../css/springer_epub.css" rel="styleSheet" type="text/css"/></head><body><div epub:type="chapter" role="doc-chapter"><div class="ChapterContextInformation"><div class="ContextInformation" id="Chap6"><div class="ChapterCopyright">© The Author(s) 2023</div><span class="ContextInformationAuthorEditorNames">M. Geronazzo, S. Serafin<span class="CollaboratorDesignation"> (eds.)</span></span><span class="ContextInformationBookTitles"><span class="BookTitle">Sonic Interactions in Virtual Environments</span></span><span class="ContextInformationSeries"><span class="SeriesTitle" lang="en">Human–Computer Interaction Series</span></span><span class="ChapterDOI"><a href="https://doi.org/10.1007/978-3-031-04021-4_6">https://doi.org/10.1007/978-3-031-04021-4_6</a></span></div></div><!--Begin Abstract--><div class="MainTitleSection"><h1 class="ChapterTitle" lang="en">6. Spatial Design Considerations for Interactive Audio in Virtual Reality</h1></div><div class="AuthorGroup"><div class="AuthorNames"><span class="Author"><span class="AuthorName">Thomas Deacon</span><sup><a href="#Aff34">1</a> <span class="ContactIcon"> </span></sup> and </span><span class="Author"><span class="AuthorName">Mathieu Barthet</span><sup><a href="#Aff35">2</a> <a aria-label="Contact information for this author" href="#ContactOfAuthor2"><span class="ContactIcon"> </span></a></sup></span></div><div class="Affiliations"><div class="Affiliation" id="Aff34"><span class="AffiliationNumber">(1)</span><div class="AffiliationText">Media and Arts Technology CDT, Queen Mary University of London, London, United Kingdom</div></div><div class="Affiliation" id="Aff35"><span class="AffiliationNumber">(2)</span><div class="AffiliationText">Centre for Digital Music, Queen Mary University of London, London, United Kingdom</div></div><div class="ClearBoth"> </div></div><div class="Contacts"><div class="Contact" id="ContactOfAuthor2"><div class="ContactIcon"> </div><div class="ContactAuthorLine"><span class="AuthorName">Mathieu Barthet</span></div><div class="ContactAdditionalLine"><span class="ContactType">Email: </span><a href="mailto:m.barthet@qmul.ac.uk">m.barthet@qmul.ac.uk</a></div></div></div></div><section class="Abstract" id="Abs1" lang="en" role="doc-abstract"><h2 class="Heading">Abstract</h2><p class="Para" id="Par1">Space is a fundamental feature of virtual reality (VR) systems, and more generally, human experience. Space is a place where we can produce and transform ideas and act to create meaning. It is also an information container. When working with sound and space interactions, making VR systems becomes a fundamentally interdisciplinary endeavour. To support the design of future systems, designers need an understanding of spatial design decisions that impact audio practitioners’ processes and communication. This chapter proposes a typology of VR interactive audio systems, focusing on their function and the role of space in their design. Spatial categories are proposed to be able to analyse the role of space within existing interactive audio VR products. Based on the spatial design considerations explored in this chapter, a series of implications for design are offered that future research can exploit.</p></section><!--End Abstract--><div class="Fulltext"><section class="Section1 RenderAsSection1" id="Sec1"><h2 class="Heading"><span class="HeadingNumber">6.1 </span>Introduction</h2><p class="Para" id="Par2">Technologies like virtual reality (VR) offer many ways of using space that could benefit creative audio production and immersive experience applications. Using VRs affordances for embodied interaction and spatial user interfaces, new forms of spatial expression can be explored. Running parallel to VR research efforts in sonic interaction in virtual environments(SIVE)<span id="ITerm1"/>, much of sonic practice exists as applied design, either as music making tools [<span class="CitationRef"><a epub:type="biblioref" href="#CR110" role="doc-biblioref">110</a></span>], experiential products [<span class="CitationRef"><a epub:type="biblioref" href="#CR106" role="doc-biblioref">106</a></span>], or games [<span class="CitationRef"><a epub:type="biblioref" href="#CR102" role="doc-biblioref">102</a></span>]. Commercial work is influenced by academia, but it is also based on broader professional constituencies and practices not related to sound and music interaction design.</p><p class="Para" id="Par3">Much of VR design practice is communicated as professional dialogues, such as platform or technology best practice guides [<span class="CitationRef"><a epub:type="biblioref" href="#CR120" role="doc-biblioref">120</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR121" role="doc-biblioref">121</a></span>], or reviews of “lessons-learned” in industrial settings [<span class="CitationRef"><a epub:type="biblioref" href="#CR105" role="doc-biblioref">105</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR122" role="doc-biblioref">122</a></span>]. Within these professional dialogues, previous research, new technological capabilities, and commercial user research are collected together to inform communities on how to best support users and task domains. For the field of SIVE, and sound and music Computing(SMC)<span id="ITerm2"/> more broadly, there is still work to be done to bridge commercial practice and academic endeavours. Despite recent works [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR77" role="doc-biblioref">77</a></span>], there is a paucity of design recommendations and analysis regarding how to build spaces, interfaces, and spatial interactions with sound. For the potential of VR to be unlocked as a creative medium, multi and interdisciplinary work must be undertaken to bring together the disciplines that touch on space, interaction, and sound.</p><p class="Para" id="Par4">Studying how people make immersive tools, in commercial and academic settings, requires a means of framing how <span id="ITerm3">spatial designdecisions</span> impact users. This brings up two problems, what role do commercial artefacts have in broadening research understanding, and how is relevant knowledge generated from such products? Objects, prototypes, and artefacts create a context for forming new understanding [<span class="CitationRef"><a epub:type="biblioref" href="#CR46" role="doc-biblioref">46</a></span>]. By analysing an artefact design, research can discover (recover and invent) requirements to create technological propositions related to domain-specific concerns [<span class="CitationRef"><a epub:type="biblioref" href="#CR82" role="doc-biblioref">82</a></span>]. This is because an artefact collects designers judgements about specific design spaces [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>], for instance how to solve interaction problems, and what aspects are of priority to users at different points in an activity. However, this means we cannot recover the needs of design by direct questioning the users alone. A broader research picture is needed, one that integrates action with tools, users, and reflection on devices. So, to develop an understanding for future design interventions, research should gather diverse data to understand the existing practice and perceived professional constituencies.<sup><a epub:type="noteref" href="#Fn1" id="Fn1_source" role="doc-noteref">1</a></sup>
</p><p class="Para" id="Par6">Section <span class="InternalRef"><a href="#Sec2">6.2</a></span> sets out the problem of space in more detail, highlighting important contributions to the design of VR sound and music interaction systems. Section <span class="InternalRef"><a href="#Sec2">6.2</a></span> also describes the suitability of typologies to spatial analysis for this research. Following on from this, Sect. <span class="InternalRef"><a href="#Sec9">6.3.1</a></span> outlines the approach taken to the design review and typology, indicating how relevant work was identified, selected, and coded. Section <span class="InternalRef"><a href="#Sec8">6.3</a></span> sets out a typology of interactive audio systems in VR, and presents case studies of spatial design in the field. Section <span class="InternalRef"><a href="#Sec18">6.5</a></span> looks across analyses and offers ways to understand the design space of VR for SMC. Based on findings and reflections, Sect. <span class="InternalRef"><a href="#Sec22">6.6</a></span> proposes actionable design outcomes for further research, then Sect. <span class="InternalRef"><a href="#Sec26">6.7</a></span> draws the work to a close.</p></section><section class="Section1 RenderAsSection1" id="Sec2"><h2 class="Heading"><span class="HeadingNumber">6.2 </span>Background</h2><section class="Section2 RenderAsSection2" id="Sec3"><h3 class="Heading"><span class="HeadingNumber">6.2.1 </span>Terminology</h3><p class="Para" id="Par7">This chapter analyses the spatial design of interactive audio systems (IAS)<span id="ITerm4"/> in VR. IAS refers to any sound and music computing system that involves human interaction that can modify the state of the sound or music system, however, we do not review information-only auditory displays or audio-rendering technologies. While both auditory displays and rendering technologies do include interactivity in their operation, this chapter is interested in the use of interactive sound as the primary function in the VR application, rather than when sound is used as an information medium or renderer of spatial sounds without interactive feedback beyond head rotation. No doubt there are significant overlaps in theory and application, that would be valuable to explore, but trying to address all aspects in one chapter requires a different focus.</p><div class="Para" id="Par8">The following research areas pertain to spatial interaction with user interfaces (UI)s:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par9">Spatial user interface (SUI)<span id="ITerm5"/>: Human-computer interaction (HCI) with 3D or 2D UI that is operated through spatial interaction, graphically or otherwise [<span class="CitationRef"><a epub:type="biblioref" href="#CR59" role="doc-biblioref">59</a></span>].</p></li><li><p class="Para" id="Par10">Three-dimensional user interface (3DUI)<span id="ITerm6"/>
<span id="ITerm7"/>: A UI that involves 3D interaction [<span class="CitationRef"><a epub:type="biblioref" href="#CR16" role="doc-biblioref">16</a></span>].</p></li><li><p class="Para" id="Par11">Distributed user interface (DUI): UIs that are distributed across devices, users, or spatial access points [<span class="CitationRef"><a epub:type="biblioref" href="#CR89" role="doc-biblioref">89</a></span>].</p></li></ul></div>
</div><div class="Para" id="Par12">There are also many terms to describe virtual spaces used for sound and music; in particular, this research is concerned with immersive VR technology, following the definition provided in [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>]:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par13">Virtual—to be a virtual reality, the reality must be simulated (e.g. computer-generated).</p></li><li><p class="Para" id="Par14">Immersive—to be a virtual reality, the reality must give its users the sensation of being surrounded by a world.</p></li><li><p class="Para" id="Par15">Interactive—to be a virtual reality, the reality must allow its users to affect the reality in some meaningful way.</p></li></ul></div>
</div><p class="Para" id="Par16">The term VR can refer to the hardware systems for delivering immersive experiences and to refer to the immersive experiences themselves. Hardware systems can include commercial head-mounted display (HMD) technology, such as Oculus or HTC Vive, through to complex stereographic projection-based Cave Automatic Virtual Environment (CAVEs) [<span class="CitationRef"><a epub:type="biblioref" href="#CR12" role="doc-biblioref">12</a></span>]. The key thing is that in these immersive environments the visual system and interaction capacities are mediated through technological <span id="ITerm8">means</span>. In the case of social virtual reality (SVR)<span id="ITerm9"/>, described in Chap. <span class="ExternalRef"><a href="478239_1_En_8_Chapter.xhtml"><span class="RefSource">8</span></a></span> of this volume, communication layers (speech, posture, and gesture) may or may not be mediated through technological means, for instance co-located users may share a virtual world via HMD but speech communication is unmediated. Or remote SVR users’ communication could be completely mediated by avatar representations and voice over internet protocol (VoIP) technology.</p></section><section class="Section2 RenderAsSection2" id="Sec4"><h3 class="Heading"><span class="HeadingNumber">6.2.2 </span>Standing on the Shoulders of Giants, but Which Ones?!</h3><p class="Para" id="Par17">SMC and SIVE are linked to the larger research field of <span id="ITerm10">HCI</span>, so it is common practice to adopt HCI research findings on how best to design systems. Below, Sect. <span class="InternalRef"><a href="#Sec5">6.2.2.1</a></span> describes two examples of how interaction methods are used in the design of VR for IAS. But as research in VR for SMC has developed, researchers have needed to define and collect design principles specific to sound and music in VR, this work is reviewed in Sect. <span class="InternalRef"><a href="#Sec6">6.2.2.2</a></span>.</p><section class="Section3 RenderAsSection3" id="Sec5"><h4 class="Heading"><span class="HeadingNumber">6.2.2.1 </span>Adapting Existing VR HCI Frameworks to Audio System Design</h4><div class="Para" id="Par18">To establish a dialogue around spatial considerations, there is a need to adopt findings from other VR HCI disciplines. But as with the adoption of HCI evaluation frameworks within new interfaces for musical expression (NIME) <span id="ITerm11"/> [<span class="CitationRef"><a epub:type="biblioref" href="#CR78" role="doc-biblioref">78</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR91" role="doc-biblioref">91</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR98" role="doc-biblioref">98</a></span>], critical understanding of the target domain (SMC) needs to be established [<span class="CitationRef"><a epub:type="biblioref" href="#CR70" role="doc-biblioref">70</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR81" role="doc-biblioref">81</a></span>]. For instance, making expressive systems for musical creation or sonic experiences has different design requirements than usability engineering [<span class="CitationRef"><a epub:type="biblioref" href="#CR42" role="doc-biblioref">42</a></span>], or demonstrations of interaction techniques [<span class="CitationRef"><a epub:type="biblioref" href="#CR8" role="doc-biblioref">8</a></span>]. This is not to say that usability engineering is not important, but rather the goal of design and evaluation needs to expand to include sonic aesthetic qualities for audio-first spatial scenarios.<figure class="Figure" id="Fig1"><div class="MediaObject" id="MO1"><img alt="" src="../images/478239_1_En_6_Chapter/478239_1_En_6_Fig1_HTML.png" style="width:31.58em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.1</span><p class="SimplePara">Selection and manipulation mechanics in VR</p></div></figcaption></figure>
</div><p class="Para ParaOneEmphasisChild" id="Par19"><strong class="EmphasisTypeBold ">Selection and Manipulation Techniques</strong></p><p class="Para" id="Par20">Object <span id="ITerm12">selection and manipulationis</span> fundamental to VR environments where users perform spatial tasks [<span class="CitationRef"><a epub:type="biblioref" href="#CR52" role="doc-biblioref">52</a></span>]. At a basic level, there are two main categories that describe <span id="ITerm13">3D interaction</span>for VR: Direct and indirect interaction techniques [<span class="CitationRef"><a epub:type="biblioref" href="#CR5" role="doc-biblioref">5</a></span>]. Object manipulation examples of direct and indirect techniques can be seen in Fig. <span class="InternalRef"><a href="#Fig1">6.1</a></span>. Direct interaction refers to having ‘virtual hands’; similar to touching and grabbing objects in the real world. A benefit of direct interaction is that control maps virtual tasks identically with real tasks, resulting in more natural interaction [<span class="CitationRef"><a epub:type="biblioref" href="#CR5" role="doc-biblioref">5</a></span>]. Indirect interaction refers to virtual pointing; like using a laser pointer (ray-casting) that can pickup and drop objects in space. Indirect interaction lets users select objects beyond their area of reach and require relatively less physical movement. Overcoming the physical constraints of the real world provides substantial benefits for the design of virtual spaces, as the arrangement of elements can expand beyond body-scaled interaction. Across both direct and indirect mechanics, interaction should be rapid, accurate, error proof, easy to understand and control, and aim for low levels of fatigue [<span class="CitationRef"><a epub:type="biblioref" href="#CR5" role="doc-biblioref">5</a></span>]. Depending on how they are designed, both direct and indirect interactions enable spatial transformations of objects, including rotation, scaling, and translation.</p><div class="Para" id="Par21">In adapting this research to sound and music interfaces, we must ask how techniques impact musical processes and practices. For example, [<span class="CitationRef"><a epub:type="biblioref" href="#CR13" role="doc-biblioref">13</a></span>] describes the trade-offs designers make when picking different control systems for virtual reality music instrument (VRMIs)<span id="ITerm14"/>. Work that has received less attention in SMC includes how to design for some of the unique properties of VR media. The affordances of VR expand into non-real interaction, so there is a fuzzy middle ground between direct and indirect interaction. For instance, the Go-Go technique enlarges a user’s limbs to be able to ‘touch’ distal objects [<span class="CitationRef"><a epub:type="biblioref" href="#CR74" role="doc-biblioref">74</a></span>]. In broader VR research, techniques like the Go-Go are described under the term homuncular flexibility [<span class="CitationRef"><a epub:type="biblioref" href="#CR93" role="doc-biblioref">93</a></span>]; the ability to augment proprioceptive perception of action capacity in VR, adapting interaction to include novel bodies that have extra appendages or appendages capable of atypical movements. An example of this type of research into IAS can be found in [<span class="CitationRef"><a epub:type="biblioref" href="#CR27" role="doc-biblioref">27</a></span>], where magical indirect <span id="ITerm15">interaction</span> was implemented to have audio control objects float towards the user based on pinch actions (via Leap Motion sensor attached to the HMD).<figure class="Figure" id="Fig2"><div class="MediaObject" id="MO2"><img alt="" src="../images/478239_1_En_6_Chapter/478239_1_En_6_Fig2_HTML.png" style="width:32em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.2</span><p class="SimplePara">Types of spatial UI for sound processes.</p><div class="Credit"><p class="SimplePara">Images from <span class="ExternalRef"><a href="http://blog.leapmotion.com/design-playground-3d-user-interfaces/"><span class="RefSource">Leap motion VR UI design sprint</span></a></span>, reproduced with permission from owner, <span class="ExternalRef"><a href="https://www.ultraleap.com/"><span class="RefSource">Ultraleap limited</span></a></span></p></div></div></figcaption></figure>
</div><p class="Para ParaOneEmphasisChild" id="Par22"><strong class="EmphasisTypeBold ">User Interface Elements</strong></p><p class="Para" id="Par23">Reviewing 3DUI for immersive <span id="ITerm16">music production</span>interfaces, [<span class="CitationRef"><a epub:type="biblioref" href="#CR11" role="doc-biblioref">11</a></span>] proposes three categories of representation for sound processes and parameters: Virtual sensors like buttons and sliders, dynamic/reactive widgets, spatial structures; Fig. <span class="InternalRef"><a href="#Fig2">6.2</a></span> provides examples. These different representation categories provide a set of design templates for audio production SUIs. For instance, fine-grained individual parameter control may be better suited to sensor devices with precise control relationships. Whereas, if spatio-visual feedback is required about an audio process being applied, a dynamic widget is a suitable device to explore. Spatial structures can be used to represent sequencers and relationships between parameters; as Sect. <span class="InternalRef"><a href="#Sec13">6.4</a></span> indicates later, several VR audio systems use these to represent either modular synthesis units or whole musical sequencers.</p></section><section class="Section3 RenderAsSection3" id="Sec6"><h4 class="Heading"><span class="HeadingNumber">6.2.2.2 </span>Audio-Specific Design Frameworks</h4><p class="Para" id="Par24">Design for IASs in VR is a developing field, surfacing the potential for new forms of sound and music experience [<span class="CitationRef"><a epub:type="biblioref" href="#CR20" role="doc-biblioref">20</a></span>]. But the opportunities and constraints of VR require critical analysis. For instance, embodied interfaces may offer benefits in productivity and creative expression [<span class="CitationRef"><a epub:type="biblioref" href="#CR62" role="doc-biblioref">62</a></span>], but we still do not know if the same effects are gathered by embodied interfaces in VR. Alongside this gap, there are gaps in design understanding, with only a few design frameworks addressing how to create VR interfaces and interactions for sound and music [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR11" role="doc-biblioref">11</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR77" role="doc-biblioref">77</a></span>]. Across these works, a deep level of design analysis around the fundamentals of perception, technology, and action is prevalent. But, in terms of design knowledge to aid designers conceptualising space, and the construction of audio interactions and experiences in it, information is limited. Below is a review of the spatial aspects implicated in the <span id="ITerm17">design guidelinesof</span> existing VR music system research.</p><p class="Para" id="Par25">Reviewing VRMI case studies, Serafin <em class="EmphasisTypeItalic ">et al.</em> outline nine principles to guide design, focusing on immersive visualisation from performers’ viewpoint [<span class="CitationRef"><a epub:type="biblioref" href="#CR77" role="doc-biblioref">77</a></span>]. Design principles support design focus on levels of abstraction, immersion, and imagination. Their review of works features many examples of hybrid virtual-physical systems and also highlights that VRMI are well suited to multi-process instruments given SUI affordances. Regarding system design their principles offer robust advice for musical performance but there is a lack of detail on how to go about designing different types of spaces and interactions. For instance, within the principles, an emphasis is put on making experiences social, but no guidance is provided on the design or evaluation of <span id="ITerm18">social experience</span>in VR. However, aspects of the case studies do draw attention to spatial factors such as menu design can ‘cloud’ the performance space; in large interfaces, the mixture of control device and interface design means arm movements and travel distances can be tiring; and the inclusion of physical control systems supports natural, body-based interaction.</p><div class="Para" id="Par26">Addressing <em class="EmphasisTypeItalic ">Artful Design</em> for VR sound interaction, Atherton and Wang describe a series of design lenses with subordinate principles using case study analysis [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>]. Their work focuses on the idea of creating totally immersive sonic VR. A central concept of their work is the difference between designing for <em class="EmphasisTypeItalic ">doing</em> as distinct from <em class="EmphasisTypeItalic ">being</em> in VR: <em class="EmphasisTypeItalic ">“doing is taking action with a purpose; intentionally acting to achieve an intended outcome. In contrast, we define being as the manner in which we inhabit the world around us”</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>]. Expanding on [<span class="CitationRef"><a epub:type="biblioref" href="#CR77" role="doc-biblioref">77</a></span>]’s suggestion to exploit the ‘magical’ opportunities of VR, Atherton and Wang highlight that designers should experiment with <em class="EmphasisTypeItalic ">virtual physics</em>, <em class="EmphasisTypeItalic ">scale and user perspective</em>, and <em class="EmphasisTypeItalic ">time</em>, however, these seem to be general principles for VR interaction rather than sound-specific opportunities. Within their discussions spatial concepts emerge, for instance, designers can phase levels interactivity to create different spaces for action in a scene. An actionable design idea relating to this is to guide gaze attention throughout a space related to narrative elements; want people to stop doing and slow down, just put something in the sky above them, as it is not an ideal place to work or interact. Atherton and Wang highlight that designers need to determine different languages of interaction. Design concepts should move beyond functional language towards things that map well to sonic expressions, e.g. instead of physical descriptors like speed of movement and gravity on an object, an interaction language would be <em class="EmphasisTypeItalic ">intensity</em> and <em class="EmphasisTypeItalic ">weight and weightlessness</em>. For Atherton and Wang, play, and particularly social play, is a synthesis of <em class="EmphasisTypeItalic ">doing</em> and <em class="EmphasisTypeItalic ">being</em>, as it is both an activity and a state. Designers can support play by: <div class="OrderedList"><ol><li class="ListItem"><div class="ItemNumber">1.</div><div class="ItemContent"><p class="Para" id="Par27">the lowering users’ inhibitions and encouraging them to play;</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">2.</div><div class="ItemContent"><p class="Para" id="Par28">engaging users in diverse movement;</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">3.</div><div class="ItemContent"><p class="Para" id="Par29">allowing users to be silly;</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">4.</div><div class="ItemContent"><p class="Para" id="Par30">making opportunities for discovery in virtual space.</p></div><div class="ClearBoth"> </div></li></ol></div>
</div><p class="Para" id="Par31">Related to play and interaction, on the social level, designers should provide sub-spaces within larger worlds and engineer collective interaction scenarios.</p></section></section><section class="Section2 RenderAsSection2" id="Sec7"><h3 class="Heading"><span class="HeadingNumber">6.2.3 </span>Typologies and Spatial Analysis</h3><div class="Para" id="Par32">A typology is a classification of individual units within a set of categories that are useful for a particular purpose. Typologies support the evaluation of a number of different indicators in an integrated manner, based on the identification of relevant links or themes. Within architecture, design typologies are a common method of spatio-visual analysis [<span class="CitationRef"><a epub:type="biblioref" href="#CR24" role="doc-biblioref">24</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR72" role="doc-biblioref">72</a></span>]. The teaching of architectural systems uses an ordered set of types to define areas of interlocking design [<span class="CitationRef"><a epub:type="biblioref" href="#CR22" role="doc-biblioref">22</a></span>], for instance, in Fig. <span class="InternalRef"><a href="#Fig3">6.3</a></span> the concept of form is described using a series of types and representative examples.<figure class="Figure" id="Fig3"><div class="MediaObject" id="MO3"><img alt="" src="../images/478239_1_En_6_Chapter/478239_1_En_6_Fig3_HTML.png" style="width:28.35em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.3</span><p class="SimplePara">Example of a spatial typology of form within architecture,</p><div class="Credit"><p class="SimplePara">adapted from [<span class="CitationRef"><a epub:type="biblioref" href="#CR22" role="doc-biblioref">22</a></span>]</p></div></div></figcaption></figure>
</div><div class="Para" id="Par33">But typologies can also represent ‘spatial qualities’ regarding interaction, see Fig. <span class="InternalRef"><a href="#Fig4">6.4</a></span> where different <span id="ITerm19">creative spaces</span>(meeting rooms, maker spaces) can possess positive and negative attributes for certain activities (socially inviting or separating, playful or serious) [<span class="CitationRef"><a epub:type="biblioref" href="#CR84" role="doc-biblioref">84</a></span>]. It is this interpretive layer within a set of similar objects that makes typologies a valuable analysis method. We can step out from just the formal representation of space and shape and ask, how does this form or behaviour impact human needs and experience.<figure class="Figure" id="Fig4"><div class="MediaObject" id="MO4"><img alt="" src="../images/478239_1_En_6_Chapter/478239_1_En_6_Fig4_HTML.png" style="width:32.15em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.4</span><p class="SimplePara">Example of a spatial typology within design,</p><div class="Credit"><p class="SimplePara">taken from [<span class="CitationRef"><a epub:type="biblioref" href="#CR84" role="doc-biblioref">84</a></span>]. Reprinted from design studies, 56, Thoring <em class="EmphasisTypeItalic ">et al.</em>, creative environments for design education and practice: A typology of creative spaces, 54–83, Copyright (2018), with permission from Elsevier and Katja Thoring</p></div></div></figcaption></figure>
</div><p class="Para" id="Par34">Compared to a systematic literature review, a <span id="ITerm20">design typology</span>includes references to artefacts regardless of whether it has received formal user evaluation or received previous research analysis. The reasoning is that much of the work happening in the VR music field is happening outside academia, so rather than reflecting design parameters only within previous academic dialogues, design understanding should also be based on practice.</p><p class="Para" id="Par35">Compared to a taxonomy, typology is preferred for this work, as the separation of types is non-hierarchical and potentially multi-faceted. Classification is done according to structural features, common characteristics, or other forms of patterns across instances. Within a typology, there is no implicit or explicit hierarchy connecting different research artefacts and products in VR. Also depending on the granularity of the type suggested, a single artefact may exist within two types simultaneously. Using typologies, themes of significance can be traced across systems, these patterns may describe best practices, observe patterns in interaction, explain good designs, or capture experience or insight so that other people can reuse these solutions.</p></section></section><section class="Section1 RenderAsSection1" id="Sec8"><h2 class="Heading"><span class="HeadingNumber">6.3 </span>Design Analysis</h2><section class="Section2 RenderAsSection2" id="Sec9"><h3 class="Heading"><span class="HeadingNumber">6.3.1 </span>Methodology</h3><p class="Para" id="Par36">As a formal process the typology was built upon identification, selection, and coding of audio-visual virtual spaces.</p><p class="Para" id="Par37"><em class="EmphasisTypeItalic ">Identification</em>: Literature gathering was achieved by parsing VR examples from the Musical XR literature dataset. Practice and product examples were gathered across the first author’s thesis research period using search engines, internet forums, interviews, and social media [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>].<sup><a epub:type="noteref" href="#Fn2" id="Fn2_source" role="doc-noteref">2</a></sup>
</p><p class="Para" id="Par39"><em class="EmphasisTypeItalic ">Selection</em>: Findings were assessed for relevance to the analysis. Cases were included on the basis of the following criteria; (1) Is the system based on immersive VR technology via an HMD? (2) Is the primary function or design intention of the artefact related to sound or music?</p><div class="Para" id="Par40"><em class="EmphasisTypeItalic ">Coding</em>: A form of deductive and inductive thematic coding was undertaken, based upon thematic analysis [<span class="CitationRef"><a epub:type="biblioref" href="#CR17" role="doc-biblioref">17</a></span>]. An inductive approach involves allowing the data to determine your themes, whereas a deductive approach involves coming to the data with some preconceived themes you expect to find reflected there, based on theory or existing knowledge. For this research, the deductive element was the setting of top-level coding categories (UI, Space Use, Social Engagement, Skill Level, Interactions) that probe how a VR IAS was constructed, the questions used are available in Table <span class="InternalRef"><a href="#Tab1">6.1</a></span>. The inductive coding reflects themes within the deductive categories based on the interface designs. Coding sources would involve: Use of the VR system where possible; review online video sources; analysis of images; and review of documentation and published literature. In each activity, notes and open-coding were undertaken on system design using qualitative data analysis software. After this, the deductive sweep was undertaken where the sources, open-codings and notes were reviewed in the context of each deductive category, and this resulted in the inductive themes that can be found in Table <span class="InternalRef"><a href="#Tab1">6.1</a></span>.<div class="Table" id="Tab1"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 6.1</span><p class="SimplePara">Coding system developed for typology. Bold codes indicate deductive code categories, italics are inductive themes</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1 align-left"/><col class="tcol2 align-left"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Code</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Description</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">UI</strong></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">What are the types of UI exploited in the VR interface?</strong></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Screen-like</em></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">3D or 2D UI is used in VR that behaves like a standard screen menu or workspace</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">3D Objects</em></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">3D UI is used for information and action</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">None</em></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">No conventional UI or SUI is provided to users, such as an open world terrain or an external musical controller</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Physical</em></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">No functional UI or SUI offered inside is VE, but external hardware musical controller used</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">Space Use</strong></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">How is space used in this device?</strong></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Sonic</em></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">The positions of people or objects in space has an impact on sound processing, space as a functional element of the sound design process</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Visual</em></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Interactive visual feedback provided based on positions or orientation in space of people or objects</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">Social Engagement</strong></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">What number of users was the application designed to support?</strong></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Solo</em></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Single user spaces with no intelligent-agent interaction</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Collaborative</em></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Multi-user or single/multi user with intelligent-agent interactions</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Collective</em></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Massive multiplayer environments, both human and agent-based</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">Skill Level</strong></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">Was the system designed for novices, experts or both?</strong></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Novice</em></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"> </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Expert</em></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"> </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">NA</em></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">No formal user study conducted</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">Interactions</strong></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><strong class="EmphasisTypeBold ">What is the flow of action and the related system response?</strong></p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Sonic-Visual</em></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Coupling between sound and visual features, where sound changes visual features</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Visual-sonic</em></p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Coupling between visual and auditory information, where the visual information changessound properties</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara"><em class="EmphasisTypeItalic ">Sonic-sonic</em></p></td><td style="text-align: left;"><p class="SimplePara">Audio input used to control system features that relate to sound</p></td></tr></tbody></table></div>
</div></section><section class="Section2 RenderAsSection2" id="Sec10"><h3 class="Heading"><span class="HeadingNumber">6.3.2 </span>Typology of Virtual Reality Interactive Audio Systems</h3><div class="Para" id="Par41">Here a typology of VR IASs is proposed, delineating how different systems overall function and the use of space in their design. The referencing of work in this section differentiates between commercial products and academic publications, using two different reference sections for clarity. The typology is split into two broad categories within which VR products and research are discussed: <div class="OrderedList"><ol><li class="ListItem"><div class="ItemNumber">1.</div><div class="ItemContent"><p class="Para" id="Par42">Type of Experience/Application—here we collate instances of products and research by their function as a sound and music system in VR.</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">2.</div><div class="ItemContent"><p class="Para" id="Par43">Role of Space—in this phase we look across the different types of systems to suggest how the design of space can be categorised.</p></div><div class="ClearBoth"> </div></li></ol></div>
</div><section class="Section3 RenderAsSection3" id="Sec11"><h4 class="Heading"><span class="HeadingNumber">6.3.2.1 </span>Type of Experience</h4><p class="Para" id="Par44">Most implementations of interactive VR sound and music systems fall into one or several of the categories in the subsequent list. Many cited products have no formal user testing results available.</p><div class="Para" id="Par45"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par46"><strong class="EmphasisTypeBold ">Audio-Visual Performance Environment</strong><span id="ITerm21"/>: Audience-oriented systems for playback or live performance of compositions with audio-visual interactions [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR101" role="doc-biblioref">101</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR109" role="doc-biblioref">109</a></span>]. For audience-oriented systems, interactivity is related to being part of a social group of spectators, rather than being able to interact sonically.</p></li><li><p class="Para" id="Par47"><strong class="EmphasisTypeBold ">Augmented Virtuality (AV)</strong>: A VR HMD acts as a visual output modality alongside physical controllers or smart objects, creating a AV system [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR100" role="doc-biblioref">100</a></span>]. This descriptor excludes augmented reality (AR) technologies, such as HoloLens, as the visual overlay effect is considered different to the total re-representation of visual stimuli that occur in VR [<span class="CitationRef"><a epub:type="biblioref" href="#CR99" role="doc-biblioref">99</a></span>].</p></li><li><p class="Para" id="Par48"><strong class="EmphasisTypeBold ">Collaborative</strong><span id="ITerm22"/>: Some form of collaborative interaction occurs in the VR audio system (human or agent-based). The interaction must be to directly make sound/music together [<span class="CitationRef"><a epub:type="biblioref" href="#CR12" role="doc-biblioref">12</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR63" role="doc-biblioref">63</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR103" role="doc-biblioref">103</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR110" role="doc-biblioref">110</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR119" role="doc-biblioref">119</a></span>], rather than more presentational systems like an audience cohabiting with performers in a virtual shared space; denoted by the <em class="EmphasisTypeItalic ">Audio-Visual Performance Environments</em> category. Examples and design considerations are described in Sect. <span class="InternalRef"><a href="#Sec13">6.4</a></span>.</p></li><li><p class="Para" id="Par49"><strong class="EmphasisTypeBold ">Conductor</strong>: Controlling audio-visual playback characteristics of pre-existing composition [<span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR117" role="doc-biblioref">117</a></span>].</p></li><li><p class="Para" id="Par50"><strong class="EmphasisTypeBold ">Control Surface</strong>: VR as a visual and interactive element to manipulate an existing digital audio workstation (DAWs) functionality, e.g., Reaper [<span class="CitationRef"><a epub:type="biblioref" href="#CR104" role="doc-biblioref">104</a></span>]. <span id="ITerm23"/>
</p></li><li><p class="Para" id="Par51"><strong class="EmphasisTypeBold ">Generative Music System</strong>: Partial or total <span id="ITerm24">algorithmic music composition</span>, where the sound is experienced in VR space, and/or controlled by spatial interaction in VR [<span class="CitationRef"><a epub:type="biblioref" href="#CR57" role="doc-biblioref">57</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR116" role="doc-biblioref">116</a></span>].</p></li><li><p class="Para" id="Par52"><strong class="EmphasisTypeBold ">Learning Interface</strong>:<span id="ITerm25"/> VR systems to support the learning of music, either as performance tutoring, theory, or general concepts in music such as genre [<span class="CitationRef"><a epub:type="biblioref" href="#CR48" role="doc-biblioref">48</a></span>].</p></li><li><p class="Para" id="Par53"><strong class="EmphasisTypeBold ">Music Game</strong><span id="ITerm26"/>: Systems where gameplay is oriented around the player’s interactions with a musical score or individual songs. A good example is Beat Sabre [<span class="CitationRef"><a epub:type="biblioref" href="#CR102" role="doc-biblioref">102</a></span>], the highest selling VR game of all time at the time of publication.</p></li><li><p class="Para" id="Par54"><strong class="EmphasisTypeBold ">Narrative and Soundscape</strong><span id="ITerm27"/>
<span id="ITerm28"/>: Pieces that integrate interactive audio in virtual reality [<span class="CitationRef"><a epub:type="biblioref" href="#CR85" role="doc-biblioref">85</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR116" role="doc-biblioref">116</a></span>].</p></li><li><p class="Para" id="Par55"><strong class="EmphasisTypeBold ">Physics Interaction</strong>: Physics-based sonic interaction systems [<span class="CitationRef"><a epub:type="biblioref" href="#CR27" role="doc-biblioref">27</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR106" role="doc-biblioref">106</a></span>].</p></li><li><p class="Para" id="Par56"><strong class="EmphasisTypeBold ">Sandbox</strong><sup><a epub:type="noteref" href="#Fn3" id="Fn3_source" role="doc-noteref">3</a></sup>:<span id="ITerm29"/> Designed like visual programming languages for digital sound synthesis—such as Pure Data, Max/MSP, and VCVRack—these VR sandboxes use patching together of modules to create sound. [<span class="CitationRef"><a epub:type="biblioref" href="#CR112" role="doc-biblioref">112</a></span>–<span class="CitationRef"><a epub:type="biblioref" href="#CR114" role="doc-biblioref">114</a></span>]</p></li><li><p class="Para" id="Par58"><strong class="EmphasisTypeBold ">Sequencer</strong>: Drum and music sequencers in VR. As sequencing is a common thing in many musical applications, this category refers to interfaces that are either just a sequencer or use sequencing somewhere within their interaction design [<span class="CitationRef"><a epub:type="biblioref" href="#CR27" role="doc-biblioref">27</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR63" role="doc-biblioref">63</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR103" role="doc-biblioref">103</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR110" role="doc-biblioref">110</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR112" role="doc-biblioref">112</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR119" role="doc-biblioref">119</a></span>].</p></li><li><p class="Para" id="Par59"><strong class="EmphasisTypeBold ">Spatial Audio Controller</strong>: Mixer style control of spatial audio characteristics of sources and effects [<span class="CitationRef"><a epub:type="biblioref" href="#CR9" role="doc-biblioref">9</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR27" role="doc-biblioref">27</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR69" role="doc-biblioref">69</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR90" role="doc-biblioref">90</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR104" role="doc-biblioref">104</a></span>].</p></li><li><p class="Para" id="Par60"><strong class="EmphasisTypeBold ">Sounding Object</strong>: Virtual object manipulation with parametric sound output [<span class="CitationRef"><a epub:type="biblioref" href="#CR67" role="doc-biblioref">67</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR68" role="doc-biblioref">68</a></span>].</p></li><li><p class="Para" id="Par61"><strong class="EmphasisTypeBold ">Scientific Instrument</strong>: VR systems designed to test an audio or interaction tool/feature, a good example is a VR-based binaural spatialisation evaluation system [<span class="CitationRef"><a epub:type="biblioref" href="#CR35" role="doc-biblioref">35</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>].</p></li><li><p class="Para" id="Par62"><strong class="EmphasisTypeBold ">VR DAW</strong>: Virtual audio environment, multi-process 3D interfaces for creation and manipulation of audio. Important feature is the recording of either audio or performance data from real-time interaction. Interface abstraction and control metaphors may differ significantly to conventional desktop DAWs [<span class="CitationRef"><a epub:type="biblioref" href="#CR12" role="doc-biblioref">12</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR27" role="doc-biblioref">27</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR88" role="doc-biblioref">88</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR103" role="doc-biblioref">103</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR110" role="doc-biblioref">110</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR119" role="doc-biblioref">119</a></span>].</p></li><li><p class="Para" id="Par63"><strong class="EmphasisTypeBold ">VRMI</strong>:<span id="ITerm30"/> Virtual modelling and representations of existing acoustic instruments or synthesis methods [<span class="CitationRef"><a epub:type="biblioref" href="#CR9" role="doc-biblioref">9</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR12" role="doc-biblioref">12</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR19" role="doc-biblioref">19</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR31" role="doc-biblioref">31</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR56" role="doc-biblioref">56</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR61" role="doc-biblioref">61</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR66" role="doc-biblioref">66</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR68" role="doc-biblioref">68</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR71" role="doc-biblioref">71</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR80" role="doc-biblioref">80</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR110" role="doc-biblioref">110</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR114" role="doc-biblioref">114</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR118" role="doc-biblioref">118</a></span>].</p></li></ul></div><strong class="EmphasisTypeBold ">Overlaps and Contrasts</strong></div><p class="Para" id="Par64">Due to the broad design scopes of some systems, an artefact can appear in multiple categories, or exist in a space between two categories. For instance, [<span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>] is in <em class="EmphasisTypeItalic ">Audio-Visual Performance Environments</em>, <em class="EmphasisTypeItalic ">Collaborative</em>, <em class="EmphasisTypeItalic ">Conductor</em>, and <em class="EmphasisTypeItalic ">VRMI</em>. While  [<span class="CitationRef"><a epub:type="biblioref" href="#CR12" role="doc-biblioref">12</a></span>] is a technically a <em class="EmphasisTypeItalic ">VR DAW</em>, the audio and interaction design concept is highly idiosyncratic, so it becomes closer to a <em class="EmphasisTypeItalic ">VRMI</em>. The following statements intend to clarify any issues regarding overlaps in terminology.</p><div class="Para" id="Par65"><div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par66"><em class="EmphasisTypeItalic ">Sounding Objects vs. Physics Interaction</em>: Both types refer to physics-based interactions, sounding objects are when the mesh structures of objects are the source of sound generation/control (e.g. scanned synthesis of an elastic mesh), whereas physics interactions include collision-based interactions for sound generation or use of physics systems to control single or multiple audio features (e.g. parameters or spatialisation). The interested reader might refer to Chap. <span class="ExternalRef"><a href="478239_1_En_2_Chapter.xhtml"><span class="RefSource">2</span></a></span> for more details on these topics.</p></li><li><p class="Para" id="Par67"><em class="EmphasisTypeItalic ">VRMI vs. Sandbox</em>: While both can refer to synthesis methods, sandboxes are specifically modular construction environments, whereas synthesis methods in VRMIs would be a closed form of synthesiser e.g. playing a DX7 emulator in virtual reality.</p></li></ul></div>
</div></section><section class="Section3 RenderAsSection3" id="Sec12"><h4 class="Heading"><span class="HeadingNumber">6.3.2.2 </span>Role of Space</h4><p class="Para" id="Par68">Many of the systems outlined above offer novel interaction methods coupled with 3D visualisation. Looking at how space is used in VR music and audio systems provides a different way to group research and design contributions. For simplicity, the following categories are presented as discrete areas, but dimensions would also be suitable (i.e. systems could belong to several categories, see [<span class="CitationRef"><a epub:type="biblioref" href="#CR15" role="doc-biblioref">15</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>] for examples of dimension-based classification for digital musical instrument (DMI)).</p><div class="Para" id="Par69"> <div class="DefinitionList"><dl><dt class="Term"><strong class="EmphasisTypeBold ">Space as a holder of elements for musical input/sonic control</strong></dt><dd class="Description"><p class="Para" id="Par70">The most dominant form of <span id="ITerm31">spatial designis</span> to use space as a container for interactive elements that either produce sound or control sound in some way. Within this category, key differences are whether menu-based SUI is used [<span class="CitationRef"><a epub:type="biblioref" href="#CR103" role="doc-biblioref">103</a></span>], or more object-based 3DUI is exploited [<span class="CitationRef"><a epub:type="biblioref" href="#CR12" role="doc-biblioref">12</a></span>]; this is discussed further in the next section. Other works include: [<span class="CitationRef"><a epub:type="biblioref" href="#CR19" role="doc-biblioref">19</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR27" role="doc-biblioref">27</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR31" role="doc-biblioref">31</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR56" role="doc-biblioref">56</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR61" role="doc-biblioref">61</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR63" role="doc-biblioref">63</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR66" role="doc-biblioref">66</a></span>–<span class="CitationRef"><a epub:type="biblioref" href="#CR69" role="doc-biblioref">69</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR71" role="doc-biblioref">71</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR88" role="doc-biblioref">88</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR100" role="doc-biblioref">100</a></span>]. [<span class="CitationRef"><a epub:type="biblioref" href="#CR104" role="doc-biblioref">104</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR109" role="doc-biblioref">109</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR110" role="doc-biblioref">110</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR112" role="doc-biblioref">112</a></span>–<span class="CitationRef"><a epub:type="biblioref" href="#CR114" role="doc-biblioref">114</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR118" role="doc-biblioref">118</a></span>]</p></dd><dt class="Term"><strong class="EmphasisTypeBold ">Space as a medium of <span id="ITerm32">sonic experience</span>
</strong></dt><dd class="Description"><p class="Para" id="Par71">In these sorts of systems, space is woven into every aspect of user experience or system design. For instance, in [<span class="CitationRef"><a epub:type="biblioref" href="#CR9" role="doc-biblioref">9</a></span>], the sonic operation of the VR system makes no sense if users do not engage in collaborative spatial behaviours [<span class="CitationRef"><a epub:type="biblioref" href="#CR9" role="doc-biblioref">9</a></span>]. In this category, the relationship of spatial interaction to system feedback can be predominantly passive, like a recorded soundscape [<span class="CitationRef"><a epub:type="biblioref" href="#CR85" role="doc-biblioref">85</a></span>], or fully interactive, like an audio-visual arts piece that maps spatial input to output modalities [<span class="CitationRef"><a epub:type="biblioref" href="#CR90" role="doc-biblioref">90</a></span>]. In some cases, visual space may only be a supporting medium for a spatial sonic experience [<span class="CitationRef"><a epub:type="biblioref" href="#CR85" role="doc-biblioref">85</a></span>]. It is worth noting that <strong class="EmphasisTypeBold ">spatial audio controllers</strong> are not instantly considered as part of this category. As <strong class="EmphasisTypeBold ">spatial audio controllers</strong> deal with controlling and manipulating elements, they are considered to be part of the <em class="EmphasisTypeItalic ">Space as a holder of elements for musical input/sonic control</em> category. Rather, this category holds experiences where spatiality is more intrinsically involved in the interaction between elements and user experience, whereas in a controller system it is a functional relationship. Other works include: [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR57" role="doc-biblioref">57</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR80" role="doc-biblioref">80</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR106" role="doc-biblioref">106</a></span>].</p></dd><dt class="Term"><strong class="EmphasisTypeBold ">Space as a visual resource to enhance musical performance</strong></dt><dd class="Description"><div class="Para" id="Par72">In this category, space is primarily used for its visual and spatial representation opportunities rather than as a direct control system or as an intrinsic part of the sonic experience derived from the system. Designers use space as an extra layer to a music performance or system, for example, this can be to: <div class="DefinitionList"><dl><dt class="Term">1.</dt><dd class="Description"><p class="Para" id="Par73">Present performers’ with enhanced visual feedback related to their Playing of a musical instrument [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>];</p></dd><dt class="Term">2.</dt><dd class="Description"><p class="Para" id="Par74">Provide a space for an audience to contribute to a collective experience of musical performance [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>]; or</p></dd><dt class="Term">3.</dt><dd class="Description"><p class="Para" id="Par75">Use space as a place for an audience to convene for a music performance in VR [<span class="CitationRef"><a epub:type="biblioref" href="#CR101" role="doc-biblioref">101</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR109" role="doc-biblioref">109</a></span>].</p></dd></dl></div>
</div></dd></dl></div>  </div></section></section></section><section class="Section1 RenderAsSection1" id="Sec13"><h2 class="Heading"><span class="HeadingNumber">6.4 </span>Spatial Design Analysis Case Studies</h2><div class="Para" id="Par76">The state of the art in VR audio production and immersive musical experiences include single-user and collaborative approaches. In the following case studies, the spatial and social design decisions are discussed; noting that each of the systems serves different purposes as musical experiences. Our motivation is to further detail design typology categories, by understanding and comparing the decisions VR designers make. Reviews are broken into four areas: <em class="EmphasisTypeItalic ">single user systems</em>, <em class="EmphasisTypeItalic ">collaborative systems</em>, <em class="EmphasisTypeItalic ">collective systems</em>, and <em class="EmphasisTypeItalic ">spatial audio production systems</em>. The reason we focus on these previous areas, only within immersive music and interactive sound production, is so that design comparisons and implications can have some level of shared context. We chose the field of <span id="ITerm33">immersive music</span>as a point of shared interest between academia and industry. But it would be valuable to probe design decisions comparatively between broader fields of SIVE design, for instance, auditory display and sound production systems; however, this would be a different contribution.<figure class="Figure" id="Fig5"><div class="MediaObject" id="MO5"><img alt="" src="../images/478239_1_En_6_Chapter/478239_1_En_6_Fig5_HTML.png" style="width:33.95em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.5</span><p class="SimplePara">Single-user VR spatial design considerations A—<em class="EmphasisTypeItalic ">music room</em> instrument space, with drum kit instrument being used and the recording panel UI visible, displaying previously recorded data</p></div></figcaption></figure>
</div><section class="Section2 RenderAsSection2" id="Sec14"><h3 class="Heading"><span class="HeadingNumber">6.4.1 </span>Single-User Systems</h3><div class="Para" id="Par77">Figure <span class="InternalRef"><a href="#Fig5">6.5</a></span> shows the <em class="EmphasisTypeItalic ">music room</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR118" role="doc-biblioref">118</a></span>], an <em class="EmphasisTypeItalic ">instrument space</em> containing multiple VRMIs that are designed to be played with the VR controllers, following a DAW-like workflow of perform and record, then arrange and edit. Instruments include a drum-kit, laser harp, pedal steel guitar and a chord harp. The spatial setup mimics a conventional studio. In Fig. <span class="InternalRef"><a href="#Fig5">6.5</a></span> we can see spatial 2D graphical user interface (GUIs) presenting recorded information and menu function, while 3DUI objects are used to represent instruments, and a 360 photograph of a real studio provides the visual backdrop. A design decision of the space was to situate all instruments in a circle around the user, presumably to be able to play all the VRMIs in a small physical space. Two areas are utilised for the UI, <em class="EmphasisTypeItalic ">action space</em> and <em class="EmphasisTypeItalic ">display space</em>. The action space is for the VRMIs, and the display space, further away from the user, provides a conventional GUI. To be able to interact with the distant GUI, laser pointers are used.<figure class="Figure" id="Fig6"><div class="MediaObject" id="MO6"><img alt="" src="../images/478239_1_En_6_Chapter/478239_1_En_6_Fig6_HTML.png" style="width:33.65em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.6</span><p class="SimplePara">Single-user VR spatial design considerations B—sandboxes, node-edge structures and modular systems</p></div></figcaption></figure>
</div><p class="Para" id="Par78"><em class="EmphasisTypeItalic ">Sound Stage</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR114" role="doc-biblioref">114</a></span>] (Fig. <span class="InternalRef"><a href="#Fig6">6.6</a></span>a) and <em class="EmphasisTypeItalic ">Mux</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR113" role="doc-biblioref">113</a></span>] are modular instrument building <em class="EmphasisTypeItalic ">Sandboxes</em> in VR. Users can define their own systems to perform music through those systems. Both are multi-process VRMIs designed for room-scale interaction. In these systems, a user surrounds themselves with modules and reactive widgets, and ‘patches’ them up using VR controllers. While stimulating and highly interactive, the resulting virtual spaces can be complex and messy spatial arrangements (author’s opinion); Fig. <span class="InternalRef"><a href="#Fig6">6.6</a></span>a shows an example of a sound system made with Mux, highlighting the spatial-visual complexity. One possible reason arrangements become complex is because spatial organisation is arbitrary and user-defined. A novel spatial feature is that speaker scale controls source loudness, and this turns a slider or number UI into a 3DUI interaction process.</p><div class="Para" id="Par79">The <em class="EmphasisTypeItalic ">LyraVR</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR112" role="doc-biblioref">112</a></span>] and <em class="EmphasisTypeItalic ">Drops</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR106" role="doc-biblioref">106</a></span>] are two examples of <em class="EmphasisTypeItalic ">Sandbox</em> <span id="ITerm34">systems</span> that build the temporal behaviour of the composition using spatial relationships. Figure <span class="InternalRef"><a href="#Fig6">6.6</a></span>b and c show <em class="EmphasisTypeItalic ">LyraVR</em> a musical ‘playground’ where users build music sequences in space to create audio-visual compositions. The node-based sequencer allows the creation of units in free space. Although aimed at single users, such interaction and playback method would be scalable to collaborative systems. <em class="EmphasisTypeItalic ">Drops</em> is a VR ‘rhythm garden’, where a user creates musical patterns using the interaction of objects and simulated gravity. The system requires setting up of object nodes (‘eggs’) that releases ‘marbles’ that make a sound when they strike other surfaces—the size and release frequency of marbles can be manipulated by the user. By adding more surfaces and modifying planes of movement for marbles, the musical composition is built using a ‘physical’ design process. In <em class="EmphasisTypeItalic ">LyraVR</em>, <em class="EmphasisTypeItalic ">Mux</em>, and <em class="EmphasisTypeItalic ">Sound Stage</em>, users interact with sound elements via spatial node-edge structures, and this gains a level of immediacy for musical changes at the cost of vision-spatial complexity. But the embodied control of temporal musical behaviour via the arbitrary positioning 3DUI does create an experimental creative process driven by interaction in space.<figure class="Figure" id="Fig7"><div class="MediaObject" id="MO7"><img alt="" src="../images/478239_1_En_6_Chapter/478239_1_En_6_Fig7_HTML.png" style="width:34.05em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.7</span><p class="SimplePara">Collaborative VR music production interfaces</p></div></figcaption></figure>
</div></section><section class="Section2 RenderAsSection2" id="Sec15"><h3 class="Heading"><span class="HeadingNumber">6.4.2 </span>Collaborative Systems</h3><p class="Para" id="Par80"><em class="EmphasisTypeItalic ">Block Rocking Beats</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR103" role="doc-biblioref">103</a></span>], <em class="EmphasisTypeItalic ">LeMo</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR63" role="doc-biblioref">63</a></span>], and <em class="EmphasisTypeItalic ">Polyadic</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>] are collaborative music making (CMM) <span id="ITerm35"> <em class="EmphasisTypeItalic ">Sequencers</em></span> <span id="ITerm36"/>. However, the systems have different approaches to spatial design for collaborative interaction. Both <em class="EmphasisTypeItalic ">LeMo</em> and <em class="EmphasisTypeItalic ">Polyadic</em> are the only collaborative systems in this review that have undergone formal user studies [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR63" role="doc-biblioref">63</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>].</p><p class="Para" id="Par81"><em class="EmphasisTypeItalic ">Block Rocking Beats</em>, Fig. <span class="InternalRef"><a href="#Fig7">6.7</a></span>a and b, enables avatar-based (head and hands only)<span id="ITerm37"/> remote collaborative music production in a virtual sound studio for up to three people. The space is modelled like a futuristic studio, adapting a conventional layout of production equipment areas and multiple screens. The environment provides a sequencer interface for each user while project information is displayed on a single large screen within the environment, and this provides some level of shared visual information. Additionally, reactive systems alter environment appearance in sync with music created. As a spatial layout, users’ positions are fixed in the space, a few meters from each other in a semi-circle facing the front screen. The layout limits the capacity to view each other’s workspaces and may inhibit forms of mutual monitoring. Regarding avatar design, the character’s design is highly stylised, and the ‘hand’ representation is designed like a tapered wand. The taper is designed to enlarge the usable sequencer area, as when buttons are designed at a normal scale the size of the controller would hit multiple buttons.</p><p class="Para" id="Par82">The <em class="EmphasisTypeItalic ">LeMo</em> allows two co-located users avatar-based CMM in VR, using a variety of sequencer instruments [<span class="CitationRef"><a epub:type="biblioref" href="#CR63" role="doc-biblioref">63</a></span>–<span class="CitationRef"><a epub:type="biblioref" href="#CR65" role="doc-biblioref">65</a></span>]. Depending on experimental condition, different spatial features would be activated, such as private workspace areas and spatially reactive loudness. Studies of <em class="EmphasisTypeItalic ">LeMo</em> evaluated visual and sonic workspace design, based on the concept of public and private territory, developing design implications for SVR; for detailed findings please consult Chap. <span class="ExternalRef"><a href="478239_1_En_8_Chapter.xhtml"><span class="RefSource">8</span></a></span> of this volume. Barring the experimental findings, as a spatial design, compared to <em class="EmphasisTypeItalic ">Block Rocking Beats</em> and <em class="EmphasisTypeItalic ">Polyadic</em>, <em class="EmphasisTypeItalic ">LeMo</em> allows users to move and rotate their workspaces to accommodate social interaction around the task of music making, commonly using face-to-face or side-by-side arrangements (see Fig. <span class="InternalRef"><a href="#Fig7">6.7</a></span>e). A novel design feature of note is that SUI sequencers can be minimised into ‘bubbles’ to rearrange space. As these sounds are spatially located, the bubble acts as both a UI and an audio object. Additionally, the inclusion of 3D drawing as a communication medium enables a variety of annotation behaviours. Like <em class="EmphasisTypeItalic ">Block Rocking Beats</em> and <em class="EmphasisTypeItalic ">Polyadic</em>, avatar design was rudimentary offering a head with gaze direction, however, the use of Leap Motion as the input device enables more detailed hand representations. These were used for functional input and social communication, e.g. waving and pointing.</p><p class="Para" id="Par83"><em class="EmphasisTypeItalic ">Polyadic</em> enables collaborative composition of drum loops to accompany backing tracks for two co-located participants [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>]. The system is designed to be instantiated in two user interface media, VR and Desktop. The design motivation of <em class="EmphasisTypeItalic ">Polyadic</em> was to compare VR and Desktop media concerning usability, creativity support, and collaboration. In order to create a fair comparison of media, constraints were imposed on the design of both media types. This limited the design of features to only use control methods that could work equally across both conditions, namely a standard step sequencer with per step volume and timing control. In the VR condition, the environment uses fixed placement of 3DUI sequencers made up of virtual sensor buttons and sliders, see Fig. <span class="InternalRef"><a href="#Fig7">6.7</a></span>f. Low fidelity avatars were utilised to allow rudimentary social cues. Avatars used a sphere head with ‘sunglasses’ to indicate gaze direction and two smaller spheres to indicate hands, enabling simple spatial referencing. Additionally, each user’s workspace and interface actions were replicated within the other users’ environment, enabling referencing and looking at what the other is doing.</p><p class="Para" id="Par84"><em class="EmphasisTypeItalic ">EXA</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR110" role="doc-biblioref">110</a></span>], Fig. <span class="InternalRef"><a href="#Fig7">6.7</a></span>d, is a collaborative <em class="EmphasisTypeItalic ">Instrument Space</em> where multiple users can compose, record, and perform music using instruments of their own design. <em class="EmphasisTypeItalic ">EXA</em> differs from the previous examples as users input musical sequence information in real time using drum-like instruments, rather than pressing sequencer buttons. Once sequences are made they can be edited using menus and button presses. Similar to <em class="EmphasisTypeItalic ">LeMo</em>, EXA allows users to freely organise their workspace in line with collaborative needs. Also, the custom design of VRMIs introduces idiosyncratic uses of space in order to perform each VRMIs. Like others, <em class="EmphasisTypeItalic ">EXA</em> utilises simple head and hands avatars.</p></section><section class="Section2 RenderAsSection2" id="Sec16"><h3 class="Heading"><span class="HeadingNumber">6.4.3 </span>Collective Systems</h3><div class="Para" id="Par85">The following reviews are special cases, social <span id="ITerm38">VR</span> platforms designed for <span id="ITerm39">musical experiences</span>, pictured in Fig. <span class="InternalRef"><a href="#Fig8">6.8</a></span>. As predominantly music visualisations in VR, there is limited sonic interactivity for users. So the focus is on how these spaces act as collective social experiences in VR. For broader discussion of music visualisation in XR, see [<span class="CitationRef"><a epub:type="biblioref" href="#CR92" role="doc-biblioref">92</a></span>]. While not sound production platforms in themselves, the experience of a collective engagement in VR, related to audio-visual performance, is an area of immersive entertainment where new production tools and design experience are required.<figure class="Figure" id="Fig8"><div class="MediaObject" id="MO8"><img alt="" src="../images/478239_1_En_6_Chapter/478239_1_En_6_Fig8_HTML.png" style="width:34.07em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.8</span><p class="SimplePara">Collective music experience spaces in VR</p></div></figcaption></figure>
</div><p class="Para" id="Par86"><em class="EmphasisTypeItalic ">The WaveVR</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR101" role="doc-biblioref">101</a></span>], Fig. <span class="InternalRef"><a href="#Fig8">6.8</a></span>a, is a cross-platform social VR experience, like going to a ‘gig’ in VR. Artists can use it to make audio-visual experiences for audiences across the world. As a virtual space, the shared focus of a stage is used for most performances, but the virtual space is reconfigured for each ‘gig’; similar to different theatre performances all taking place on the same stage. In one instance, music toy spaces were designed for the audience to interact with musical compositions, these took the form of objects that change the level of audio effects based on spatial position or touch interaction. As the objects cannot all be controlled by one person, this creates a collective ‘remix’ of the content [<span class="CitationRef"><a epub:type="biblioref" href="#CR111" role="doc-biblioref">111</a></span>]. For further reviews of some individual ‘gigs’ in <em class="EmphasisTypeItalic ">The WaveVR</em> see [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>].</p><p class="Para" id="Par87"><em class="EmphasisTypeItalic ">Volta</em> is an immersive experience creation and broadcasting system [<span class="CitationRef"><a epub:type="biblioref" href="#CR108" role="doc-biblioref">108</a></span>]. Performances are rendered in VR using artists’ existing tools and workflows, such as parameter mapping a DAW to drive visual feedback systems. In addition to the VR performance, a mixed reality (MR)<span id="ITerm40"/> experience is also broadcast to streaming platforms like Youtube and Twitch. Volta differs to <em class="EmphasisTypeItalic ">The WaveVR</em> in its production method for the artist. In <em class="EmphasisTypeItalic ">The WaveVR</em> developing a performance environment can take a development team months to build, and a significant cost. <em class="EmphasisTypeItalic ">Volta</em> cuts down production time by integrating existing tools with spatial experience design templates (e.g. particle systems), into a streamlined production process for real-time virtual performance environments.<sup><a epub:type="noteref" href="#Fn4" id="Fn4_source" role="doc-noteref">4</a></sup>
</p></section><section class="Section2 RenderAsSection2" id="Sec17"><h3 class="Heading"><span class="HeadingNumber">6.4.4 </span>Spatial Audio Production Systems</h3><div class="Para" id="Par89">In the following review of spatial audio production systems in VR, all systems use binaural spatial sound presented over headphones (Chaps. <span class="ExternalRef"><a href="478239_1_En_3_Chapter.xhtml"><span class="RefSource">3</span></a></span> and <span class="ExternalRef"><a href="478239_1_En_4_Chapter.xhtml"><span class="RefSource">4</span></a></span> provide an effective introduction to such audio <span id="ITerm41">technology</span>). It is possible for some of the systems (<em class="EmphasisTypeItalic ">DearVR Spatial Connect</em>, <em class="EmphasisTypeItalic ">ObjectsVR</em>) to be used with speaker arrays but the design implications of this are not considered in this review.<figure class="Figure" id="Fig9"><div class="MediaObject" id="MO9"><img alt="" src="../images/478239_1_En_6_Chapter/478239_1_En_6_Fig9_HTML.png" style="width:34.03em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.9</span><p class="SimplePara">VR spatial audio production systems</p></div></figcaption></figure>
</div><p class="Para" id="Par90">Addressing <span id="ITerm42">spatial audio production</span>, both the <em class="EmphasisTypeItalic ">Invoke</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>] &amp; <em class="EmphasisTypeItalic ">DearVR Spatial Connect</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR104" role="doc-biblioref">104</a></span>] systems allow users to record motion in VR to control sound objects. The main functional difference between the systems is that <em class="EmphasisTypeItalic ">DearVR Spatial Connect</em> uses a DAW to host the audio session with the VR system acting as a control layer for spatial and FX automation, while <em class="EmphasisTypeItalic ">Invoke</em> is a self-contained collaborative spatial audio mixing system. The systems also differ in their design approach to space and sonic interaction.</p><p class="Para" id="Par91">Figure <span class="InternalRef"><a href="#Fig9">6.9</a></span>a shows <em class="EmphasisTypeItalic ">Invoke</em>, a collaborative system that focuses on expressive spatial audio production using voice as an input method. The system utilises a mixture of direct and indirect spatial interaction to record spatial-sonic relationships. A <em class="EmphasisTypeItalic ">Voice Drawing</em> feature allows for the specification of spatio-temporal sonic behaviour in a continuous multimodal interaction. Voice input is recorded as loudness automation, while a drawn trajectory controls the location of the spatialised audio over time. Using an automated process the trajectory is segmented in a bézier curve with multiple control points for further collaborative manipulation. The UI design uses a mixture of 3DUI (audio objects, trajectories) and semi-transparent ‘screens-in-space’ (hand menus, world-space menus). Spatially, users can navigate the virtual space using <span id="ITerm43">teleport</span> functionality, all menus travel with the user when they teleport. <em class="EmphasisTypeItalic ">Invoke</em> is the only system in this review to implement more detailed <span id="ITerm44">avatar</span>design, each user is represented by a body, head and arms, utilising additional sensors on each user to provide accurate body-to-avatar positioning. This enabled detailed forms of social interaction and spatial <span id="ITerm45">awareness</span>[<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>].</p><div class="Para" id="Par92">Figure <span class="InternalRef"><a href="#Fig9">6.9</a></span>b shows <em class="EmphasisTypeItalic ">DearVR Spatial Connect</em>, a professional spatial audio production application. The system uses indirect interaction method to control objects in space; a laser pointer controls position while the VR controller thumb-stick controls distance from the centre. The design of the surrounding space adds no features beyond the interface panels and 3DUI (e.g. sound sources), as users commonly project a 360 video into the production space. Also, the user is ‘pinned’ to the centre of the space, again in line with the rendering perspective of spatial audio for 360 video. One issue of the central design is a lack of perspective on multiple objects that may be distant from the centre. Also, fatigue and motion noise (distant object ‘wobble’ more spatially) impact control of objects at a distance (dependent on input device design and user-based ergonomic factors like strength and motor control) [<span class="CitationRef"><a epub:type="biblioref" href="#CR5" role="doc-biblioref">5</a></span>]. Comparing this to <em class="EmphasisTypeItalic ">Invoke</em>, which does not constrain users to the central listening position when mixing audio objects, users can freely teleport around to gain different sonic and visual/interaction perspectives. This is important as the spatio-temporal mixing of sound creates a complex field of trajectories and sound objects [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>].<sup><a epub:type="noteref" href="#Fn5" id="Fn5_source" role="doc-noteref">5</a></sup>
<figure class="Figure" id="Fig10"><div class="MediaObject" id="MO10"><img alt="" src="../images/478239_1_En_6_Chapter/478239_1_En_6_Fig10_HTML.png" style="width:32.88em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.10</span><p class="SimplePara">ObjectsVR interface user interaction examples</p></div></figcaption></figure>
</div><p class="Para" id="Par94"><em class="EmphasisTypeItalic ">ObjectsVR</em> is a system for expressive interaction with spatial sound objects.</p><p class="Para" id="Par95">The system provides spatio-temporal interaction with electronic music using 3D geometric shapes and a series of novel interaction mappings, examples can be seen in Fig. <span class="InternalRef"><a href="#Fig10">6.10</a></span>. User hand control is provided via a Leap Motion, and the experience is rendered using a HMD. As a spatial audio control system, object positions were a mixture of direct manipulation and ‘magical’ physics-based interaction. Users could pick up and throw sounds around the space, but an orbiting mechanic meant that sound objects would always move back within grabbing distance. A novel spatial feature of this environment was the use of contextual UI when users grabbed certain objects. When a user grabbed objects that had 3D mappings, a 3D grid of points would appear to provide relative positioning guidance. When released the grid fades away. System design and evaluation investigate users’ natural exploration and probes the formation of understanding needed to interact creatively in VR, full details of the evaluation can be found in [<span class="CitationRef"><a epub:type="biblioref" href="#CR27" role="doc-biblioref">27</a></span>].<sup><a epub:type="noteref" href="#Fn6" id="Fn6_source" role="doc-noteref">6</a></sup>
</p></section></section><section class="Section1 RenderAsSection1" id="Sec18"><h2 class="Heading"><span class="HeadingNumber">6.5 </span>Discussion and Implications</h2><section class="Section2 RenderAsSection2" id="Sec19"><h3 class="Heading"><span class="HeadingNumber">6.5.1 </span>Spatial Design Considerations</h3><p class="Para" id="Par97">Consolidating the reviews of products and research, a series of design parameters <span id="ITerm46">emerge</span>.</p><p class="Para ParaOneEmphasisChild" id="Par98"><strong class="EmphasisTypeBold ">Complexity of spatial representation</strong></p><p class="Para" id="Par99">Based on the analysis of <em class="EmphasisTypeItalic ">Sandbox</em> systems (Mux and Sound Stage), it is suggested that an unrestricted patching metaphor may be too visually complex for applications like collaborative audio production in VR. Also, systems that build the timing of compositions in space, <em class="EmphasisTypeItalic ">LyraVR</em> &amp; <em class="EmphasisTypeItalic ">Drops</em>, suffer from spatial-visual complexity issues. Similar to visual programming languages [<span class="CitationRef"><a epub:type="biblioref" href="#CR36" role="doc-biblioref">36</a></span>], when all points of state-change are presented in one space (a low level of abstraction), the information becomes diffuse, and errors may become more frequent. Also, when space is used for functional relationships, like musical time, visual design cannot bracket the visual complexity without the design of abstractions. Related to these issues, the impacts of these design features is unknown for collaborative systems. Future research could design systems to observe spatial organisation patterns undertaken by users to make sense of, and work with arrangements.</p><p class="Para ParaOneEmphasisChild" id="Par100"><strong class="EmphasisTypeBold ">Screens-in-space and workspace zones</strong></p><p class="Para" id="Par101">For certain information (selection menus, settings, note sequences), systems use either conventional 2D information presentation in a floating screen (<em class="EmphasisTypeItalic ">Music Room, Block Rocking Beats, EXA, DearVR Spatial Connect</em>), or attempt to redesign information using forms of 3DUI (<em class="EmphasisTypeItalic ">Lyra, Mux</em>). Also, as described in the <em class="EmphasisTypeItalic ">Music Room</em> analysis, space can be delineated into different action or information presentation spaces. The decision to locate functionality in screens or more novel 3DUI is an important one for collaborative systems, as each different method offers different access points and levels of shared visual information for collaboration. For instance, in <em class="EmphasisTypeItalic ">LeMo</em>, each SUI could be minimised into a bubble for easy arrangement and organisation. A temptation of VR design could be to embody all interaction in ‘physical’ 3DUI, such as novel interaction widgets or spatially multiplexed 3DUI (see Fig. <span class="InternalRef"><a href="#Fig2">6.2</a></span>). But this could result in added spatio-visual complexity like in <em class="EmphasisTypeItalic ">Sandbox</em> systems, to deal with this there would be a need for contextual interaction layers (e.g. when I put a cube here its different from when I put it there), or function navigation using button combinations on controllers (VR 3D modelling software do this [<span class="CitationRef"><a epub:type="biblioref" href="#CR107" role="doc-biblioref">107</a></span>]). Another impact of using entirely 3DUI is that it could limit the amount of shared visual information, as arrangements of ‘physical’ objects naturally obscure each other. However, 3DUI may provide more access points to embodied collaboration.</p><p class="Para ParaOneEmphasisChild" id="Par102"><strong class="EmphasisTypeBold ">Level of acoustic spatial freedom</strong></p><p class="Para" id="Par103">Related to spatial audio the ability to move from the centre position is a key design decision that needs to be made, especially for collaborative audio production software. For single-user apps, being able to manipulate arrangements, away from the sweet spot is of value. For collaborative apps, multiple users located at the sweet spot would severely impact normal social interaction.</p><p class="Para ParaOneEmphasisChild" id="Par104"><strong class="EmphasisTypeBold ">Workspace organisation</strong></p><p class="Para" id="Par105">For workspace organisation, it should be considered whether fixed or movable UI is preferred for certain audio production tasks. For instance <em class="EmphasisTypeItalic ">LeMo</em>, <em class="EmphasisTypeItalic ">EXA</em>, and <em class="EmphasisTypeItalic ">Invoke</em> each utilised methods for users to reorganise the SUI, while artefacts like <em class="EmphasisTypeItalic ">Block Rocking Beats</em> and <em class="EmphasisTypeItalic ">Polyadic</em> did not.</p><p class="Para ParaOneEmphasisChild" id="Par106"><strong class="EmphasisTypeBold ">Control, Play and Expression</strong></p><p class="Para" id="Par107">Designers should consider how playful they make spatial audio experiences, or whether specific control and sound automation is the design target. For instance, in the <em class="EmphasisTypeItalic ">ObjectsVR</em> system spatial audio objects had ‘magical’ interaction, contrasting this, <em class="EmphasisTypeItalic ">DearVR Spatial Connect</em> emulates DAW automation. What is missing here is more examples of user experience in mixed systems, and environments to playfully explore spatial sound interactions with levels of direct control and serendipity. Related to making experience of control more expressive, integrating different modalities provides opportunities to expand on the DAW control paradigm, such as in <em class="EmphasisTypeItalic ">Invoke</em>.</p><p class="Para ParaOneEmphasisChild" id="Par108"><strong class="EmphasisTypeBold ">Egocentric spatial design</strong></p><p class="Para" id="Par109">Related to the previous two features, some systems (e.g. <em class="EmphasisTypeItalic ">Mux, Music Room</em>) tend towards egocentric spatial patterns, with devices and elements situated around the user, oriented to one spatial <span id="ITerm47">viewpoint</span>. While making sense for an individual application, these forms of design decisions need to be carefully considered in collaborative systems.</p><p class="Para ParaOneEmphasisChild" id="Par110"><strong class="EmphasisTypeBold ">Avatar Design</strong></p><p class="Para" id="Par111">An issue of importance to collaborative systems is <span id="ITerm48">avatardesign</span> and the spatial behaviours that they enable. For instance, inside <em class="EmphasisTypeItalic ">LeMo</em>, the use of the Leap Motion compared to standard VR controllers enabled more detailed forms of hand gesturing. Within HCI work has already begun to evaluate avatars based on the constraints of commercial VR [<span class="CitationRef"><a epub:type="biblioref" href="#CR53" role="doc-biblioref">53</a></span>]. What this area should focus on is moving beyond the so-called <em class="EmphasisTypeItalic ">Minimalist Immersion</em> in VR using only simplistic avatar design. Within <em class="EmphasisTypeItalic ">Invoke</em>, the avatar design utilised a more detailed body representation, offering beneficial characteristics for social space awareness, as users can interpret gaze and body orientations along with hand gestures. This highlights an important area of further research for collaborative and collective systems, where there should be detailed evaluations of the avatar designs’ impact music production activities.<sup><a epub:type="noteref" href="#Fn7" id="Fn7_source" role="doc-noteref">7</a></sup>
</p></section><section class="Section2 RenderAsSection2" id="Sec20"><h3 class="Heading"><span class="HeadingNumber">6.5.2 </span>Role of Space and Interaction</h3><div class="Para" id="Par113">Comparing the separation of the <em class="EmphasisTypeItalic ">Role of Space</em> with previous research on the <em class="EmphasisTypeItalic ">space of interaction</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR75" role="doc-biblioref">75</a></span>], similarities <span id="ITerm49">emerge</span>. River and MacTavish analyse space, time and information concepts within HCI across a series of paradigms [<span class="CitationRef"><a epub:type="biblioref" href="#CR75" role="doc-biblioref">75</a></span>]:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par114">Media Spaces [<span class="CitationRef"><a epub:type="biblioref" href="#CR86" role="doc-biblioref">86</a></span>]—media types</p></li><li><p class="Para" id="Par115">Windows, icons, menus, pointer (WIMP) [<span class="CitationRef"><a epub:type="biblioref" href="#CR47" role="doc-biblioref">47</a></span>]—user space management</p></li><li><p class="Para" id="Par116">Tangible user interface (TUI) [<span class="CitationRef"><a epub:type="biblioref" href="#CR44" role="doc-biblioref">44</a></span>]—space-body-thing interaction</p></li><li><p class="Para" id="Par117">Reality-based interaction (RBI) [<span class="CitationRef"><a epub:type="biblioref" href="#CR49" role="doc-biblioref">49</a></span>]—emerging embodied interaction <span id="ITerm50">styles</span>
</p></li><li><p class="Para" id="Par118">Information spaces [<span class="CitationRef"><a epub:type="biblioref" href="#CR10" role="doc-biblioref">10</a></span>]—interaction trajectories and navigation of information</p></li><li><p class="Para" id="Par119">Proxemic interactions [<span class="CitationRef"><a epub:type="biblioref" href="#CR37" role="doc-biblioref">37</a></span>]—social spatial <span id="ITerm51">relationships</span>
</p></li></ul></div>
</div><p class="Para" id="Par120">The key spatial dimensions that emerge are:</p><div class="Para" id="Par121"> <div class="DefinitionList"><dl><dt class="Term"><strong class="EmphasisTypeBold ">Dimension 1</strong></dt><dd class="Description"><p class="Para" id="Par122"><em class="EmphasisTypeItalic ">Media and Space Management</em> <span class="InlineEquation" id="IEq1"><img alt="$$\leftrightarrow $$" src="../images/478239_1_En_6_Chapter/478239_1_En_6_Chapter_TeX_IEq1.png" style="width:1.32em"/></span> <em class="EmphasisTypeItalic ">Meaning through interaction</em></p></dd><dt class="Term"><strong class="EmphasisTypeBold ">Dimension 2</strong></dt><dd class="Description"><p class="Para" id="Par123"><em class="EmphasisTypeItalic ">Personal and physical</em> <span class="InlineEquation" id="IEq2"><img alt="$$\leftrightarrow $$" src="../images/478239_1_En_6_Chapter/478239_1_En_6_Chapter_TeX_IEq2.png" style="width:1.32em"/></span> <em class="EmphasisTypeItalic ">Social and behavioural</em></p></dd></dl></div>  </div><div class="Para" id="Par124">Dimension 1 describes the difference between conventional GUI design (e.g. WIMP) versus approaches using space and the embodiment of technology (e.g. RBI). Dimension 1 relates to the previous analysis on the <em class="EmphasisTypeItalic ">Role of Space</em> (Sect. <span class="InternalRef"><a href="#Sec12">6.3.2.2</a></span>):<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par125">Space as a holder of elements for musical input/sonic control</p></li><li><p class="Para" id="Par126">Space as a medium of sonic experience</p></li><li><p class="Para" id="Par127">Space as a visual resource to enhance musical performance</p></li></ul></div>
</div><div class="Para" id="Par128">Dimension 2 highlights how space influences personal and social interactions. This is because information is distributed across technologies and is also embedded into contextual spaces, from immediate personal space through to social groups and larger collective social interaction spaces. Looking at these ideas together, a framework of research emerges for VR IAS spatial design. The functional uses of space in VR IAS relates to traditional understanding in the design of media types, user space management, and TUI. While <em class="EmphasisTypeItalic ">space as a medium of sonic experience</em> can benefit from research in the areas of RBI, and information spaces. Finally, proxemic interaction can inform things like social spaces for musical enhancement. But this doesn’t go far enough. What needs to be included in space for interactive audio is an understanding of architectural space. This is because VR designers must make important decisions regarding space as an element of user experience. Regarding social aspects, as highlighted earlier in Fig. <span class="InternalRef"><a href="#Fig4">6.4</a></span> [<span class="CitationRef"><a epub:type="biblioref" href="#CR84" role="doc-biblioref">84</a></span>], we can design space for functions, activities and for their spatial quality. We must design spaces for intimate individual action, shareable group interaction, and visibility and safety in large collective action spaces. Acoustically the sorts of choices we make here matter too. For example, using simple voice chat algorithms could make voice intelligibility poor and yield something similar to ‘zoom fatigue’ [<span class="CitationRef"><a epub:type="biblioref" href="#CR7" role="doc-biblioref">7</a></span>]. Instead, we can utilise spatially aware audio communications to deliver intelligible audio for each user in an area of space [<span class="CitationRef"><a epub:type="biblioref" href="#CR60" role="doc-biblioref">60</a></span>], a commercial approach to this already exists that can handle hundreds of listener-sources across a space [<span class="CitationRef"><a epub:type="biblioref" href="#CR115" role="doc-biblioref">115</a></span>].<figure class="Figure" id="Fig11"><div class="MediaObject" id="MO11"><img alt="" src="../images/478239_1_En_6_Chapter/478239_1_En_6_Fig11_HTML.png" style="width:31.58em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.11</span><p class="SimplePara">Spatial experience design in VR IAS Venn diagram</p></div></figcaption></figure>
</div><p class="Para" id="Par129">We suggest that spaces need elevated priority in our VR design and evaluation practices. To support this process, we suggest three top-level spatial categories that need to be addressed through interdisciplinary design work: <em class="EmphasisTypeItalic ">spaces/places, interfaces, interactions</em><span id="ITerm52"/>. Visualised in Fig. <span class="InternalRef"><a href="#Fig11">6.11</a></span>, some of the elements discussed in this chapter are positioned within the different design spaces; for instance, VR selection and manipulation techniques sit between <em class="EmphasisTypeItalic ">interfaces</em> and <em class="EmphasisTypeItalic ">interactions</em>. For brevity, only the category of <em class="EmphasisTypeItalic ">spaces/places</em> is discussed in detail below, as previous research within <em class="EmphasisTypeItalic ">interfaces</em> and <em class="EmphasisTypeItalic ">interactions</em> is already well documented in this chapter and other research [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR12" role="doc-biblioref">12</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR77" role="doc-biblioref">77</a></span>]. The categories scaffold future design by drawing together topics, theories, and previous art. Addressing elements that overlap with <em class="EmphasisTypeItalic ">spaces/places</em> in Fig. <span class="InternalRef"><a href="#Fig11">6.11</a></span>, we can use the Venn structure to ask new questions about the interaction of spaces in feature design. For instance, context-aware on body UI refers to the idea that if we have more specific spaces for interaction we can also tune the needs of UI to be relevant to that moment in space and time. The notion of putting it on our body, like a virtual smart watch, means that this design element is part of both <em class="EmphasisTypeItalic ">interfaces</em>, <em class="EmphasisTypeItalic ">interactions</em>, and <em class="EmphasisTypeItalic ">spaces/places</em>. Implicit in such simple categories is the equalising of spaces as a design concern alongside more thoroughly investigated work like spatial interfaces and spatial interaction. Fully describing such a framework is out-with the capacity of this chapter; instead, it is offered as a proposition for the research field to further explore together.</p><section class="Section3 RenderAsSection3" id="Sec21"><h4 class="Heading"><span class="HeadingNumber">6.5.2.1 </span>Spaces/Places</h4><div class="Para" id="Par130">Spaces are the architectural layouts and areas that form features of a virtual environment used for sound and music activities in VR. An example of a space can be seen in Fig. <span class="InternalRef"><a href="#Fig12">6.12</a></span>. In that figure a central production area is enclosed in a grid/cage structure, bounding it off from the wider spatial setting of floating ‘sand-dunes’ and night sky. But what does it mean to design for experience within space, and how does this related to an IAS? Borrowing from human geography and architecture [<span class="CitationRef"><a epub:type="biblioref" href="#CR22" role="doc-biblioref">22</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR87" role="doc-biblioref">87</a></span>], some spatial concepts to consider are: <div class="OrderedList"><ol><li class="ListItem"><div class="ItemNumber">1.</div><div class="ItemContent"><p class="Para" id="Par131">Boundaries;</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">2.</div><div class="ItemContent"><p class="Para" id="Par132">Form and space;</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">3.</div><div class="ItemContent"><p class="Para" id="Par133">Organisations and arrangements;</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">4.</div><div class="ItemContent"><p class="Para" id="Par134">Circulation (i.e. movement through space);</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">5.</div><div class="ItemContent"><p class="Para" id="Par135">Proportion and scale;</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">6.</div><div class="ItemContent"><p class="Para" id="Par136">Principles and metaphors (e.g. Symmetry, Hierarchy, Rhythm).</p></div><div class="ClearBoth"> </div></li></ol></div>
</div><p class="Para" id="Par137">Places are spaces with fixed or emergent social meaning [<span class="CitationRef"><a epub:type="biblioref" href="#CR32" role="doc-biblioref">32</a></span>]. We can aim to design the spatial qualities of spaces, for instance, the typology of [<span class="CitationRef"><a epub:type="biblioref" href="#CR84" role="doc-biblioref">84</a></span>] in Fig. <span class="InternalRef"><a href="#Fig4">6.4</a></span>, gives designers ways to conceptualise <span id="ITerm53">creative spaces</span>. We can ask, what is the space type (e.g. personal or collaborative), and what is the intended spatial quality (e.g. knowledge processor or process enabler)? Then we can ask, within those boundaries, what are other spatial characteristics i.e. comfort, sound, sight, spaciousness, movement, aliveness/animus?</p><div class="Para" id="Par138">As architecture, human geography, and interior design are such deep disciplines, interdisciplinary work needs to be done here to produce a dialogue around the design of space for sonic and musical expression. One area of mutual influence to consider is the design of immersive installations that involve technology to alter user experience. VR can learn from techniques and theories in this area [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>], as well as be used to prototype systems for physical installation.<figure class="Figure" id="Fig12"><div class="MediaObject" id="MO12"><img alt="" src="../images/478239_1_En_6_Chapter/478239_1_En_6_Fig12_HTML.png" style="width:33.95em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 6.12</span><p class="SimplePara">Example of a VR IAS space, <em class="EmphasisTypeItalic ">invoke</em> artefact’s spatial audio composition area</p></div></figcaption></figure>
</div></section></section></section><section class="Section1 RenderAsSection1" id="Sec22"><h2 class="Heading"><span class="HeadingNumber">6.6 </span>Research Directions and Opportunities</h2><section class="Section2 RenderAsSection2" id="Sec23"><h3 class="Heading"><span class="HeadingNumber">6.6.1 </span>Embodied Motion Design</h3><p class="Para" id="Par139">Echoing the design principles within Atherton and Wang’s work [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>], <span id="ITerm54">motion</span>, <span id="ITerm55">embodimentand</span> play are important design spaces to explore. However, human motion and spatial analysis is not a new discipline for computing and technology, with special research groups such as the International Conference on Movement and Computing (MOCO) and the ACM SIGGRAPH Conference on Motion, Interaction and Games (MIG). Within these existing dialogues, the role of embodiment is a central topic of design [<span class="CitationRef"><a epub:type="biblioref" href="#CR83" role="doc-biblioref">83</a></span>] (see Chap. <span class="ExternalRef"><a href="478239_1_En_7_Chapter.xhtml"><span class="RefSource">7</span></a></span> for further details). What would differ in virtual spaces is a form of synthesis, or symbiosis, between visual and proprioceptive embodiments. The plural is intentional, as virtual environments may introduce the idea that embodiment is not a fixed state, with avatars and motion feedback being augmented by the virtual setting. A research problem in this area is determining appropriate vocabularies for low-level and high-level motion so that systems of <span id="ITerm56">motion analysis</span>and mapping can be utilised in an informed way. But the difficulty in VR IAS is systems will often need to utilise data from only the headset and controllers, where many previous approaches have been developed using high-resolution <span id="ITerm57">motion capture</span>data [<span class="CitationRef"><a epub:type="biblioref" href="#CR29" role="doc-biblioref">29</a></span>]. Also, motion design is not just a single person experience. Take for instance dancing in a crowd. Research into virtual <em class="EmphasisTypeItalic ">togetherness</em> through joint embodied action is a rich direction for collaborative and collective systems to explore [<span class="CitationRef"><a epub:type="biblioref" href="#CR40" role="doc-biblioref">40</a></span>].</p></section><section class="Section2 RenderAsSection2" id="Sec24"><h3 class="Heading"><span class="HeadingNumber">6.6.2 </span>Designing for Collaborative Sound and Music in Virtual Reality</h3><p class="Para" id="Par140">There is a paucity of design and evaluation frameworks addressing social experiences in sound and music <span id="ITerm58">VR</span>. While work is ongoing in this area. For instance, Men and Bryan-Kinns’ chapter in this volume (Chap. <span class="ExternalRef"><a href="478239_1_En_8_Chapter.xhtml"><span class="RefSource">8</span></a></span>), to address the gap in design knowledge for VR, design perspectives from other embodied CMM and HCI research provide valid considerations for the design of SVR. The following integration of research from other fields intends to offer SMC actionable research directions to support <span id="ITerm59">collaboration</span> in VR.</p><p class="Para ParaOneEmphasisChild" id="Par141"><strong class="EmphasisTypeBold ">Adapting Tangible User Interface Research</strong></p><p class="Para" id="Par142">An area of potential influence on spatial design for social VR is to look at how TUIs are designed to support spatial <span id="ITerm60">collaboration</span>. For example, [<span class="CitationRef"><a epub:type="biblioref" href="#CR65" role="doc-biblioref">65</a></span>]’s research on CMM in VR shows similar results to co-located CMM using TUIs [<span class="CitationRef"><a epub:type="biblioref" href="#CR96" role="doc-biblioref">96</a></span>], regarding the design of public and private workspaces. When designing TUIs for co-located CMM, spatial orientation and configuration are important design areas. The <em class="EmphasisTypeItalic ">Hitmachine</em> is a tangible music-making tool for children, focused on creating and understanding collective interaction experiences [<span class="CitationRef"><a epub:type="biblioref" href="#CR38" role="doc-biblioref">38</a></span>]. To understand interactions with devices like the <em class="EmphasisTypeItalic ">Hitmachine</em>, there is a need to design social interactions and technology together. For designers, this means specifying and evaluating how people <em class="EmphasisTypeItalic ">distribute attention</em>, <em class="EmphasisTypeItalic ">share attention</em>, <em class="EmphasisTypeItalic ">dialogue</em>, and engage in <em class="EmphasisTypeItalic ">collective action</em>. To analyse designs in context, spatial formations of peoples’ positions and orientations can be analysed to understand different constructions of social play in CMM [<span class="CitationRef"><a epub:type="biblioref" href="#CR38" role="doc-biblioref">38</a></span>]. Observations of social engagement around <em class="EmphasisTypeItalic ">Hitmachine</em> found that the configuration of space (people, furniture, and music interfaces) altered the level of social interaction. Also, regarding the design of space in VR, research findings from VR CMM resemble the results from the <em class="EmphasisTypeItalic ">Hitmachine</em> analysis [<span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>]: How spatial encounters are set up for music interaction impact social interaction. So, to design collective interaction spaces, how basic spatial partitions are implemented matters.</p><p class="Para" id="Par143">Another TUI design principle of relevance is to provide multiple access points to a collaborative task [<span class="CitationRef"><a epub:type="biblioref" href="#CR45" role="doc-biblioref">45</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR76" role="doc-biblioref">76</a></span>]. This means devising multiple spatial ways for different users to act on the same object, creating a form of DUI. Research suggests that the more access points participants have to a collaborative task improves how equitable participation is [<span class="CitationRef"><a epub:type="biblioref" href="#CR76" role="doc-biblioref">76</a></span>]. Increasing the tangibility is also said to improve participation. This is because users can complement what each other are doing in spatial tasks, using space as an organiser of the shared activity [<span class="CitationRef"><a epub:type="biblioref" href="#CR76" role="doc-biblioref">76</a></span>]. Adapting tangibility to VR means designing the affordances of objects appropriately to allow collective spatial interaction, while keeping in mind that we can move beyond some of the constraints embedded in physical reality. A good example of this is in VR <em class="EmphasisTypeItalic ">Sandboxes</em>. In physical reality, physics governs layout patterns of blocks whereas in VR elements can be placed in any part of 3D space. This in turn impacts the design of modules how users connect them [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>]. But as mentioned previously, idiosyncratic design patterns within <em class="EmphasisTypeItalic ">Sandboxes</em> may need additional support for collaboration, and this is where previous TUI work could be integrated [<span class="CitationRef"><a epub:type="biblioref" href="#CR97" role="doc-biblioref">97</a></span>].</p><p class="Para" id="Par144">Collectively, these similarities suggest that as a form of spatial collaboration, VR CMM can benefit from other non-VR research findings regarding spatial interaction to design systems. But, directly importing collaborative design concepts from other media should be done carefully, and thoroughly evaluated for any differences in results across media (see [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>] for a media comparison study focusing on this).</p><p class="Para ParaOneEmphasisChild" id="Par145"><strong class="EmphasisTypeBold ">Designing for Embodiment in Collaboration</strong></p><p class="Para" id="Par146">Embodied spatial input and avatar representation are key features of VR for supporting intimacy [<span class="CitationRef"><a epub:type="biblioref" href="#CR54" role="doc-biblioref">54</a></span>], awareness and coordination [<span class="CitationRef"><a epub:type="biblioref" href="#CR41" role="doc-biblioref">41</a></span>], and control [<span class="CitationRef"><a epub:type="biblioref" href="#CR1" role="doc-biblioref">1</a></span>]. Spatial media, such as VR, has the capacity for visual and spatial abstraction of UI, something needed for the complex requirements of expert music production [<span class="CitationRef"><a epub:type="biblioref" href="#CR28" role="doc-biblioref">28</a></span>]. The following examples highlight some specific opportunities to support spatial collaboration.</p><div class="Para" id="Par147"> <div class="DefinitionList"><dl><dt class="Term"><strong class="EmphasisTypeBold ">Augmented Object Interaction</strong></dt><dd class="Description"><p class="Para" id="Par148">The affordances of embodied interaction in SUI offer possibilities to transform how joint action on complex digital objects can occur [<span class="CitationRef"><a epub:type="biblioref" href="#CR1" role="doc-biblioref">1</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR2" role="doc-biblioref">2</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR8" role="doc-biblioref">8</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR21" role="doc-biblioref">21</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR55" role="doc-biblioref">55</a></span>].</p></dd><dt class="Term"><strong class="EmphasisTypeBold ">Awareness Support</strong></dt><dd class="Description"><p class="Para" id="Par149">Embodied control and spatial representation in VR can ameliorate mutual understanding issues in shared workspaces compared to other media [<span class="CitationRef"><a epub:type="biblioref" href="#CR79" role="doc-biblioref">79</a></span>]; support informal awareness to co-ordinate actions given shared visual information [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>]; provide pointer mechanisms that support referencing of content and environmental objects [<span class="CitationRef"><a epub:type="biblioref" href="#CR23" role="doc-biblioref">23</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR94" role="doc-biblioref">94</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR95" role="doc-biblioref">95</a></span>]; allow for the recording of embodied motion, as a form of embodied memory within an environment [<span class="CitationRef"><a epub:type="biblioref" href="#CR58" role="doc-biblioref">58</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR63" role="doc-biblioref">63</a></span>]; provide novel mechanisms for the division of labour and workspace organisation [<span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>].</p></dd><dt class="Term"><strong class="EmphasisTypeBold ">Spatial Problems</strong></dt><dd class="Description"><p class="Para" id="Par150">Space is a powerful organiser of human memory and can change how we solve problems [<span class="CitationRef"><a epub:type="biblioref" href="#CR18" role="doc-biblioref">18</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR50" role="doc-biblioref">50</a></span>], and VR, compared to WIMP systems, is suggested to alter problem-solving strategy in spatial tasks [<span class="CitationRef"><a epub:type="biblioref" href="#CR50" role="doc-biblioref">50</a></span>].</p></dd></dl></div>  </div><p class="Para" id="Par151">These considerations have in common an influence on the interaction space in collaboration. This suggests that the collaborative process in sound and music production could be improved by designing support for <span id="ITerm61">augmented interaction</span>and <span id="ITerm62">awareness</span>. For example, in a common studio environment, usually, a shared screen (or set of screens), a keyboard and mouse, mixing desk with dedicated audio outboards, are the tools in the hands of audio producers. In contrast, in an embodied VR interface, the possible interaction space can centre around collaborative spaces where functionality is engineered to support mutual access and modification, adapting levels of visibility and position based on collaborative needs.</p></section><section class="Section2 RenderAsSection2" id="Sec25"><h3 class="Heading"><span class="HeadingNumber">6.6.3 </span>Spatial Audio Production for Immersive Entertainment</h3><p class="Para" id="Par152">VR provides an ostensibly promising environment for spatial audio production, it is an example of professional workflow that could benefit from further research into interaction methods in VR. The spatial nature of the technology, and action in it, could support problem issues encountered when making audio compositions in space (e.g. transformation of spatial reference frames between self and audience) [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>]. Regarding the previous analysis, a highly significant research area would be the management of complexity in the information design of spatial representation. The impact of these improvements would be felt within fields such as immersive entertainment, where spatial audio technologies allow the engineering of soundscapes that represent real or imagined sonic worlds, using the location of sounds in space as a critical component of audience experience. In particular, there is an under-explored research opportunity in VR to enable more collaborative practice for <span id="ITerm63">spatial audio production</span>. This addresses a need in professional audio production communities that look to make content for immersive entertainment.<sup><a epub:type="noteref" href="#Fn8" id="Fn8_source" role="doc-noteref">8</a></sup>
</p></section></section><section class="Section1 RenderAsSection1" id="Sec26"><h2 class="Heading"><span class="HeadingNumber">6.7 </span>Conclusion</h2><p class="Para" id="Par154">Much of how we design VR is based on borrowed design principles. We import ideas from other disciplines and hope they ‘fit’. But to capitalise on any opportunities for enhanced expression and new forms of sonic entertainment presented by VR, we must set out how we design, what that involves, and what that excludes. Given such a broad focus embedded in the concept of space, the first goal of any schematic representation of design types and guidelines is to find suitable descriptors to collect the features relevant to domains of research. For researchers, this means setting out the design rationale behind systems clearly, so that over time we can understand the emerging practice and propose novel directions. This research offered the beginning of this process for the design of IAS for VR, setting out the different functional types both research and commercial interests pursue while reflecting on the way space is implicated in their design. This provides a framework for spatial design, highlighting a set of actionable areas for future design research. From our perspective, a key missing piece is guidance about how to design spatial social experiences in VR for engagement with sound and music. We need to define the transitions between individual, collaborative and collective interaction when it comes to audio interaction. A stepping stone in this gap is more research into avatar design for SIVE, as to start assessing spatial transitions in social activity we need to understand virtual embodiment as the vessel that affords basic social communication beyond speech. Looking forward, we should begin to think about what it means to be an immersive application designer that is audio-first. Realising that practice will need to integrate concepts from acoustics, architecture, phenomenology, HCI and SMC, this calls us to think about transdisciplinary pedagogical models to support development in the field.</p></section><div class="License LicenseSubType-cc-by"><a href="https://creativecommons.org/licenses/by/4.0"><img alt="Creative Commons" src="../css/cc-by.png"/></a><p class="SimplePara"><strong class="EmphasisTypeBold ">Open Access</strong> This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (<span class="ExternalRef"><a href="http://creativecommons.org/licenses/by/4.0/"><span class="RefSource">http://​creativecommons.​org/​licenses/​by/​4.​0/​</span></a></span>), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p><p class="SimplePara">The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p></div><aside aria-labelledby="Bib1Heading" class="Bibliography" id="Bib1"><div epub:type="bibliography" role="doc-bibliography"><div class="Heading" id="Bib1Heading">References</div><ol class="BibliographyWrapper"><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">1.</div><div class="CitationContent" id="CR1">Aguerreche, L., Duval, T., Lécuyer, A.: Comparison of Three Interactive Techniques for Collaborative Manipulation of Objects in Virtual Reality in CGI (Computer Graphics International) (Singapore, 2010).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">2.</div><div class="CitationContent" id="CR2">Aguerreche, L., Duval, T., Lécuyer, A.: Reconfigurable Tangible Devices for 3D Virtual Object Manipulation by Single or Multiple Users. Proceedings of the 17th ACM Symposium on Virtual Reality Software and Technology, 227–230 (2010).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">3.</div><div class="CitationContent" id="CR3">Akpan, I., Marshall, P., Bird, J., Harrison, D.: Exploring the effects of space and place on engagement with an interactive installation in ACM Conference on Human Factors in Computing Systems (CHI) (ACM Press, New York, New York, USA, Apr. 2013), 2213.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">4.</div><div class="CitationContent" id="CR4">Andersson, N., Erkut, C., Serafin, S.: Immersive Audio Programming in a Virtual Reality Sandbox English. in Audio Engineering Society Conference:2019 AES International Conference on Immersive and Interactive Audio (Audio Engineering Society, Mar. 2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">5.</div><div class="CitationContent" id="CR5">Argelaguet, F., Andujar, C.: A survey of 3D object selection techniques for virtual environments. Computers and Graphics (Pergamon) <strong class="EmphasisTypeBold ">37</strong>, 121–136 (2013).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">6.</div><div class="CitationContent" id="CR6">Atherton, J., Wang, G.: Doing vs. Being: A philosophy of design for artful VR. Journal of New Music Research <strong class="EmphasisTypeBold ">49</strong>, 35–59 (2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">7.</div><div class="CitationContent" id="CR7">Bailenson, J.N.: Nonverbal Overload:ATheoretical Argument for the Causes of Zoom Fatigue. Technology, Mind, and Behavior 2 (Feb. 23, 2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">8.</div><div class="CitationContent" id="CR8">Baron, N.: CollaborativeConstraint : UI for Collaborative 3D Manipulation Operations in IEEE Symposium on 3D User Interfaces (2016), 269–270.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">9.</div><div class="CitationContent" id="CR9">Barrass, S., Barrass, T.: Musical creativity in collaborative virtual environments. Virtual Reality <strong class="EmphasisTypeBold ">10</strong>, 149–157 (2006).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">10.</div><div class="CitationContent" id="CR10">Benyon, D., Höök, K., Nigay, L.: Spaces of Interaction in Proceedings of the 2010 ACM-BCS Visions of Computer Science Conference (BCS Learning &amp; Development Ltd., Swindon, GBR, Apr. 2010), 1–7.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">11.</div><div class="CitationContent" id="CR11">Berthaut, F.: 3D interaction techniques for musical expression. Journal of New Music Research <strong class="EmphasisTypeBold ">49</strong>, 60–72 (2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">12.</div><div class="CitationContent" id="CR12">Berthaut, F., Desainte-Catherine, M., Hachet, M.: DRILE: an immersive environment for hierarchical live-looping in Proceedings of the International Conference on New Interfaces for Musical Expression (NIME) (2010), 192–197.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">13.</div><div class="CitationContent" id="CR13">Berthaut, F., Hachety, M., Desainte-Catherine, M.: Piivert: Percussion-based interaction for immersive virtual environments. IEEE Symposium on 3D User Interfaces (3DUI), 15–18 (2010).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">14.</div><div class="CitationContent" id="CR14">Berthaut, F., Martinez, D., Hachet, M.: Reflets : Combining and Revealing Spaces for Musical Performances. Proceedings of the International Conference on New Interfaces for Musical Expression, 116–120 (2015).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">15.</div><div class="CitationContent" id="CR15">Birnbaum, D., Fiebrink, R., Malloch, J.,Wanderley, M. M.: Towards a dimension space for musical devices in Proceedings of the International Conference on New Interfaces for Musical Expression (NIME) (2005), 192–195.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">16.</div><div class="CitationContent" id="CR16">Bowman, D. a. et al.: New Directions in 3D User Interfaces. International Journal <strong class="EmphasisTypeBold ">5</strong>, 3–14 (2006).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">17.</div><div class="CitationContent" id="CR17">Braun, V., Clarke, V.: Using thematic analysis in psychology. Qualitative research in psychology <strong class="EmphasisTypeBold ">3</strong>, 77–101 (2006).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">18.</div><div class="CitationContent" id="CR18">Burgess,N.: Spatial memory: how egocentric and allocentric combine. Trends in Cognitive Sciences <strong class="EmphasisTypeBold ">10</strong>, 551–557 (2006).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">19.</div><div class="CitationContent" id="CR19">Cabral, M. et al.: Crosscale: A 3D virtual musical instrument interface in 2015 IEEE Symposium on 3D User Interfaces (3DUI) (Mar. 2015), 199–200.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">20.</div><div class="CitationContent" id="CR20">Çamcı, A., Hamilton, R.: Audio-first VR: New perspectives on musical experiences in virtual environments. Journal of New Music Research <strong class="EmphasisTypeBold ">49</strong>, 1–7 (2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">21.</div><div class="CitationContent" id="CR21">Chénéchal, M. L., Lacoche, J.: When the Giant meets the Ant An Asymmetric Approach for Collaborative and Concurrent Object Manipulation in a Multi-Scale Environment. IEEE Symposium on 3D User Interfaces, 277–278 (2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">22.</div><div class="CitationContent" id="CR22">Ching, F. D. K.: Architecture: Form, Space, &amp; Order Fourth edition (Wiley, Hoboken, New Jersey, 2015).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">23.</div><div class="CitationContent" id="CR23">Cockburn, A., Quinn, P., Gutwin, C., Ramos, G., Looser, J.: Air pointing: Design and evaluation of spatial target acquisition with and without visual feedback. International Journal of Human-Computer Studies <strong class="EmphasisTypeBold ">69</strong>, 401–414 (2011).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">24.</div><div class="CitationContent" id="CR24">Colquhoun, A.: Typology and Design Method. Perspecta, 71–74 (1969).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">25.</div><div class="CitationContent" id="CR25">Deacon, T.: Shaping Sounds in Space: Exploring the Design of Collaborative Virtual Reality Audio Production Tools PhD thesis (Queen Mary University of London, 2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">26.</div><div class="CitationContent" id="CR26">Deacon, T., Bryan-Kinns, N., Healey, P. G., Barthet, M.: Shaping sounds: The role of gesture in collaborative spatial music composition in Creativity and Cognition (ACM, San Diego, 2019), 121–132.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">27.</div><div class="CitationContent" id="CR27">Deacon, T., Stockman, T., Barthet, M. in Bridging People and Sound: 12th International Symposium, CMMR 2016, Sáo Paulo, Brazil, July 5–8, 2016, Revised Selected Papers (eds Aramaki, M., Kronland-Martinet, R., Ystad, S.) vol 10525, 192–216 (Springer International Publishing, Cham, 2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">28.</div><div class="CitationContent" id="CR28">Duignan, M., Noble, J., Biddle, R.: Abstraction and Activity in Computer- Mediated Music Production. Computer Music Journal <strong class="EmphasisTypeBold ">34</strong>, 22–33 (2010).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">29.</div><div class="CitationContent" id="CR29">Durupinar, F., Kapadia, M., Deutsch, S., Neff, M., Badler, N. I.: PERFORM: Perceptual Approach for Adding OCEAN Personality to Human Motion Using Laban Movement Analysis. ACM Transactions on Graphics <strong class="EmphasisTypeBold ">36</strong>, 6:1–6:16 (Oct. 2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">30.</div><div class="CitationContent" id="CR30">Ens, B. et al.: Revisiting collaboration through mixed reality: The evolution of groupware. Computer Supported Cooperative Work <strong class="EmphasisTypeBold ">131</strong>, 81–98 (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">31.</div><div class="CitationContent" id="CR31">Fillwalk, J.: ChromaChord : A Virtual Musical Instrument in 2015 IEEE Symposium on 3D User Interfaces, 3DUI 2015 - Proceedings (2015), 201–202.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">32.</div><div class="CitationContent" id="CR32">Gardair, C., Healey, P. G. T., Welton, M.: Performing places. Proceedings of the 8th ACM conference on Creativity and cognition - C &amp;C ’11, 51 (2011).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">33.</div><div class="CitationContent" id="CR33">Gaver, W. W.: What Should We Expect From Research Through Design? In ACM Conference on Human Factors in Computing Systems (CHI) (2012), 937–946.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">34.</div><div class="CitationContent" id="CR34">Gelineck, S., Böttcher, N., Martinussen, L., Serafin, S.: Virtual Reality Instruments capable of changing Dimensions in Real-time in Enactive (2005).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">35.</div><div class="CitationContent" id="CR35">Geronazzo, M. et al.: The Impact of an Accurate Vertical Localization with HRTFs on Short Explorations of ImmersiveVirtual Reality Scenarios in 2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) (Oct. 2018), 90–97.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">36.</div><div class="CitationContent" id="CR36">Green, T. R. G., Petre, M.: Usability Analysis of Visual Programming Environments: A ’Cognitive Dimensions’ Framework. Journal of Visual Languages and Computing <strong class="EmphasisTypeBold ">7</strong>, 131–174 (1996).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">37.</div><div class="CitationContent" id="CR37">Greenberg, S., Marquardt, N., Ballendat, T., Diaz-Marino, R., Wang, M.: Proxemic Interactions: The New Ubicomp? Interactions <strong class="EmphasisTypeBold ">18</strong>, 42–50 (Jan. 2011).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">38.</div><div class="CitationContent" id="CR38">Grønbæk, J. E. et al.: Designing for Children’s Collective Music Making: How Spatial Orientation and Configuration Matter in Nordic Conference on Human-Computer Interaction (NordiCHI) (2016), 23–27.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">39.</div><div class="CitationContent" id="CR39">Hattwick, I., Wanderley, M. M.: A Dimension Space for Evaluating Collaborative Musical Performance Systems in Proceedings of the International Conference on New Interfaces for Musical Expression (NIME) (2012), 429–432.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">40.</div><div class="CitationContent" id="CR40">Himberg, T., Laroche, J., Bigé, R., Buchkowski, M., Bachrach, A.: Coordinated Interpersonal Behaviour in Collective Dance Improvisation: The Aesthetics of Kinaesthetic Togetherness. en. Behavioral Sciences 8, 23 (Feb. 2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">41.</div><div class="CitationContent" id="CR41">Hindmarsh, J., Fraser, M., Heath, C., Benford, S., Greenhalgh, C.: Object focused interaction in collaborative virtual environments. ACM Transactions on Computer-Human Interaction <strong class="EmphasisTypeBold ">7</strong>, 477–509 (2000).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">42.</div><div class="CitationContent" id="CR42">Hix, D., Gabbard, J. L. in Handbook of Virtual Environments chap. 28 (2014).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">43.</div><div class="CitationContent" id="CR43">Honigman, C.: The Third Room : A 3D Virtual Music Paradigm in Proceedings of the International Conference onNewInterfaces for Musical Expression (NIME) (2011).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">44.</div><div class="CitationContent" id="CR44">Hornecker, E., Buur, J.: Getting a grip on tangible interaction in ACM Conference on Human Factors in Computing Systems (CHI) (2006), 437.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">45.</div><div class="CitationContent" id="CR45">Hornecker, E., Marshall, P., Rogers, Y.: From Entry and Access - How Shareability Comes About in Designing pleasurable products and interfaces (2007).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">46.</div><div class="CitationContent" id="CR46">Houde, S., Hill, C.: What do prototypes prototype? Handbook of Human Computer Interaction, 1–16 (1997).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">47.</div><div class="CitationContent" id="CR47">Hutchings, D. R., Stasko, J.: Revisiting Display Space Management: Understanding Current Practice to Inform next-Generation Design in Proceedings of Graphics Interface 2004 (Canadian Human-Computer Communications Society, Waterloo, CAN, May 2004), 127–134.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">48.</div><div class="CitationContent" id="CR48">Innocenti, E. D. et al.: Mobile Virtual Reality for Musical Genre Learning in Primary Education. en. Computers &amp; Education <strong class="EmphasisTypeBold ">139</strong>, 102–117 (Oct. 2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">49.</div><div class="CitationContent" id="CR49">Jacob, R. et al.: Reality-based interaction: a framework for post-WIMP interfaces in ACM Conference on Human Factors in Computing Systems (CHI) (2008), 201–210.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">50.</div><div class="CitationContent" id="CR50">Jin, Y., Lee, S.: Designing in virtual reality: a comparison of problem-solving styles between desktop and VR environments. Digital Creativity 6268 (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">51.</div><div class="CitationContent" id="CR51">Jung, B., Hwang, J., Lee, S., Kim, G. J., Kim, H.: Incorporating Co-Presence in DistributedVirtual Music Environment in Proceedings of theACMSymposium onVirtualReality Software and Technology (Association for Computing Machinery, New York, NY, USA, Oct. 2000), 206–211.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">52.</div><div class="CitationContent" id="CR52">Jung, J. et al.: A Review on Interaction Techniques in Virtual Environments. Proceedings of the 2014 International Conference on Industrial Engineering and Operations Management, 1582–1590 (2014).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">53.</div><div class="CitationContent" id="CR53">Kolesnichenko, A., McVeigh-Schultz, J., Isbister, K.: Understanding Emerging Design Practices for Avatar Systems in the Commercial Social VR Ecology in Proceedings of the 2019 on Designing Interactive Systems Conference (Association for Computing Machinery, New York, NY, USA, June 2019), 241–252.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">54.</div><div class="CitationContent" id="CR54">Kolkmeier, J., Vroon, J., Heylen, D.: Interacting with virtual agents in shared space: Single and joint effects of gaze and proxemics. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) <strong class="EmphasisTypeBold ">10011 LNAI</strong>, 1–14 (2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">55.</div><div class="CitationContent" id="CR55">Lages, W.: Ray, Camera, Action ! A Technique for Collaborative 3D Manipulation. IEEE Symposium on 3D User Interfaces, 277–278 (2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">56.</div><div class="CitationContent" id="CR56">Lages, W., Nabiyouni, M., Tibau, J., Bowman, D. A.: Interval Player: Designing a virtual musical instrument using in-air gestures in 2015 IEEE Symposium on 3D User Interfaces, 3DUI 2015 - Proceedings (2015), 203–204.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">57.</div><div class="CitationContent" id="CR57">Le Groux, S., Manzolli, J., Verschure, P. F. J.: VR-RoBoser: Real-Time Adaptive Sonification of Virtual Environments Based on Avatar Behavior in Proceedings of the International Conference on New Interfaces for Musical Expression (NIME) (2007).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">58.</div><div class="CitationContent" id="CR58">Lilija, K., Pohl, H., Hornbæk, K.: Manipulation Who Put That There? Temporal Navigation of Spatial Recordings by Direct Manipulation in CHI Conference on Human Factors in Computing Systems (Association for Computing Machinery, 2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">59.</div><div class="CitationContent" id="CR59">Lubos, P., Bruder, G., Ariza, O., Steinicke, F.: Touching the Sphere: Leveraging Joint-Centered Kinespheres for Spatial User Interaction. Proceedings of the ACM Symposium on Spatial User Interaction (SUI’16), 13–22 (2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">60.</div><div class="CitationContent" id="CR60">Lugasi, M., Rafaely, B.: Speech Enhancement Using Masking for Binaural Reproduction of Ambisonics Signals. IEEE/ACM Transactions on Audio, Speech, and Language Processing <strong class="EmphasisTypeBold ">28</strong>, 1767–1777 (2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">61.</div><div class="CitationContent" id="CR61">Mäki-patola, T., Laitinen, J., Kanerva, A., Takala, T.: Experiments with virtual reality instruments in Proceedings of the International Conference on New Interfaces for Musical Expression (NIME) (2005), 11–16.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">62.</div><div class="CitationContent" id="CR62">Melchior, F., Pike, C., Brooks, M., Grace, S.: Sound Source Control in Spatial Audio Systems in Audio Engineering Society Convention (Rome, Italy, 2013).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">63.</div><div class="CitationContent" id="CR63">Men, L., Bryan-Kinns, N.: LeMo: Supporting Collaborative Music Making in Virtual Reality. IEEE 4TH VR Workshop SIVE (2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">64.</div><div class="CitationContent" id="CR64">Men, L., Bryan-Kinns, N.: LeMo: Exploring virtual space for collaborative creativity in Creativity and Cognition (ACM, San Diego, USA, June 2019), 71–82.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">65.</div><div class="CitationContent" id="CR65">Men, L., Bryan-Kinns, N., Bryce, L.: Designing spaces to support collaborative creativity in shared virtual environments. PeerJ Computer Science <strong class="EmphasisTypeBold ">5</strong>, e229 (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">66.</div><div class="CitationContent" id="CR66">Moore, A. G., Howell, M. J., Stiles, A. W., Herrera, N. S., McMahan, R. P.: Wedge: A musical interface for building and playing composition-appropriate immersive environments in 2015 IEEE Symposium on 3D User Interfaces (3DUI) (Mar. 2015), 205–206.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">67.</div><div class="CitationContent" id="CR67">Mulder, A., Fels, S. S., Mase, K.: Mapping virtual object manipulation to sound variation. IPSJ Sig Notes <strong class="EmphasisTypeBold ">97</strong>, 63–68 (1997).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">68.</div><div class="CitationContent" id="CR68">Mulder, A., Fels, S. S., Mase, K.: Design of Virtual 3D Instruments for Musical Interaction. Graphics Interface, 76–83 (1999).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">69.</div><div class="CitationContent" id="CR69">Naef, M., Collicott, D.: A VR Interface for Collaborative 3D Audio Performance in Proceedings of the International Conference on New Interfaces for Musical Expression (NIME) (2006).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">70.</div><div class="CitationContent" id="CR70">O’Modhrain, S.: A Framework for the Evaluation of Digital Musical Instruments. en. Computer Music Journal <strong class="EmphasisTypeBold ">35</strong>, 28–42 (Mar. 2011).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">71.</div><div class="CitationContent" id="CR71">Palumbo, M., Zonta, A.,Wakefield, G.: Modular reality: Analogues of patching in immersive space. Journal of New Music Research <strong class="EmphasisTypeBold ">49</strong>, 8–23 (2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">72.</div><div class="CitationContent" id="CR72">Plowright, P. D.: Revealing Architectural Design: Methods, Frameworks and Tools (Routledge, 2014).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">73.</div><div class="CitationContent" id="CR73">Poirier-Quinot, D., Katz, B.: Assessing the Impact of Head-Related Transfer Function Individualization on Task Performance: Case of a Virtual Reality Shooter Game. en. Journal of the Audio Engineering Society <strong class="EmphasisTypeBold ">68</strong>, 248–260 (May 2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">74.</div><div class="CitationContent" id="CR74">Poupyrev, I., Billinghurst, M., Weghorst, S., Ichikawa, T.: The Go-Go Interaction Technique: Non-Linear Mapping for Direct Manipulation in VR. Proc. UIST ’96 (ACM Symposium on User Interface Software and Technology), 79–80 (1996).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">75.</div><div class="CitationContent" id="CR75">River, J., MacTavish, T.: Research through Provocation: A Structured Prototyping Tool Using Interaction Attributes of Time, Space and Information. The Design Journal <strong class="EmphasisTypeBold ">20</strong>, S3996–S4008 (July 2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">76.</div><div class="CitationContent" id="CR76">Rogers, Y., Lim, Y.-k., Hazlewood,W. R., Marshall, P.: Equal Opportunities: Do Shareable Interfaces Promote More Group Participation Than Single User Displays? Human-Computer Interaction <strong class="EmphasisTypeBold ">24</strong>, 79–116 (2009).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">77.</div><div class="CitationContent" id="CR77">Serafin, S., Erkut, C.,Kojs, J., Nilsson,N. C.,Nordahl, R.: VirtualReality Musical Instruments: State of the Art, Design Principles, and Future Directions. Computer Music Journal <strong class="EmphasisTypeBold ">40</strong>, 22–40 (2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">78.</div><div class="CitationContent" id="CR78">El-shimy, D., Cooperstock, J. R.: User-Driven Techniques for the Design and Evaluation of New Musical Interfaces. Computer Music journal <strong class="EmphasisTypeBold ">39</strong>, 28–46 (2015).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">79.</div><div class="CitationContent" id="CR79">Smith, H. J., Neff, M.: Communication Behavior in Embodied Virtual Reality in ACM Conference on Human Factors in Computing Systems (CHI) (2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">80.</div><div class="CitationContent" id="CR80">Snook, K. et al.: Concordia: A musical XR instrument for playing the solar system. Journal of New Music Research <strong class="EmphasisTypeBold ">49</strong>, 88–103 (2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">81.</div><div class="CitationContent" id="CR81">Stowell, D., Robertson, A., Bryan-Kinns, N., Plumbley, M. D.: Evaluation of live human-computer music-making: Quantitative and qualitative approaches. International Journal of Human-Computer Studies <strong class="EmphasisTypeBold ">67</strong>, 960–975 (2009).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">82.</div><div class="CitationContent" id="CR82">Suchman, L., Trigg, R., Blomberg, J.: Working artefacts: ethnomethods of the prototype. The British Journal of Sociology <strong class="EmphasisTypeBold ">53</strong>, 163–179 (2002).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">83.</div><div class="CitationContent" id="CR83">Svanæs, D.: Interaction Design for and with the Lived Body : Some Implications of Merleau-Ponty ’ s Phenomenology. ACM Transactions on Computer-Human Interaction <strong class="EmphasisTypeBold ">20</strong>, 1–30 (2013).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">84.</div><div class="CitationContent" id="CR84">Thoring, K., Desmet, P., Badke-Schaub, P.: Creative Environments for Design Education and Practice: A Typology of Creative Spaces. en. Design Studies <strong class="EmphasisTypeBold ">56</strong>, 54–83 (May 2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">85.</div><div class="CitationContent" id="CR85">Trommer, M.: Points Further North: An acoustemological cartography of non-place. Journal of New Music Research <strong class="EmphasisTypeBold ">49</strong>, 73–87 (2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">86.</div><div class="CitationContent" id="CR86">Trumbo, J.: The Spatial Environment in Multimedia Design: Physical, Conceptual, Perceptual, and Behavioral Aspects of Design Space. Design Issues <strong class="EmphasisTypeBold ">13</strong>, 19–28 (1997).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">87.</div><div class="CitationContent" id="CR87">Tuan, Y.-F.: Space and Place: The Perspective of Experience. <strong class="EmphasisTypeBold ">4</strong>, 513 (1978).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">88.</div><div class="CitationContent" id="CR88">Valbom, L., Marcos, A.: Wave: Sound and music in an immersive environment. Computers &amp; Graphics <strong class="EmphasisTypeBold ">29</strong>, 871–881 (2005).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">89.</div><div class="CitationContent" id="CR89">Vanderdonckt, J.: Distributed user interfaces: how to distribute user interface elements across users, platforms, and environments. Proc. of XI Interacción, 20–32 (2010).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">90.</div><div class="CitationContent" id="CR90">Wakefield, G., Smith,W.: Cosm : a Toolkit for Composing Immersive Audio- Visual Worlds of Agency and Autonomy in Proceedings of the International Computer Music Conference (ICMC) (2011).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">91.</div><div class="CitationContent" id="CR91">Wanderley, M. M., Orio, N.: Evaluation of Input Devices for Musical Expression : Borrowing Tools from HCI. Computer Music Journal <strong class="EmphasisTypeBold ">26</strong>, 62–76 (2002).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">92.</div><div class="CitationContent" id="CR92">Weinel, J. in Technology, Design and the Arts-Opportunities and Challenges 209–227 (Springer, Cham, 2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">93.</div><div class="CitationContent" id="CR93">Won, A. S., Bailenson, J. N., Lanier, J. in Emerging Trends in the Social and Behavioral Sciences 1–16 (2015).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">94.</div><div class="CitationContent" id="CR94">Wong, N., Gutwin, C.: Where are you pointing? Proceedings of the 28th international conference on Human factors in computing systems - CHI ’10, 1029 (2010).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">95.</div><div class="CitationContent" id="CR95">Wong, N., Gutwin, C.: Support for Deictic Pointing in CVEs : Still Fragmented after All These years? in Computer Supported Cooperative Work (2014), 1377–1387.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">96.</div><div class="CitationContent" id="CR96">Xambó, A., Laney, R., Dobbyn, C., Jordá, S. P.: Multi-touch interaction principles for collaborative real-time music activities: towards a pattern language. Proc. of ICMC’11, 403–406 (2011).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">97.</div><div class="CitationContent" id="CR97">Xambó, A. et al.: Exploring Social Interaction With a Tangible Music Interface. Interacting with Computers 28 (2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">98.</div><div class="CitationContent" id="CR98">Young, G., Murphy, D.: HCI Models for Digital Musical Instruments: Methodologies for Rigorous Testing of Digital Musical Instruments. International Symposium on Computer Music Multidisciplinary Research (CMMR) (2015).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">99.</div><div class="CitationContent" id="CR99">Zhou, F., Dun, H. B. L., Billinghurst, M.: Trends in augmented reality tracking, interaction and display: A review of ten years of ISMAR. Proceedings - 7th IEEE International Symposium on Mixed and Augmented Reality 2008, ISMAR 2008, 193–202 (2008).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">100.</div><div class="CitationContent" id="CR100">Zielasko, D. et al.: Cirque des Bouteilles : The Art of Blowing on Bottles in 2015 IEEE Symposium on 3D User Interfaces, 3DUI 2015 - Proceedings (2015), 209–210.</div></li></ol><div class="BibSection" id="BSec1"><div class="Heading">Products and Grey Literature</div><ol class="BibliographyWrapper"><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">101.</div><div class="CitationContent" id="CR101">Arrigo, A., Lemke, A.: Wave <span class="ExternalRef"><a href="http://wavexr.com/"><span class="RefSource">http://​wavexr.​com/​</span></a></span>. Austin, TX, USA, 2016.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">102.</div><div class="CitationContent" id="CR102">Beat Saber - VR Rhythm Game Beat Games. 2019.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">103.</div><div class="CitationContent" id="CR103">Block Rocking Beats <span class="ExternalRef"><a href="http://blockrockingbeats.com/"><span class="RefSource">http://​blockrockingbeat​s.​com/​</span></a></span>. 2016.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">104.</div><div class="CitationContent" id="CR104">DearVR Spatial Connect <span class="ExternalRef"><a href="https://www.dearvr.com/products/dearvrspatial-connect"><span class="RefSource">https://​www.​dearvr.​com/​products/​dearvrspatial-connect</span></a></span>. Düsseldorf, Germany, 2018.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">105.</div><div class="CitationContent" id="CR105">Designing For Virtual Reality en. <span class="ExternalRef"><a href="https://www.ustwo.com/blog/designing-for-virtual-reality/"><span class="RefSource">https://​www.​ustwo.​com/​blog/​designing-for-virtual-reality/​</span></a></span>. 2015.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">106.</div><div class="CitationContent" id="CR106">Drops <span class="ExternalRef"><a href="https://drops.garden/"><span class="RefSource">https://​drops.​garden/​</span></a></span>. 2018.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">107.</div><div class="CitationContent" id="CR107">Gravity Sketch <span class="ExternalRef"><a href="https://www.gravitysketch.com/"><span class="RefSource">https://​www.​gravitysketch.​com/​</span></a></span>. Aug. 2017.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">108.</div><div class="CitationContent" id="CR108">Kane, A.: Volta <span class="ExternalRef"><a href="https://volta-xr.com/"><span class="RefSource">https://​volta-xr.​com/​</span></a></span>. 2021.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">109.</div><div class="CitationContent" id="CR109">Kane, A.: Volta <span class="ExternalRef"><a href="https://www.voltaaudio.com"><span class="RefSource">https://​www.​voltaaudio.​com</span></a></span>. London, UK, 2019.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">110.</div><div class="CitationContent" id="CR110">Kinstner, Z.: EXA: The Infinite Instrument <span class="ExternalRef"><a href="https://store.steampowered.com/app/606920/EXA_The_Infinite_Instrument/"><span class="RefSource">https://​store.​steampowered.​com/​app/​606920/​EXA_​The_​Infinite_​Instrument/​</span></a></span>. Grand Rapids, Michigan, USA, 2017.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">111.</div><div class="CitationContent" id="CR111">Lee, J., Strangeloop: The Lune Rouge Experience The WaveVR. 2017.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">112.</div><div class="CitationContent" id="CR112">LyraVR <span class="ExternalRef"><a href="http://lyravr.com/"><span class="RefSource">http://​lyravr.​com/​</span></a></span>. 2018.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">113.</div><div class="CitationContent" id="CR113">Mux <span class="ExternalRef"><a href="https://store.steampowered.com/app/673970/MuX/"><span class="RefSource">https://​store.​steampowered.​com/​app/​673970/​MuX/​</span></a></span><span class="ExternalRef"><a href="http://playmux.com/"><span class="RefSource">http://​playmux.​com/​</span></a></span>. 2017.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">114.</div><div class="CitationContent" id="CR114">Olson, L., Havok, R., Ozil, G., Fish, R.: Soundstage VR <span class="ExternalRef"><a href="https://github.com/googlearchive/soundstagevr"><span class="RefSource">https://​github.​com/​googlearchive/​soundstagevr</span></a></span>. 2017.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">115.</div><div class="CitationContent" id="CR115">Spatial Audio API <span class="ExternalRef"><a href="https://www.highfidelity.com/"><span class="RefSource">https://​www.​highfidelity.​com/​</span></a></span>. 2021.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">116.</div><div class="CitationContent" id="CR116">The Garden <span class="ExternalRef"><a href="https://www.biomecollective.com/the-garden"><span class="RefSource">https://​www.​biomecollective.​com/​the-garden</span></a></span>. Dundee, UK, 2019.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">117.</div><div class="CitationContent" id="CR117">The Last Maestro <span class="ExternalRef"><a href="https://www.maestrogames.com/"><span class="RefSource">https://​www.​maestrogames.​com/​</span></a></span>. 2021.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">118.</div><div class="CitationContent" id="CR118">The Music Room <span class="ExternalRef"><a href="http://www.musicroomvr.com/"><span class="RefSource">http://​www.​musicroomvr.​com/​</span></a></span>. 2016.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">119.</div><div class="CitationContent" id="CR119">Tranzient <span class="ExternalRef"><a href="https://www.aliveintech.com"><span class="RefSource">https://​www.​aliveintech.​com</span></a></span>. 2019.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">120.</div><div class="CitationContent" id="CR120">Virtual Reality Best Practices en-US. <span class="ExternalRef"><a href="https://docs.unrealengine.com/en-US/SharingAndReleasing/XRDevelopment/VR/DevelopVR/ContentSetup/index.html"><span class="RefSource">https://​docs.​unrealengine.​com/​en-US/​SharingAndReleas​ing/​XRDevelopment/​VR/​DevelopVR/​ContentSetup/​index.​html</span></a></span>.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">121.</div><div class="CitationContent" id="CR121">VR Best Practice <span class="ExternalRef"><a href="https://learn.unity.com/tutorial/vr-bestpractice"><span class="RefSource">https://​learn.​unity.​com/​tutorial/​vr-bestpractice</span></a></span>. 2017.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">122.</div><div class="CitationContent" id="CR122">VR Design : Best Practices en-US. <span class="ExternalRef"><a href="http://blog.dsky.co/2015/07/30/vr-design-best-practices/"><span class="RefSource">http://​blog.​dsky.​co/​2015/​07/​30/​vr-design-best-practices/​</span></a></span>. July 2015.</div></li></ol></div></div></aside><aside aria-label="Footnotes" class="FootnoteSection" epub:type="footnotes"><div class="Heading">Footnotes</div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn1_source">1</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn1" role="doc-footnote"><p class="Para" id="Par5">Prototypes are any representation of a design idea, regardless of medium, and an artefact is a product or interactive system created for a design intervention/experiment [<span class="CitationRef"><a epub:type="biblioref" href="#CR46" role="doc-biblioref">46</a></span>].</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn2_source">2</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn2" role="doc-footnote"><p class="Para" id="Par38"><span class="ExternalRef"><a href="https://github.com/lucaturchet/Musical_XR_publication_database"><span class="RefSource">https://​github.​com/​lucaturchet/​Musical_​XR_​publication_​database</span></a></span>.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn3_source">3</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn3" role="doc-footnote"><p class="Para" id="Par57">Category name and description sourced from [<span class="CitationRef"><a epub:type="biblioref" href="#CR4" role="doc-biblioref">4</a></span>].</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn4_source">4</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn4" role="doc-footnote"><p class="Para" id="Par88">The first author supported the design of early prototypes of Volta XR, interested readers can review the design development at <span class="ExternalRef"><a href="https://thefuturehappened.org/Volta"><span class="RefSource">https://​thefuturehappene​d.​org/​Volta</span></a></span>.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn5_source">5</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn5" role="doc-footnote"><p class="Para" id="Par93">The first author participated in formal beta testing of the DearVR Spatial Connect product.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn6_source">6</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn6" role="doc-footnote"><p class="Para" id="Par96"><em class="EmphasisTypeItalic ">ObjectsVR</em> was a single-user system designed and tested by the first author during a research internship at a VR experience design firm.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn7_source">7</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn7" role="doc-footnote"><p class="Para" id="Par112">Preprint available at <span class="ExternalRef"><a href="https://hal.archives-ouvertes.fr/hal-03099274"><span class="RefSource">https://​hal.​archives-ouvertes.​fr/​hal-03099274</span></a></span>.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn8_source">8</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn8" role="doc-footnote"><p class="Para" id="Par153">Narrative and physical experiences that engage an audience member in a fictional world, for instance immersive VR theatre production.</p></div><div class="ClearBoth"> </div></div></aside></div></div></body></html>