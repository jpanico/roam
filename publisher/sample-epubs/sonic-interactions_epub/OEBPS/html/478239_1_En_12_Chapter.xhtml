<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops"><head><title>Augmenting Sonic Experiences Through Haptic Feedback</title><meta content="text/html; charset=utf-8" http-equiv="content-type"/><link href="../css/springer_epub.css" rel="styleSheet" type="text/css"/></head><body><div epub:type="chapter" role="doc-chapter"><div class="ChapterContextInformation"><div class="ContextInformation" id="Chap12"><div class="ChapterCopyright">© The Author(s) 2023</div><span class="ContextInformationAuthorEditorNames">M. Geronazzo, S. Serafin<span class="CollaboratorDesignation"> (eds.)</span></span><span class="ContextInformationBookTitles"><span class="BookTitle">Sonic Interactions in Virtual Environments</span></span><span class="ContextInformationSeries"><span class="SeriesTitle" lang="en">Human–Computer Interaction Series</span></span><span class="ChapterDOI"><a href="https://doi.org/10.1007/978-3-031-04021-4_12">https://doi.org/10.1007/978-3-031-04021-4_12</a></span></div></div><!--Begin Abstract--><div class="MainTitleSection"><h1 class="ChapterTitle" lang="en">12. Augmenting Sonic Experiences Through Haptic Feedback</h1></div><div class="AuthorGroup"><div class="AuthorNames"><span class="Author"><span class="AuthorName">Federico Fontana</span><sup><a href="#Aff34">1</a> <a aria-label="Contact information for this author" href="#ContactOfAuthor1"><span class="ContactIcon"> </span></a></sup>, </span><span class="Author"><span class="AuthorName">Hanna Järveläinen</span><sup><a href="#Aff35">2</a> <a aria-label="Contact information for this author" href="#ContactOfAuthor2"><span class="ContactIcon"> </span></a></sup> and </span><span class="Author"><span class="AuthorName">Stefano Papetti</span><sup><a href="#Aff35">2</a> <a aria-label="Contact information for this author" href="#ContactOfAuthor3"><span class="ContactIcon"> </span></a></sup></span></div><div class="Affiliations"><div class="Affiliation" id="Aff34"><span class="AffiliationNumber">(1)</span><div class="AffiliationText">Department of Mathematics, Computer Science and Physics, University of Udine, via delle Scienze 206, Udine, 33100, Italy</div></div><div class="Affiliation" id="Aff35"><span class="AffiliationNumber">(2)</span><div class="AffiliationText">Institute for Computer Music and Sound Technology, Zurich University of the Arts, Pfingstweidstrasse 96, Zurich, 8005, Switzerland</div></div><div class="ClearBoth"> </div></div><div class="Contacts"><div class="Contact" id="ContactOfAuthor1"><div class="ContactIcon"> </div><div class="ContactAuthorLine"><span class="AuthorName">Federico Fontana</span> (Corresponding author)</div><div class="ContactAdditionalLine"><span class="ContactType">Email: </span><a href="mailto:federico.fontana@uniud.it">federico.fontana@uniud.it</a></div></div><div class="Contact" id="ContactOfAuthor2"><div class="ContactIcon"> </div><div class="ContactAuthorLine"><span class="AuthorName">Hanna Järveläinen</span></div><div class="ContactAdditionalLine"><span class="ContactType">Email: </span><a href="mailto:hanna.jarvelainen@zhdk.ch">hanna.jarvelainen@zhdk.ch</a></div></div><div class="Contact" id="ContactOfAuthor3"><div class="ContactIcon"> </div><div class="ContactAuthorLine"><span class="AuthorName">Stefano Papetti</span></div><div class="ContactAdditionalLine"><span class="ContactType">Email: </span><a href="mailto:stefano.papetti@zhdk.ch">stefano.papetti@zhdk.ch</a></div></div></div></div><section class="Abstract" id="Abs1" lang="en" role="doc-abstract"><h2 class="Heading">Abstract</h2><p class="Para" id="Par1">Sonic experiences are usually considered as the result of auditory feedback alone. From a psychological standpoint, however, this is true only when a listener is kept isolated from concurrent stimuli targeting the other senses. Such stimuli, in fact, may either interfere with the sonic experience if they distract the listener, or conversely enhance it if they convey sensations coherent with what is being heard. This chapter is concerned with haptic augmentations having effects on auditory perception, for example how different vibrotactile cues provided by an electronic musical instrument may affect its perceived sound quality or the playing experience. Results from different experiments are reviewed showing that the auditory and somatosensory channels together can produce constructive effects resulting in measurable perceptual enhancement. That may affect sonic dimensions ranging from basic auditory parameters, such as the perceived intensity of frequency components, up to more complex perceptions which contribute to forming our ecology of everyday or musical sounds.</p></section><!--End Abstract--><div class="Fulltext"><section class="Section1 RenderAsSection1" id="Sec1"><h2 class="Heading"><span class="HeadingNumber">12.1 </span>Introduction</h2><p class="Para" id="Par2">During a sonic <span id="ITerm1">experience</span>, humans give meaning to what is being listened to, based on their perception and cognition of the auditory scene. As other sensory channels normally convey stimuli in parallel to hearing, the human brain integrates a continuous flow of sensations while contextualizing the experience. If, on the one hand, vision, smell, and taste concur in describing an auditory scene, thanks to high-level connections involving our mental imagery [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>], on the other hand, <span id="ITerm2">touch</span> is often exposed to temporal patterns that exhibit a strong affinity with the acoustic signals hitting the eardrum with respect to their synchronism, amplitude, spectral content, and mutual localization. This similarity is evident, for instance, when a musician plays an instrument, and more in general whenever a human action generates an event producing sound as a (by-)product.</p><p class="Para" id="Par3">Our chapter is about whether the somatosensory <span id="ITerm3">feedback</span> consequence of that action contributes to <em class="EmphasisTypeItalic ">augment</em> the sonic <span id="ITerm4">experience</span>. Here, the term augmentation embraces all sorts of enrichment that a sonic experience would benefit from through the somatosensory channel, whether it makes a perceived sound stronger, clearer, more vivid, meaningful, pleasant, or ecologically valid. Such a variety of effects, affecting sound ranging from fundamental physical dimensions until its semantics, can be explained by the tight interactions that sound and vibration establish with one another, as soon as our brain associates them both with a unique event. Understanding such interactions and their effects is the main goal of scientists who investigate the psychophysics of auditory-<span id="ITerm5">tactile</span> perception.</p><p class="Para" id="Par4">Perception psychologists were able to isolate the role of touch, especially during passive auditory tasks. Such tasks in fact lead to generally more robust design, control, and repeatability of the experiments. For this reason, the reference literature introducing this chapter deals mainly with passive touch. However, the most interesting sonic augmentations in an ecological or musical sense involve perception-action loops, in which the listener physically interacts with a <span id="ITerm6">sounding</span> object. In the case of active exploration, or when a device reproduces tactile cues, the sense of touch conveys <em class="EmphasisTypeItalic ">haptic</em> <span id="ITerm7">feedback</span>. Accordingly, our chapter will focus on effects reported by active listeners, as well as on sonic (either ecologic or musical) experiences resulting from passive tasks in the presence of various haptic interfaces.</p><section class="Section2 RenderAsSection2" id="Sec2"><h3 class="Heading"><span class="HeadingNumber">12.1.1 </span>Multisensory Processing of Touch and Audition</h3><p class="Para" id="Par5">Multisensory processing—the convergence of information from various sensory channels—happens both in early cortical stages and in high-level structures. These processes can either enhance or depress response relative to the most robust unisensory information. This multisensory <span id="ITerm8">integration</span> benefits feature integration, object processing, event detection, and decision-making especially when cues are weak or ambiguous [<span class="CitationRef"><a epub:type="biblioref" href="#CR16" role="doc-biblioref">16</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR54" role="doc-biblioref">54</a></span>] (please refer also to Chap. <span class="ExternalRef"><a href="478239_1_En_10_Chapter.xhtml"><span class="RefSource">10</span></a></span> for a bigger picture on this topic). There is ample evidence of integration and interaction between the senses of hearing and touch. While somatosensory influence on higher auditory structures is well-known, evidence of low-level influence is more recent and increasing [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>]. The cochlear nucleus in the brainstem responds to both somatosensory and auditory stimulation; this way somatosensory input may influence both sound lateralization and the suppression of self-<span id="ITerm9">generated</span> sounds [<span class="CitationRef"><a epub:type="biblioref" href="#CR10" role="doc-biblioref">10</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR53" role="doc-biblioref">53</a></span>]. The first cortical stages—previously thought to process unimodal sensory information—are now known to converge and sometimes process heteromodal information. The primary and the belt areas of the auditory cortex receive inputs from various low-level somatosensory areas, while fewer reports point to pathways from auditory to somatosensory areas. Higher-level multisensory areas that process auditory and somatosensory information include the <em class="EmphasisTypeItalic ">Superior temporal cortex</em> and the <em class="EmphasisTypeItalic ">Insular cortex</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR15" role="doc-biblioref">15</a></span>].</p><p class="Para" id="Par6">However, much of the multisensory integration that is necessary for the identification and localization of events takes place in the <em class="EmphasisTypeItalic ">Superior Colliculus</em> (SC), which is located in the midbrain: several subcortical and primary cortical areas project auditory, somatosensory, and visual information to this area. The neurons in the SC can respond differently to cross-modal stimuli than to either of the respective unimodal stimuli. Information is integrated according to a few general principles: spatially and temporally coherent stimuli produce maximal enhancement, and weaker stimuli produce a relatively greater enhancement (inverse effectiveness) [<span class="CitationRef"><a epub:type="biblioref" href="#CR47" role="doc-biblioref">47</a></span>].</p><p class="Para" id="Par7">Similar observations have been made on behavioral level: sounds and vibrations have been shown to interact constructively when congruent stimuli are delivered simultaneously [<span class="CitationRef"><a epub:type="biblioref" href="#CR56" role="doc-biblioref">56</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR57" role="doc-biblioref">57</a></span>], with measurable auditory effects of somatosensory feedback [<span class="CitationRef"><a epub:type="biblioref" href="#CR4" role="doc-biblioref">4</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR36" role="doc-biblioref">36</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR37" role="doc-biblioref">37</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR52" role="doc-biblioref">52</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR61" role="doc-biblioref">61</a></span>]. Here congruence is defined depending on the experimental procedure: in general, it refers to conditions in which the multisensory stimulus shares common spatio-temporal as well as spectral features, as if it was originating from a unique source producing sounds and vibrations together. In parallel, simultaneity refers to a stimulus pair whose acoustic and vibratory components are rigorously constrained concerning their mutual synchronization: audio-tactile temporal resolution is superior to audio-visual or visuo-tactile combinations [<span class="CitationRef"><a epub:type="biblioref" href="#CR20" role="doc-biblioref">20</a></span>]. In this regard, it must be kept in mind that hearing and touch are both very sensitive to temporal delays, and detect especially low latency values relative to each other. By varying these values in the range 5–70 ms, Kaaresoja et al. have been able to change the perceived quality of virtual buttons during a clicking gesture [<span class="CitationRef"><a epub:type="biblioref" href="#CR29" role="doc-biblioref">29</a></span>]. More in general, mutual unsynchronization and/or delocalization of the acoustic and vibratory components leads to disparate effects that must be dealt with case by case [<span class="CitationRef"><a epub:type="biblioref" href="#CR50" role="doc-biblioref">50</a></span>], revealing the complexity of audio-tactile interactions. As this chapter focuses on haptic feedback, we will instead describe experiments where stimuli are simultaneous and co-localized.</p><p class="Para" id="Par8">Spatial collocation seems in fact somewhat less critical than temporal synchrony, judging by the presence of audio-tactile interactions and enhancement in many experiments where participants receive vibrotactile feedback through the hand and auditory stimuli <span id="ITerm10">through</span> headphones [<span class="CitationRef"><a epub:type="biblioref" href="#CR31" role="doc-biblioref">31</a></span>]. Nevertheless, humans have good spatial discrimination ability between auditory and tactile stimuli: lateral angles of <span class="InlineEquation" id="IEq1"><img alt="$${\ge } 5.3^\circ $$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq1.png" style="width:2.74em"/></span> were detected between electrotactile stimulation at the fingertip and sound source in an experiment by Altinsoy [<span class="CitationRef"><a epub:type="biblioref" href="#CR2" role="doc-biblioref">2</a></span>]. (To put this in context, auditory localization blur for the scraping sounds used as stimuli in the experiment was <span class="InlineEquation" id="IEq2"><img alt="$$3.9^\circ $$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq2.png" style="width:1.88em"/></span>.) Indeed, the seeming failure of some studies to demonstrate spatial modulations of audio-tactile interactions may be due to the fact that stimuli have been presented at hands or otherwise at some distance from the head; more recently, spatial modulation effects have indeed been observed especially in the space close to the head [<span class="CitationRef"><a epub:type="biblioref" href="#CR31" role="doc-biblioref">31</a></span>]. However, these phenomena are not thoroughly known yet; note that in the peripersonal <span id="ITerm11">space</span>, even unimodal auditory localization differs from that at greater distances [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>–<span class="CitationRef"><a epub:type="biblioref" href="#CR8" role="doc-biblioref">8</a></span>].</p><p class="Para" id="Par9">The psychophysical literature specifically dealing with the effects of touch on auditory perception is sparse, mostly focusing on intensity and pitch as primary objects of investigation. As opposed to the previously described constructive effect valid for multisensory cues of intensity, the interactions between auditory pitch and tactile frequency discrimination are more complex [<span class="CitationRef"><a epub:type="biblioref" href="#CR5" role="doc-biblioref">5</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR59" role="doc-biblioref">59</a></span>]. In particular, tactile frequencies do not need simultaneity nor co-localization to affect pitch perception [<span class="CitationRef"><a epub:type="biblioref" href="#CR62" role="doc-biblioref">62</a></span>]. As part of their study on the audio-tactile pitch and loudness interactions, Yau et al. found separate mechanisms for tactile influence on loudness and pitch, with audio-tactile loudness perception depending more on the timing of the stimuli [<span class="CitationRef"><a epub:type="biblioref" href="#CR60" role="doc-biblioref">60</a></span>]. Anyhow, pitch is perceived much more accurately through the hearing system, hence touch in general plays no supportive role during the perception of frequency components in an audio-tactile signal. Still, tactile frequency discrimination ability has been ascertained [<span class="CitationRef"><a epub:type="biblioref" href="#CR23" role="doc-biblioref">23</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR55" role="doc-biblioref">55</a></span>], with surprising accuracy in congenitally deaf individuals [<span class="CitationRef"><a epub:type="biblioref" href="#CR35" role="doc-biblioref">35</a></span>]. This evidence naturally leads to the question about musical sensations induced by touch, an issue which has fascinated several scientists [<span class="CitationRef"><a epub:type="biblioref" href="#CR49" role="doc-biblioref">49</a></span>] and, hence, occupies an important part of this chapter.</p><p class="Para" id="Par10">Some deaf musicians show an indisputable ability to “feel the vibrations” during music performance, not merely for entraining with other musicians [<span class="CitationRef"><a epub:type="biblioref" href="#CR12" role="doc-biblioref">12</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR22" role="doc-biblioref">22</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>], but also for sharing melody and timbre with them. This ability seems to be the result of the long <span id="ITerm12">training</span> <em class="EmphasisTypeItalic ">any</em> (i.e., including the normally able) good musician has accumulated with all senses on their instrument [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>] during a continuous perception-action process. Such a training, hence, refines a multisensory acuity for the instrument quality, not limited to its sound [<span class="CitationRef"><a epub:type="biblioref" href="#CR21" role="doc-biblioref">21</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR58" role="doc-biblioref">58</a></span>].</p><p class="Para" id="Par11">Non-musicians can also discriminate musical timbre and relative pitch intervals from vibrotactile cues, to some extent even without training [<span class="CitationRef"><a epub:type="biblioref" href="#CR24" role="doc-biblioref">24</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR49" role="doc-biblioref">49</a></span>]. However, generalizing the above-mentioned higher-level phenomena to musically untrained individuals is not obvious [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>]. Being inherently psychophysical, there is no reason to think that the summation of auditory and tactile cues of intensity would not apply to non-musicians. In parallel, musical training seems to facilitate more subtle audio-tactile synergies mediated by higher nervous system levels, such as those linking pitch and tactile frequency recognition [<span class="CitationRef"><a epub:type="biblioref" href="#CR11" role="doc-biblioref">11</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>]. Amid these two facts, the possibility for touch to enable the detection in normal listeners of frequency components otherwise inaudible, due to masking or threshold effects, is yet to be systematically explored.</p></section><section class="Section2 RenderAsSection2" id="Sec3"><h3 class="Heading"><span class="HeadingNumber">12.1.2 </span>Chapter Outline</h3><div class="Para" id="Par12">In their respective interaction contexts and with different confidence levels, hence, the experiments chosen for this chapter share the general assumption that a sonic experience can be influenced by somatosensory cues. Some of them (e.g., [<span class="CitationRef"><a epub:type="biblioref" href="#CR19" role="doc-biblioref">19</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR48" role="doc-biblioref">48</a></span>]) contributed to give form to the musical haptics research methodology and, hence, led to inevitably less robust conclusions. For this reason, they are certainly more suggestive <span id="ITerm13">than</span> conclusive.<div class="Table" id="Tab1"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 12.1</span><p class="SimplePara">Key characteristics of the experiments forming the chapter</p></div></div><div class="MediaObject" id="MO1"><img alt="" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Tab1_HTML.png" style="width:33.95em"/></div></div>
</div><p class="Para" id="Par13">In an aim to orient the reader to the experiments which reflect his or her interests, Table <span class="InternalRef"><a href="#Tab1">12.1</a></span> summarizes their key characteristics. Moreover, the table labels the experiments with gray tones classifying their dependence on specific elements. According to this classification, the first two experiments define an abstract context which is in principle applicable to multiple interaction contexts. The third and fourth experiments limit these contexts respectively to musical scales and plucked strings perception. The fifth and sixth ones further restrict the context respectively to acoustic and digital pianos. Finally, the seventh experiment specifically targets haptic versions of sound wave templates.</p><p class="Para" id="Par14">More in detail, the first experiment suggests a role of tactile frequency discrimination in enhancing the auditory perception of near-threshold frequency components; this role emerged during the audio-tactile identification of everyday materials from their response to a ball hitting them [<span class="CitationRef"><a epub:type="biblioref" href="#CR13" role="doc-biblioref">13</a></span>]. Next, we present an experiment conducted using an audio-tactile interface [<span class="CitationRef"><a epub:type="biblioref" href="#CR40" role="doc-biblioref">40</a></span>], showing that individuals performing a basic musical gesture such as finger pressing were able to reproduce previously learnt target forces more accurately if receiving contextual audio-tactile feedback instead of auditory or tactile feedback alone [<span class="CitationRef"><a epub:type="biblioref" href="#CR27" role="doc-biblioref">27</a></span>].</p><p class="Para" id="Par15">The third and fourth experiments link the aforementioned effects to musical experiences. As evidence of the power of the vibrotactile channel to deliver musical information, we first review a test in which Western and Indian musicians categorized and even identified music scales from both traditions by touching the surface of a harmonium [<span class="CitationRef"><a epub:type="biblioref" href="#CR48" role="doc-biblioref">48</a></span>]. Then, a robotic stringed instrument prototype called Keytar is described, in which the accurate haptic rendering of its virtual strings was significantly appreciated by users, however with no significant improvements for the perceived sound quality [<span class="CitationRef"><a epub:type="biblioref" href="#CR44" role="doc-biblioref">44</a></span>].</p><p class="Para" id="Par16">Conversely, a constructive effect was measured in pianists playing an acoustic piano whose natural vibrations could be switched on and off, thanks to peculiar engineering of the keyboard: in this case, the inclusion of vibrotactile feedback resulted in a measurable improvement of the instrument sound quality [<span class="CitationRef"><a epub:type="biblioref" href="#CR18" role="doc-biblioref">18</a></span>]. A similar effect was measured in musicians playing an actuated digital piano when this instrument reproduced vibrations recorded on a real piano [<span class="CitationRef"><a epub:type="biblioref" href="#CR19" role="doc-biblioref">19</a></span>].</p><p class="Para" id="Par17">Finally, using a force-sensitive haptic surface for musical expression which controlled a synthesizer, the effect of various vibration types on perceived quality attributes and the playing experience was assessed [<span class="CitationRef"><a epub:type="biblioref" href="#CR41" role="doc-biblioref">41</a></span>].</p></section></section><section class="Section1 RenderAsSection1" id="Sec4"><h2 class="Heading"><span class="HeadingNumber">12.2 </span>Ball Bouncing on Everyday Materials</h2><div class="Para" id="Par18">Two experiments [<span class="CitationRef"><a epub:type="biblioref" href="#CR13" role="doc-biblioref">13</a></span>] studied the role of impact sounds and vibrations for the subjective classification of three flat objects, which were respectively made of wood, plastic, and metal—see Fig. <span class="InternalRef"><a href="#Fig1">12.1</a></span><span id="ITerm14"/>.<figure class="Figure" id="Fig1"><div class="MediaObject" id="MO2"><img alt="" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Fig1_HTML.png" style="width:33.98em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 12.1</span><p class="SimplePara">Materials used in the experiment. Left: wood. Center: plastic. Right: metal</p></div></figcaption></figure>
</div><div class="Para" id="Par19">The task consisted of feeling an actuated surface and listening through headphones to the recorded feedback of a ping-pong ball hitting such objects (Fig. <span class="InternalRef"><a href="#Fig2">12.2</a></span>, left), after they had been experienced during a training task (Fig. <span class="InternalRef"><a href="#Fig2">12.2</a></span>, right).<figure class="Figure" id="Fig2"><div class="MediaObject" id="MO3"><img alt="" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Fig2_HTML.png" style="width:33.95em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 12.2</span><p class="SimplePara">Experimental tasks. Left: perceptual task. Right: training task</p></div></figcaption></figure>
</div><p class="Para" id="Par20">In Experiment 1, sounds and vibrations were recorded by keeping the objects in mechanical isolation. In Experiment 2, recordings were taken while the same objects stood on a table, causing their resonances to fade faster due to mechanical coupling with the support. Twenty-five subjects, aged between 23 and 61 years (M <span class="InlineEquation" id="IEq3"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq3.png" style="width:0.87em"/></span> 32.1, SD <span class="InlineEquation" id="IEq4"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq4.png" style="width:0.87em"/></span> 10.1), participated in Experiment 1, and twenty-seven (21–54 years old; M <span class="InlineEquation" id="IEq5"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq5.png" style="width:0.87em"/></span> 29.0, SD <span class="InlineEquation" id="IEq6"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq6.png" style="width:0.87em"/></span> 6.8) in Experiment 2. Eight subjects participated in both experiments. Roughly one-third of the participants were female. In terms of musical training, participants were not screened, and they reflect the general population average.</p><div class="Para" id="Par21">As a general result, in both experiments tactile identification was less accurate than auditory identification. In parallel, the <em class="EmphasisTypeItalic ">bimodal</em> (i.e., simultaneously auditory and tactile) identification ranked significantly better in both experiments, providing evidence of support from touch to auditory material identification (Fig. <span class="InternalRef"><a href="#Fig3">12.3</a></span>).<figure class="Figure" id="Fig3"><div class="MediaObject" id="MO4"><img alt="" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Fig3_HTML.png" style="width:33.95em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 12.3</span><p class="SimplePara">Boxplot of and mean proportions correct with SE bars for all condition combinations. Left: Experiment 1. Right: Experiment 2</p></div></figcaption></figure>
</div><p class="Para" id="Par22">This conclusion was not contradicted by a control experiment, in which participants were asked to identify the materials from real bounces as during the training shown in Fig. <span class="InternalRef"><a href="#Fig2">12.2</a></span>, right.</p><p class="Para" id="Par23">Between Experiments 1 and 2, some interesting differences are observed between materials. In Experiment 1, metal was identified from auditory cues almost perfectly (difference between both plastic and wood was significant in multiple comparisons following a significant Friedman test: AuditoryWood-AuditoryMetal: Z <span class="InlineEquation" id="IEq7"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq7.png" style="width:0.87em"/></span> 4.3, Bonferroni-corrected p &lt; 0.01; AuditoryPlastic-AuditoryMetal: Z <span class="InlineEquation" id="IEq8"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq8.png" style="width:0.87em"/></span> 3.4, p <span class="InlineEquation" id="IEq9"><img alt="$${&amp;lt;}$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq9.png" style="width:1.06em"/></span> .01). In contrast, in Experiment 2, the identification of metal was the poorest of the three materials. In a two-way repeated-measures ANOVA with Greenhouse-Geisser correction for insphericity, a significant main effect of Material was detected (F(1.61,41.9) = 16.3, p <span class="InlineEquation" id="IEq10"><img alt="$$\le $$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq10.png" style="width:1.18em"/></span> 0.001). The 95% confidence intervals of the three materials result in a partial overlap between Plastic (0.51–0.64) and Metal (0.42–0.57), whereas the 95% CI for Wood is entirely above their combined range (0.65–0.78). As the main difference between the stimuli in Experiments 1 and 2 was the length of the decay, it seems that the longer decay in Experiment 1 was an important identification cue, especially for metal.</p><p class="Para" id="Par24">Importantly for this chapter, the ability of our subjects to maximize their identification accuracy when using sounds and vibrations together suggests that audio-tactile summation may work in all individuals as soon as they have acquired a solid knowledge about a multisensory event belonging to the everyday experience, and not only if they have accumulated peculiar audio-tactile skills, e.g., by practicing for a long time with a musical instrument. This conclusion was reinforced by a further test, part of the same research, where <em class="EmphasisTypeItalic ">incongruent</em> bimodal stimuli were prepared by assembling sounds and vibrations reporting respectively on two different materials. This test in fact suggested that tactile feedback, in its limited possibility to convey timbre, became progressively more relevant as the auditory channel, in front of incongruent materials, left its leading role while remaining supportive of cross-modal perception.</p></section><section class="Section1 RenderAsSection1" id="Sec5"><h2 class="Heading"><span class="HeadingNumber">12.3 </span>Reproduction of Target Pressing Forces</h2><div class="Para" id="Par25">An effect of haptic feedback on the control of finger-pressing force has been shown in the literature (e.g., [<span class="CitationRef"><a epub:type="biblioref" href="#CR1" role="doc-biblioref">1</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR28" role="doc-biblioref">28</a></span>]). The present setup [<span class="CitationRef"><a epub:type="biblioref" href="#CR27" role="doc-biblioref">27</a></span>] approaches a musical task in that it measures memorized force targets in the presence of both auditory and vibrotactile <span id="ITerm15">feedback</span>. The experiment was carried out by means of a tabletop device capable of measuring normal force while displaying vibrotactile feedback at its top panel (Fig. <span class="InternalRef"><a href="#Fig4">12.4</a></span>).<figure class="Figure" id="Fig4"><div class="MediaObject" id="MO5"><img alt="" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Fig4_HTML.png" style="width:20.72em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 12.4</span><p class="SimplePara">The interface used in the experiment for recording finger-pressing force and providing vibrotactile feedback</p></div></figcaption></figure>
</div><p class="Para" id="Par26">To simulate the haptic exchange taking place when playing acoustic or electroacoustic instruments—where musicians would learn the response of the instrument and would then perform by relying on kinesthetic memory [<span class="CitationRef"><a epub:type="biblioref" href="#CR38" role="doc-biblioref">38</a></span>]—participants first learned three target forces during a training phase, without additional feedback. Those targets were chosen empirically according to low, medium, and high pressing forces, within the data resolution of the interface (10-bit, corresponding to the 0–1023 range) and without anchoring them to corresponding values in Newton: the low target was set to 400, the medium one to 650, and the high target to 850. A double-sided window of 50 units was considered around each target as the acceptance range. The task was then to reproduce such forces “out of memory” under four feedback conditions: no feedback (N), auditory only (A), vibrotactile only (T), and auditory and vibrotactile together (AT). When participants believed they had reached the asked target they had to press an “OK” button with their free hand, while maintaining the pressing force on the touch panel.</p><p class="Para" id="Par27">For the sake of simplicity, a sinusoidal signal was chosen for rendering both auditory and tactile feedback, whose amplitude varied proportionally to the applied pressing force—thus implementing a gesture mapping commonly found in musical practice. The maximum intensity of vibrotactile stimuli was empirically set to the highest level that could be reproduced without perceivable distortion. The frequency of the sine wave was set to 200 Hz so as to maximize the produced vibrotactile sensation [<span class="CitationRef"><a epub:type="biblioref" href="#CR55" role="doc-biblioref">55</a></span>].</p><p class="Para" id="Par28">The test followed a 2-factor within-subjects design, where each participant was tested under each combination of conditions (12). All combinations were repeated 10 times, resulting in 120 trials that were presented in randomized order. Fourteen people (average age 33) participated in the experiment: five of them were pianists, five other musicians, and four non-musicians.<sup><a epub:type="noteref" href="#Fn1" id="Fn1_source" role="doc-noteref">1</a></sup>
</p><div class="Para" id="Par30">Data analysis<sup><a epub:type="noteref" href="#Fn2" id="Fn2_source" role="doc-noteref">2</a></sup> showed a significant main effect of feedback factor (F(3,143) <span class="InlineEquation" id="IEq11"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq11.png" style="width:0.87em"/></span> 16, p &lt; 0.0001). The effect of target force level was not significant (F(2,143) <span class="InlineEquation" id="IEq12"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq12.png" style="width:0.87em"/></span> 0.7, p <span class="InlineEquation" id="IEq13"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq13.png" style="width:0.87em"/></span> 0.52); however, the interaction “feedback <span class="InlineEquation" id="IEq14"><img alt="$$\times $$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq14.png" style="width:1.18em"/></span> target level” was significant (F(6,143) <span class="InlineEquation" id="IEq15"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq15.png" style="width:0.87em"/></span> 6.0, p &lt; 0.0001).<figure class="Figure" id="Fig5"><div class="MediaObject" id="MO6"><img alt="" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Fig5_HTML.png" style="width:27.03em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 12.5</span><p class="SimplePara">Interaction plots. Top panel: mean relative errors at the three target forces, presented for each feedback condition. Bottom panel: mean relative errors at the four feedback conditions, presented for each target force level</p></div></figcaption></figure>
</div><p class="Para" id="Par32">The interaction plots in Fig. <span class="InternalRef"><a href="#Fig5">12.5</a></span> show that, for the low target force, mean errors are much smaller in the presence of auditory (A) or audio-tactile (AT) feedback, and somewhat smaller with tactile-only feedback (T) than with no-feedback (N). For the medium target force, mean errors decrease in case of no-feedback (showing that the task becomes increasingly easier for higher forces) and with tactile-only feedback (T), whereas with auditory or audio-tactile feedback (A, AT) they did not change much from the low target force. For the high target force, however, the results are almost equivalent at all feedback conditions.</p><p class="Para" id="Par33">The results generally show that the addition of vibrations to auditory feedback may improve performance in musical finger-pressing tasks, enabling subjects to achieve memorized target forces with higher accuracy.</p></section><section class="Section1 RenderAsSection1" id="Sec6"><h2 class="Heading"><span class="HeadingNumber">12.4 </span>Vibrotactile Recognition of Traditional Musical Scales</h2><div class="Para" id="Par34">The harmonium, visible in Fig. <span class="InternalRef"><a href="#Fig6">12.6</a></span> (left), is played in both Western and Oriental music using scales that belong to the respective <span id="ITerm16">tradition</span>. Musicians and also listeners with a normal understanding of music immediately recognize the ethnicity of a scale. In fact, the human ear is especially accurate in assessing the intervals existing between the fundamental frequencies of musical notes.<figure class="Figure" id="Fig6"><div class="MediaObject" id="MO7"><img alt="" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Fig6_HTML.png" style="width:34.05em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 12.6</span><p class="SimplePara">Left: the harmonium. Right: experimental setup</p></div></figcaption></figure>
</div><p class="Para" id="Par35">Does a haptic counterpart of scale recognition ability exist, result of a tactile frequency identification process musicians have internalized as part of their practice on an instrument? And, if recognition does not occur, would they be able to at least discriminate between different ethnicities? If either answer was positive, then musical vibrations would prove to be active carriers of spectral information capable of supporting, or even substituting, an especially important component of the musical message coming from an instrument.</p><p class="Para" id="Par36">Western and Indian notes have fundamental frequencies that in general do not match; furthermore, such intervals between notes differ depending on the scale. As a result, clearly audible discrepancies exist between Western and Indian musical scales, and then between different scales belonging to the same ethnicity.</p><div class="Para" id="Par37">The stimuli for the experiment [<span class="CitationRef"><a epub:type="biblioref" href="#CR48" role="doc-biblioref">48</a></span>] consisted of two Western (<em class="EmphasisTypeItalic ">C natural</em> and <em class="EmphasisTypeItalic ">A minor</em>) and two Indian (<em class="EmphasisTypeItalic ">Raag Bhairav</em> and <em class="EmphasisTypeItalic ">Raag Yaman-Kalyan</em>) scales played on the harmonium in the setup of Fig. <span class="InternalRef"><a href="#Fig6">12.6</a></span> (right) by an Indian performer living in Europe. After listening to the four scales without touching the instrument during a training session, participants in a tactile recognition test were sitting on the left side of the same setup with their hands on the harmonium. At every trial, they were exposed to a train of vibrations corresponding to the sequence of notes belonging to a scale played by the performer. At the end of it, they had to decide whether the vibration was reporting about a Western or Indian scale, and to which one of the two. During the test, they neither wore headphones emitting masking noise nor could they observe the playing action, thanks to a panel standing amid the harmonium body, avoiding the performer and participant from seeing each other.<div class="Table" id="Tab2"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 12.2</span><p class="SimplePara">Individual subjective performance</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1 align-left"/><col class="tcol2 align-left"/><col class="tcol3 align-left"/><col class="tcol4 align-left"/><col class="tcol5 align-left"/></colgroup><thead><tr><th rowspan="2" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Subject typology</p></th><th colspan="2" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Western participants</p></th><th colspan="2" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Indian participants</p></th></tr><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Recognition of tradition</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Recognition of scale</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Recognition of tradition</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Recognition of scale</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">A—teacher of music</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">7/16</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">4/16</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">12/16</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">12/16</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">B—teacher of music</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">11/16</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">9/16</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">13/16</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">11/16</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">C—amateur musician</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">15/16</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">12/16</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">13/16</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">8/16</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">D—professional musician</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">16/16</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">12/16</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">10/16</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">7/16</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">E—professional musician</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">13/16</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">11/16</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">10/16</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">5/16</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Overall recognition</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">62/80</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">48/80</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">58/80</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">43/80</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Overall percentage</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">77.5%</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">60%</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">72.5%</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">53.75%</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Chance percentage</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">50%</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">25%</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">50%</p></td><td style="text-align: left;"><p class="SimplePara">25%</p></td></tr></tbody></table></div>
</div><p class="Para" id="Par38">The test was performed by a native group of Italians and then repeated in India. The two groups of participants, identical in number, were selected so as to have comparable levels of musical knowledge and performing skills. Results are listed in Table <span class="InternalRef"><a href="#Tab2">12.2</a></span>: They reveal the ability of both groups to recognize the ethnic origin with no significant differences between groups. Limited to specific subgroups, i.e., Western performers and Indian music teachers, the specific scale was recognized as well. The surprisingly high performance shown by our participants suggests the existence of a well-developed tactile memory for tones and/or note scales in musicians, a possible result of musical instrument training. However, the support during the task of nearly masked auditory cues of pitch bypassing the headphone insulators, or traveling from the hands to the cochlea through bone conduction, in principle could not be excluded. Similarly, scale-dependent temporal nuances biasing the recognition of the stimuli might have been unconsciously introduced by the performer during playing. In spite of its limited control, this experiment nevertheless represented an interesting starting point for the study of the role of touch in musical scale recognition.</p></section><section class="Section1 RenderAsSection1" id="Sec7"><h2 class="Heading"><span class="HeadingNumber">12.5 </span>Perception of Plucked Strings</h2><div class="Para" id="Par39"><em class="EmphasisTypeItalic ">Keytar</em> is a plucked-string instrument interface [<span class="CitationRef"><a epub:type="biblioref" href="#CR17" role="doc-biblioref">17</a></span>]. Its software was developed within the Unity3D development engine. While running on a PC, Keytar provides real-time auditory, visual, and haptic feedback to the player who controls a virtual plectrum through a Phantom Omni robotic arm with one hand, while selecting notes and chords with the other hand (see Fig. <span class="InternalRef"><a href="#Fig7">12.7</a></span>, left). An accurate haptic rendering of the interaction point was made possible by modeling each string as a queue of short cylinders with alternating radius, and then by characterizing the contact of the plectrum using physical parameters which, due to the elastic behavior of the string, fall within the operating range of the Phantom Omni (see Fig. <span class="InternalRef"><a href="#Fig7">12.7</a></span>, right). This way, the robotic arm not only reproduces the elastic response of the plucked strings, but also some fine-grained dynamic textures arising between the colliding plectrum and the vibrating string. The sensation of rubbing the string during plucking is further enhanced by a realistic noise of frictional contacts coming from the servo-mechanisms of the robotic arm, while they are continuously switched on and off by the collision detection software module. The overall virtual environment defined an especially convincing reproduction of string plucking [<span class="CitationRef"><a epub:type="biblioref" href="#CR45" role="doc-biblioref">45</a></span>].<figure class="Figure" id="Fig7"><div class="MediaObject" id="MO8"><img alt="" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Fig7_HTML.png" style="width:31.3em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 12.7</span><p class="SimplePara">Left: Keytar. Right: particular of the plectrum-string interaction point</p></div></figcaption></figure>
</div><p class="Para" id="Par40">In a virtual reality experiment [<span class="CitationRef"><a epub:type="biblioref" href="#CR44" role="doc-biblioref">44</a></span>], twenty-nine participants on average having 8.2 years (SD <span class="InlineEquation" id="IEq16"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq16.png" style="width:0.87em"/></span> 8.3) of regular practice on a music instrument were asked to first pluck the strings of a real guitar, and then to wear an Oculus Rift CV1 helmet displaying an electric guitar and a plectrum in a nondescript virtual room. Twenty-one such participants in particular reported being able to play one or more stringed instruments. Interaction with the plectrum was made possible using the robotic arm controlled by Keytar, furthermore, the collision detection module controlled also a vibro-tactile actuator standing below Phantom Omni. This active stand was used to produce additional vibrations independently of the kinesthetic feedback. On such a setup, a within-subjects study compared four different haptic conditions during plucking: no feedback (N), force only (F), vibration only (V), and force and vibration together (FV).</p><div class="Para" id="Par41">Each participant was exposed to every condition, in randomized order, for approximately 20 minutes each. On every condition, first all six strings were plucked twice in a randomized order by the guidance of a visual marker emphasizing the string to pluck; then, participants were encouraged to freely interact by both plucking each string individually and strumming the entire string set. When one condition was completely tested, each participant evaluated four metrics on a Likert scale (see Fig. <span class="InternalRef"><a href="#Fig8">12.8</a></span>): overall perceptual similarity with the real instrument (from completely different to identical); stiffness similarity between virtual and real strings (from much lower to much higher); overall realism of the virtual instrument (from strong disagreement to strong agreement); touch realism of the virtual strings (from strong disagreement to strong agreement); effects of haptic cues on sound <span id="ITerm17">realism</span>. At the end of the test each participant was additionally asked to choose his/her preferred condition. Finally, the errors made on plucking a wrong instead of a visually marked string during the part of the test involving individual strings were logged.<figure class="Figure" id="Fig8"><div class="MediaObject" id="MO9"><img alt="" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Fig8_HTML.png" style="width:26.35em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 12.8</span><p class="SimplePara">Keytar: experimental results</p></div></figcaption></figure>
</div><p class="Para" id="Par42">Results suggest the existence of significant effects of haptic feedback on the perceived realism of the strings. Further considerations can be drawn from the specific histograms [<span class="CitationRef"><a epub:type="biblioref" href="#CR46" role="doc-biblioref">46</a></span>]. By contrast, as can be seen from the left histogram below in Fig. <span class="InternalRef"><a href="#Fig8">12.8</a></span>, no effects on sound realism were measured. The lesson to take home from this experiment, hence, is that increasing the haptic realism of a virtual musical instrument in principle has no effects on its perceived auditory quality.</p></section><section class="Section1 RenderAsSection1" id="Sec8"><h2 class="Heading"><span class="HeadingNumber">12.6 </span>Piano Playing</h2><p class="Para" id="Par43">A different lesson was instead learnt from an experiment in which the realism of the interaction with the musical instrument, in this case a piano, was pushed to its limit [<span class="CitationRef"><a epub:type="biblioref" href="#CR18" role="doc-biblioref">18</a></span>]. The piano keyboard in fact offers a controlled experimental setting, as the performer can only hit and then release one or more keys with one or more fingers while the rest of their body is disconnected from the <span id="ITerm18">instrument</span>. This setting permitted to design a task in which auditory and haptic feedback could be delivered separately and independently. Furthermore, the intensity of both feedback channels is a reliable function of the key velocity which, in turn, is driven by the pianist’s finger. Under these experimental premises, Yamaha’s Disklavier pianos in particular offer two specific advantages: first, they can both record and mechanically reproduce the action of a pianist on all keys; secondly, they can be automatically switched between normal operation and a <em class="EmphasisTypeItalic ">silent</em> mode. When this mode is set, all strings are decoupled from the respective key hammers in ways that the instrument produces no sound, meanwhile conveying the same haptic feedback as to when the performer also hears the instrument.</p><div class="Para" id="Par44">The group of participants was split into two independent subgroups. Either subgroup performed on a grand Disklavier model DC3 M4 (in Padova, Italy) or on an upright model DU1A (in Zurich, Switzerland). During the tasks, the acoustic and silent modes were randomly switched across trials, letting the participants receive either natural or no steady vibrations from the keys after the initial percussive event. In both configurations participants via insulated headphones received the same auditory feedback, consisting of piano sounds synthesized by Modartt Pianoteq 4.5 digital piano software which was set to simulate a grand or an upright piano, and was driven in real time by the respective Disklavier’s Musical Instrument Digital Interface (MIDI)<span id="ITerm19"/>. The synthetic sounds were equalized so as to match those of the corresponding piano, by positioning a KEMAR mannequin visible in Fig. <span class="InternalRef"><a href="#Fig9">12.9</a></span> (left), where the setup is shown during the calibration <span id="ITerm20">procedure</span>. Figure <span class="InternalRef"><a href="#Fig9">12.9</a></span> (right) shows a typical train of vibrations reaching the pianist’s finger when the piano was operating in acoustic mode: the initial percussion event preceding the vibrations coming from the strings is evident in this figure.<figure class="Figure" id="Fig9"><div class="MediaObject" id="MO10"><img alt="" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Fig9_HTML.png" style="width:30.9em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 12.9</span><p class="SimplePara">Left: setup calibration. Right: acceleration signal measured on the key surface (note A2; MIDI velocity equal to 12; grand piano)</p></div></figcaption></figure>
</div><p class="Para" id="Par45">Participants performed first a playing task and then a rating task. The former is relevant for this chapter. Three note ranges were considered separately across the keyboard, labeled low (keys below D3), mid (keys between D3 and A5), and high (keys above A5). Participants could play freely, within one range at a time, to compare the quality of the instrument in the presence and absence of string vibrations following the initial percussive events. Twenty-five professional pianists, mostly classical and a few jazz, took part in the tests: 15 on the upright and 10 on the grand piano (the slight imbalance in group sizes was due to varying easiness of recruitment in the two locations). Their average age was 27 years and their average piano experience was 15 years. Using a manual control, they could switch at their convenience between two setups, X and Y, associated with the silent and acoustic modes of the Disklavier. The difference between the two setups was not explained to them.</p><div class="Para" id="Par46">The task was to compare the setups on a Likert scale (from “X much better than Y” until “Y much better than X”) with respect to the following attributes: dynamic range, loudness, richness, naturalness, and preference. The first four were rated separately in the low, mid, and high ranges, while the preference rating was given considering the entire keyboard. Participants were given definitions of the attributes and informed that dynamic range, loudness, and richness were mainly related to sound, whereas naturalness and preference could also be related to touch. A laptop finding place next to the piano displayed a set of sliders that were accessible at any moment to pianists for rating such <span id="ITerm21">attributes</span>.<figure class="Figure" id="Fig10"><div class="MediaObject" id="MO11"><img alt="" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Fig10_HTML.png" style="width:20.62em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 12.10</span><p class="SimplePara">Results with errorbars ±SE. Positive values signify preference for the vibrating mode. X-axis presents ratings for dynamic range, loudness, richness, and naturalness at low (A0-D3), mid (D3-A5), and high ranges (A5-C7) (l, m, and h, respectively). Preference was rated in full range only</p></div></figcaption></figure>
<figure class="Figure" id="Fig11"><div class="MediaObject" id="MO12"><img alt="" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Fig11_HTML.png" style="width:22.15em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 12.11</span><p class="SimplePara">Quality rating profiles projected onto the first two principal components. Subjects were segmented a posteriori according to positive/negative rating on preference. Ellipses enclose 68% of subjects in each group</p></div></figcaption></figure>
</div><p class="Para" id="Par47">Results are shown in Figs. <span class="InternalRef"><a href="#Fig10">12.10</a></span> and <span class="InternalRef"><a href="#Fig11">12.11</a></span>, suggesting a general preference for the vibrating mode. Since this preference was not explicitly linked to a specific attribute, two principal components, PC1 and PC2, were discovered to account for 80% of the variance. PC1 had the highest positive correlations with richness, naturalness, and preference; PC2, less powerful, was associated with dynamic range and loudness, which conversely decrease as naturalness and preference increase.</p><p class="Para" id="Par48">Analysis of Lin concordance correlation coefficients revealed a subgroup of seven subjects whose inter-individual consistency was negative. It was observed that most of them belonged to the group of five subjects who gave a negative preference rating. Therefore, participants were segmented a posteriori based on a positive versus negative preference rating. As seen in Fig. <span class="InternalRef"><a href="#Fig11">12.11</a></span>, the negative group differs from the majority of participants in that their ratings are negative on both principal components; in fact, while both groups gave rather similar ratings for dynamic range and loudness, their mean ratings for richness, naturalness, and preference are nearly opposite to each other. The conclusion was that approximately 80% of the participants preferred the vibrating setup and perceived higher naturalness and richness from it. Why the remaining 20% did not perceive any benefits from vibrations could not be thoroughly explained; however, in that group were two subjects who performed significantly under average in a vibration detection experiment related to this study. Notably, the negative group also included some jazz <span id="ITerm22">pianists</span>. They reported performing frequently in small ensembles where digital stage pianos are used, which lack the natural vibrotactile feedback found on acoustic pianos.</p><p class="Para" id="Par49">At any rate, after completing the test in Zurich, the experimenter asked each participant what may have caused the difference between the setups: Interestingly, only 1 out of 15 participants could pinpoint vibrations. Thus, while the participants generally preferred the vibrating setup, they were not actively aware of vibrations. Their unawareness testifies to the especially high level of cross-modal integration that piano sounds and vibrations achieve in a real instrument.</p></section><section class="Section1 RenderAsSection1" id="Sec9"><h2 class="Heading"><span class="HeadingNumber">12.7 </span>Digital Piano Playing</h2><div class="Para" id="Par50">An effect related to what was observed on acoustic pianos was discovered to play a role with digital pianos [<span class="CitationRef"><a epub:type="biblioref" href="#CR19" role="doc-biblioref">19</a></span>]. Since electronic instruments do not vibrate except for possible mechanical perturbations coming from the internal speakers, potential additional effects of artificial vibratory feedback to perceived instrument quality, precision in timing, and dynamic performance were investigated. The setup definition required to disassemble a digital piano keyboard, and then attach two vibrotactile actuators (Fig. <span class="InternalRef"><a href="#Fig12">12.12</a></span>, right) on a stiff wooden panel which was firmly screwed below its keybed (Fig. <span class="InternalRef"><a href="#Fig12">12.12</a></span>, left).<figure class="Figure" id="Fig12"><div class="MediaObject" id="MO13"><img alt="" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Fig12_HTML.png" style="width:31.32em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 12.12</span><p class="SimplePara">Left: experimental setup. Right: transducer conveying vibrations to the keyboard</p></div></figcaption></figure>
</div><p class="Para" id="Par51">These actuators conveyed stimuli that had previously been acquired from an acoustic piano. In parallel, binaurally recorded tones were reproduced using headphones. Such tones and vibrations had previously been calibrated to have an intensity equal to that measured on the finger and ears of a pianist performing on a Disklavier grand piano, in the same fashion as the experiment in Sect. <span class="InternalRef"><a href="#Sec8">12.6</a></span>. In particular, calibration is required to equalize the vibration signals in order to avoid unrealistic resonance peaks on the digital keyboard for certain played notes.</p><p class="Para" id="Par52">Eleven pianists, five females and six males, participated in the experiment. Their average age was 26 years, and their average piano playing experience was 8 years after reaching the conservatory level. Two participants were jazz pianists. Audio-tactile stimuli were produced at runtime: the digital keyboard in fact sent MIDI messages to a computer running Modartt Pianoteq 4.5 piano synthesizer and, in parallel, Native Instruments Kontakt 5 sampler in series with MeldaProduction MEqualizer parametric equalizer for playing back the corresponding vibration <span id="ITerm23">samples</span>.</p><div class="Para" id="Par53">Perceived instrument quality was assessed by feeding the digital keyboard respectively with (A) no vibrations, (B) grand piano vibrations, (C) grand piano vibrations with 9 dB boost, and (D) synthetic vibrations. By contrast, the sound synthesis parameters were kept constant throughout the experiment. Pianists were asked to play freely while assessing the experience on five attribute rating scales: Dynamic control, Richness, Engagement, Naturalness, and General preference. During playing, at their convenience they could switch among two unknown setups, <span class="InlineEquation" id="IEq17"><img alt="$$\alpha $$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq17.png" style="width:0.87em"/></span> and <span class="InlineEquation" id="IEq18"><img alt="$$\beta $$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq18.png" style="width:0.94em"/></span>: the former was always made to correspond to A, whereas the latter could randomly correspond to B, C, or D. The assessment was conducted by rating <span class="InlineEquation" id="IEq19"><img alt="$$\beta $$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq19.png" style="width:0.94em"/></span> relatively to <span class="InlineEquation" id="IEq20"><img alt="$$\alpha $$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq20.png" style="width:0.87em"/></span> during 10 minutes of piano performance, for a session that hence lasted half an hour. During each assessment, participants at any time could rate every attribute by pointing to the respective virtual slider and setting a level by clicking with the mouse on a graphical user interface that was displayed by a laptop computer at hand reach. Each slider exposed a continuous Comparison Category Rating scale ranging from –3 (“<span class="InlineEquation" id="IEq21"><img alt="$$\beta $$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq21.png" style="width:0.94em"/></span> much better than <span class="InlineEquation" id="IEq22"><img alt="$$\alpha $$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq22.png" style="width:0.87em"/></span>”) to +3 (“<span class="InlineEquation" id="IEq23"><img alt="$$\beta $$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq23.png" style="width:0.94em"/></span> much worse than <span class="InlineEquation" id="IEq24"><img alt="$$\alpha $$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq24.png" style="width:0.87em"/></span>”). Once the quality rating of the keyboard was over, another half an hour was spent by each participant to participate in the remaining two tests, assessing precision in timing as well as dynamic performance.<figure class="Figure" id="Fig13"><div class="MediaObject" id="MO14"><img alt="" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Fig13_HTML.png" style="width:18.68em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 12.13</span><p class="SimplePara">Results of the quality experiment. Boxplot presenting median and quartile for each attribute scale and vibration condition</p></div></figcaption></figure>
</div><div class="Para" id="Par54">Results show that the augmented setups were generally preferred, with an emphasis on boosted vibrations (Fig. <span class="InternalRef"><a href="#Fig13">12.13</a></span>). Again, heterogeneity was observed in the data, as might be expected due to the high degree of variability in the inter-individual agreement scores. A k-means clustering algorithm was used to segment the subjects a posteriori into two classes, according to their opinion on General preference. Eight subjects were classified into a “positive” group and the remaining three into a “negative” group. The results of the respective groups are presented in Fig. <span class="InternalRef"><a href="#Fig14">12.14</a></span>. A difference of opinion is evident: The median ratings for the preferred setup C are nearly +2 in the positive group and –1.5 in the negative group for General preference. In the positive group, the median was positive in all cases except for Naturalness in D, whereas in the negative group, the median was positive only for Dynamic control in B.<figure class="Figure" id="Fig14"><div class="MediaObject" id="MO15"><img alt="" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Fig14_HTML.png" style="width:31.88em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 12.14</span><p class="SimplePara">Differences in quality ratings between the positive (left) and negative (right) groups formed by a posteriori segmentation. Boxplot presenting median and quartile for each attribute scale and vibration condition</p></div></figcaption></figure>
</div><p class="Para" id="Par55">Similar to what was observed in Sect. <span class="InternalRef"><a href="#Sec8">12.6</a></span> while experimenting with the acoustic piano, low concordance between pianists exposed to vibration suggests that intra- and inter-individual consistency is an issue also while playing a digital piano. By contrast, no effect was observed on timing or dynamics accuracy in the performance tests. Taken together, these considerations point to conclude that vibrations do unconsciously influence the perceived keyboard instrument quality, however, along a direction which depends on the performer’s previous multisensory experience of a specific instrument. Hence, augmenting a digital piano with the vibrations of an acoustic piano might not increase sense of quality if the performer played a digital (i.e., non vibrating) keyboard for most of the time. In parallel, haptic augmentation neither improves nor disrupts key aspects of piano performance such as timing and dynamic <span id="ITerm24">control</span>.</p></section><section class="Section1 RenderAsSection1" id="Sec10"><h2 class="Heading"><span class="HeadingNumber">12.8 </span>Playing Experience on a Haptic Surface for Musical Expression</h2><div class="Para" id="Par56">A multi-touch force-sensitive surface for musical expression was equipped with multi-point localized vibrotactile feedback, resulting in the HSoundplane haptic interface [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>] shown <span id="ITerm25">in</span> Fig. <span class="InternalRef"><a href="#Fig15">12.15</a></span>. A subjective assessment was conducted using the HSoundplane, which measured how the presence and type of vibration affect the perceived quality of the device, as well as various attributes related to the playing experience [<span class="CitationRef"><a epub:type="biblioref" href="#CR41" role="doc-biblioref">41</a></span>].<figure class="Figure" id="Fig15"><div class="MediaObject" id="MO16"><img alt="" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Fig15_HTML.png" style="width:17.28em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 12.15</span><p class="SimplePara">The experimental setting for the HSoundplane experiment</p></div></figcaption></figure>
</div><section class="Section2 RenderAsSection2" id="Sec11"><h3 class="Heading"><span class="HeadingNumber">12.8.1 </span>Design</h3><p class="Para" id="Par57">Two clearly distinct sound presets were tested, each with three vibrotactile feedback strategies.</p><div class="Para" id="Par58">The pitch of the audio feedback—ranging from A2 (<span class="InlineEquation" id="IEq25"><img alt="$$f_0=110\,\textrm{Hz}$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq25.png" style="width:5.31em"/></span>) to D5 (<span class="InlineEquation" id="IEq26"><img alt="$$f_0=587.33\,\textrm{Hz}$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq26.png" style="width:6.63em"/></span>)—was controlled along the x-axis. The two offered sound <span id="ITerm26">presets</span> were  <div class="DefinitionList"><dl><dt class="Term"><strong class="EmphasisTypeBold ">Sound 1</strong>—</dt><dd class="Description"><p class="Para" id="Par59">A sawtooth wave filtered by a resonant low-pass and modulated by a vibrato effect (i.e., amplitude and pitch modulation). A markedly expressive setting, responding to subtleties and nuances in the performer’s gesture. y-axis control: Vibrato intensity is controlled along the y-axis, from no-vibrato (bottom) to strong vibrato (top). z-axis control: The filter cutoff frequency is controlled by the applied pressing force (i.e., higher force maps to brighter sound), and so is the sound level (i.e., higher force maps to louder sound).</p></dd><dt class="Term"><strong class="EmphasisTypeBold ">Sound 2</strong>—</dt><dd class="Description"><p class="Para" id="Par60">A simple sine wave is added with noise depending on the location on the y-axis. A setting offering a rather limited sonic palette and no amplitude dynamics. y-axis control: Moving upwards adds white noise of increasing amplitude, filtered by a resonant band-pass. The filter’s center frequency follows the pitch of the respective tone. z-axis control: Pressing force data are ignored, resulting in fixed intensity.</p></dd></dl></div>   The different degrees of variability and expressive potential of the two sound settings allowed us to investigate whether the possible effect depends on audio feedback characteristics. All sounds were processed by a reverb effect so as to make the playing experience more acoustic-like. Sound was provided to the participants by means of closed-back headphones (Beyerdynamic DT 770 Pro). Audio examples of the two sound types are made available online,<sup><a epub:type="noteref" href="#Fn3" id="Fn3_source" role="doc-noteref">3</a></sup> demonstrating C3, C4, and C5 tones modulated along the y- and z-axes.</div><div class="Para" id="Par62">Before being routed to the actuators layer, vibration signals were filtered in the <span class="InlineEquation" id="IEq27"><img alt="$$10\mathrm {-}500\,\textrm{Hz}$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq27.png" style="width:4.82em"/></span> range by a 10th-order band-pass, so as to optimize the actuators’ efficiency and consequently the vibratory response of the device, as well as to minimize sound leakage. Any residual sound spillage produced by the actuators was taken care of by the closed-back headphones carrying auditory feedback. Three vibrotactile strategies were implemented:  <div class="DefinitionList"><dl><dt class="Term"><strong class="EmphasisTypeBold ">Sine</strong>—</dt><dd class="Description"><p class="Para" id="Par63">Pure sinusoidal signals, whose pitch follows the fundamental of the played tones (<span class="InlineEquation" id="IEq28"><img alt="$$f_0$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq28.png" style="width:1.06em"/></span> within <span class="InlineEquation" id="IEq29"><img alt="$$110\mathrm {-}587.33\,\textrm{Hz}$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq29.png" style="width:6.57em"/></span>), and whose amplitude is controlled by the intensity of the pressing forces. By focusing vibratory energy at a single frequency component, this setting aimed at producing sharp vibrotactile feedback.</p></dd><dt class="Term"><strong class="EmphasisTypeBold ">Audio</strong>—</dt><dd class="Description"><p class="Para" id="Par64">The same sounds generated by the HSoundplane used to render vibration: the audio signals are also routed to the actuators layer. Vibration signals thus share the same spectrum (within the <span class="InlineEquation" id="IEq30"><img alt="$$10\mathrm {-}500\,\textrm{Hz}$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq30.png" style="width:4.82em"/></span> pass-band) and dynamics of the related sound. This approach ensured the highest coherence between musical output and tactile feedback, mimicking what occurs on acoustic musical instruments, where the source of vibration coincides with that of sound.</p></dd><dt class="Term"><strong class="EmphasisTypeBold ">Noise</strong>—</dt><dd class="Description"><p class="Para" id="Par65">A white noise signal of fixed amplitude. This setting produced vibrotactile feedback generally uncorrelated with the auditory one, ignoring any spectral and amplitude cues possibly conveyed by it. The only exception is with Sound 2 and high y-axis values, which resulted in a similar noisy signal.</p></dd></dl></div>   The designed vibration types offered different spectral and dynamics cues resulting in varying degrees of similarity with the audio feedback, thus enabling to determine the importance of the match between sound and vibration. The intensity of vibration feedback was set by the authors in a pilot phase, aiming at two main goals: (i) sound and vibration intensities had to feel reciprocally consistent; (ii) while levels had to be overall comfortable for prolonged use, vibration had to be clearly perceivable even at low force-pressing values [<span class="CitationRef"><a epub:type="biblioref" href="#CR42" role="doc-biblioref">42</a></span>].</div><p class="Para" id="Par66">At each trial, the task was to play freely while comparing two related setups: they were labeled A/B in a balanced way, and differed only in the presence/absence of vibration (i.e., they shared the same sound setting). Participants could switch at any time between A and B and had to provide ratings for four attributes: <em class="EmphasisTypeItalic ">Preference</em>, <em class="EmphasisTypeItalic ">Control and responsiveness</em> (referred to as <em class="EmphasisTypeItalic ">Control</em>), <em class="EmphasisTypeItalic ">Expressive potential</em> (referred to as <em class="EmphasisTypeItalic ">Expression</em>), and <em class="EmphasisTypeItalic ">Enjoyment</em>. Ratings were given by adjusting a respective slider on a continuous visual analog scale ranging from A (left) to B (right) to reflect the degree of preference in terms of the given attribute. In case of perceived equality between A and B, the slider would be set to the midpoint. All 4 (attributes) <span class="InlineEquation" id="IEq31"><img alt="$$\times $$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq31.png" style="width:1.18em"/></span> 3 (vibration types) <span class="InlineEquation" id="IEq32"><img alt="$$\times $$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq32.png" style="width:1.18em"/></span> 2 (sound types) factor combinations were evaluated twice.</p><p class="Para" id="Par67">All 29 participants—7 males and 22 females, aged 18–48 years (M <span class="InlineEquation" id="IEq33"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq33.png" style="width:0.87em"/></span> 25.4, SD <span class="InlineEquation" id="IEq34"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq34.png" style="width:0.87em"/></span> 7.1)—were professional musicians or music students. Their main instrument was either a keyboard or a string instrument, on which they had on average 17 years of experience. Roughly one-third of the participants had significant experience with electronic musical instruments, mostly synthesizers, or digital musical interfaces.</p></section><section class="Section2 RenderAsSection2" id="Sec12"><h3 class="Heading"><span class="HeadingNumber">12.8.2 </span>Results</h3><div class="Para" id="Par68">The continuous slider scale ratings were mapped to the closed interval [0, 1], where 1 indicates a maximal preference for the vibrating setup and 0 maximal preference for the non-vibrating setup, and 0.5 is the point of perceived equality. Statistical analysis was carried out by fitting a zero-one-inflated beta (ZOIB) model, whose parameters were estimated with Bayesian methods [<span class="CitationRef"><a epub:type="biblioref" href="#CR9" role="doc-biblioref">9</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR32" role="doc-biblioref">32</a></span>]. Four parameters describe the ZOIB distribution: the mean (<span class="InlineEquation" id="IEq35"><img alt="$$\mu $$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq35.png" style="width:0.94em"/></span>) and precision (<span class="InlineEquation" id="IEq36"><img alt="$$\phi $$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq36.png" style="width:1em"/></span>) of the beta distribution, the probability of a binary <span class="InlineEquation" id="IEq37"><img alt="$$\{0,1\}$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq37.png" style="width:2.94em"/></span> outcome (zoi), and the conditional probability of outcome <span class="InlineEquation" id="IEq38"><img alt="$$\{1\}$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq38.png" style="width:1.94em"/></span> (coi). The mean of the beta distribution was modeled by sound, vibration type, their interaction, and attribute. The models for the precision (<span class="InlineEquation" id="IEq39"><img alt="$$\phi $$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq39.png" style="width:1em"/></span>) and zero-one-inflation parameters (zoi, coi) were set to depend on vibration type, sound, and attribute without interactions.<figure class="Figure" id="Fig16"><div class="MediaObject" id="MO17"><img alt="" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Fig16_HTML.png" style="width:20.72em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 12.16</span><p class="SimplePara">Marginal effects; estimated <span class="InlineEquation" id="IEq40"><img alt="$$\mu $$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq40.png" style="width:0.94em"/></span> parameters with 95% Credible Intervals (N <span class="InlineEquation" id="IEq41"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq41.png" style="width:0.87em"/></span> 29). <strong class="EmphasisTypeBold ">a</strong> Interaction between vibration and sound type; <strong class="EmphasisTypeBold ">b</strong> Effect of vibration on the evaluated attributes. 0.50 <span class="InlineEquation" id="IEq42"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq42.png" style="width:0.87em"/></span> point of perceived equality; higher values indicate preference for vibrating over non-vibrating setup</p></div></figcaption></figure>
</div><p class="Para" id="Par69">Estimates for the beta distribution means and their corresponding 95% Credible Intervals are presented in Fig. <span class="InternalRef"><a href="#Fig16">12.16</a></span>. On average, the vibrating setups were preferred to their non-vibrating versions: all mean estimates but one are above 0.50 (the point of perceived equality) as well as most of the respective credible intervals.</p><p class="Para" id="Par70">The model output showed the following effects.<sup><a epub:type="noteref" href="#Fn4" id="Fn4_source" role="doc-noteref">4</a></sup> The mean parameter for Audio vibration was not credibly different from Sine vibration, while Noise vibration was rated credibly lower. Sound type had a credible effect on the mean parameter (<span class="InlineEquation" id="IEq43"><img alt="$$\mu $$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq43.png" style="width:0.94em"/></span>) only in combination with Noise vibration. <em class="EmphasisTypeItalic ">Expression</em> and <em class="EmphasisTypeItalic ">Enjoyment</em> both had a rather credible positive effect, although slightly short of 95%, on the mean parameter relative to <em class="EmphasisTypeItalic ">Preference</em> and <em class="EmphasisTypeItalic ">Control</em>. However, many of the manipulated factors had credible effects on the precision parameter (<span class="InlineEquation" id="IEq44"><img alt="$$\phi $$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq44.png" style="width:1em"/></span>) and on the zero-inflation parameter (zoi), suggesting that even if the means are not credibly different, the shapes of the respective distributions may differ.</p><p class="Para" id="Par72">The main findings of this study may be summarized as follows: i) although not large, the measured effect of Sine or Audio vibration was appreciably positive. ii) Noise vibration did not credibly enhance the subjective quality of the interface as compared to the non-vibrating condition. iii) Vibrotactile feedback especially increased the perceived expressiveness of the interface and the enjoyment of playing. As appears from Fig. <span class="InternalRef"><a href="#Fig16">12.16</a></span> (a), a more marked effect was found when vibration was more similar to the sonic feedback and consistent with the user’s gesture: Indeed, Sine and Audio vibration follow the pitch of the produced sound and their intensity can be controlled by pressure. Conversely, Noise vibration—offering fixed amplitude, independent of the input gesture, and flat spectrum—was rated lowest among the vibrating setups. Noise vibration resulted in slightly better ratings when Sound 2 was used as compared to Sound 1: Again, that was likely because vibrotactile feedback is consistent, at least partially, with the noise-like sonic feedback produced for high y-axis values. Interestingly, no credible difference in the globally positive effect was found between Sine and Audio vibration. This may be at least partially explained by a masking effect taking place in the tactile domain toward higher frequencies, thus impairing waveform discrimination [<span class="CitationRef"><a epub:type="biblioref" href="#CR5" role="doc-biblioref">5</a></span>]. However, such phenomenon seems not to apply to markedly different signals [<span class="CitationRef"><a epub:type="biblioref" href="#CR49" role="doc-biblioref">49</a></span>]. In this regard, our informal testing revealed that Sine and Audio vibration were virtually indistinguishable, especially when Sound 1 (modulated sawtooth waveform) was selected.</p><div class="Para" id="Par73">Response consistency across repetitions was evaluated by modeling participants’ first- and second-round responses by linear regression. Pooled over participants and factor combinations, the regression coefficient (<span class="InlineEquation" id="IEq45"><img alt="$$\beta $$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq45.png" style="width:0.94em"/></span> <span class="InlineEquation" id="IEq46"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq46.png" style="width:0.87em"/></span> 0.32, p &lt; 0.001) indicated a general overall consistency (i.e., participants preferred the same vibrating or non-vibrating setup twice across repetitions). However, ten participants frequently preferred once the vibrating and once the non-vibrating setup in the same factor combination, resulting in regression coefficients <span class="InlineEquation" id="IEq47"><img alt="$${\le } 0$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq47.png" style="width:1.5em"/></span> (mean coefficient over the N <span class="InlineEquation" id="IEq48"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq48.png" style="width:0.87em"/></span> 10 subjects was <span class="InlineEquation" id="IEq49"><img alt="$$\beta = -0.19$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq49.png" style="width:4.82em"/></span>). The remaining subjects (N <span class="InlineEquation" id="IEq50"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq50.png" style="width:0.87em"/></span> 19) instead gave consistent ratings (<span class="InlineEquation" id="IEq51"><img alt="$$\beta = 0.53$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq51.png" style="width:4em"/></span>). Interestingly, the inconsistent group (N <span class="InlineEquation" id="IEq52"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq52.png" style="width:0.87em"/></span> 10) spent noticeably less time with the tasks than the reliable group (N <span class="InlineEquation" id="IEq53"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq53.png" style="width:0.87em"/></span> 19): the median length of their gestural data logs was only 62% of that of the consistent group. In order to estimate the effect of the inconsistent participants, we re-run the ZOIB model including only the N <span class="InlineEquation" id="IEq54"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq54.png" style="width:0.87em"/></span> 19 consistent subjects and finding that the main result was similar to the full dataset: only vibration type had a clearly credible effect on the estimated mean parameter. However, this way the effect is somewhat larger, as the mean estimates for vibration types Sine and Audio (with Sound 1) slightly increase, while that for Noise decreases (see Table <span class="InternalRef"><a href="#Tab3">12.3</a></span>). Also in this case, <em class="EmphasisTypeItalic ">Expression</em> is the highest rated attribute; its marginal mean estimate increases from 0.59 to 0.64 (see Table <span class="InternalRef"><a href="#Tab4">12.4</a></span>).<div class="Table" id="Tab3"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 12.3</span><p class="SimplePara">Estimated <span class="InlineEquation" id="IEq55"><img alt="$$\mu $$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq55.png" style="width:0.94em"/></span> parameters from the ZOIB fit (on original response scale) for the marginal effects of sound and vibration (attribute <span class="InlineEquation" id="IEq56"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq56.png" style="width:0.87em"/></span> <em class="EmphasisTypeItalic ">Preference</em>). N <span class="InlineEquation" id="IEq57"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq57.png" style="width:0.87em"/></span> 29: all subjects; N <span class="InlineEquation" id="IEq58"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq58.png" style="width:0.87em"/></span> 19: consistent subjects</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1 align-left"/><col class="tcol2 align-left"/><col class="tcol3 align-left"/><col class="tcol4 align-left"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Sound</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Vibration</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Estimate (N <span class="InlineEquation" id="IEq59"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq59.png" style="width:0.87em"/></span> 29)</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Estimate (N <span class="InlineEquation" id="IEq60"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq60.png" style="width:0.87em"/></span> 19)</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Sine</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.563</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.604</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Audio</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.548</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.576</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">1</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Noise</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.480</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.466</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">2</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Sine</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.536</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.558</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">2</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Audio</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.552</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.550</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">2</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Noise</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.512</p></td><td style="text-align: left;"><p class="SimplePara">0.493</p></td></tr></tbody></table></div>
<div class="Table" id="Tab4"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 12.4</span><p class="SimplePara">Estimated <span class="InlineEquation" id="IEq61"><img alt="$$\mu $$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq61.png" style="width:0.94em"/></span> parameters from the ZOIB fit (on original response scale) for the marginal effects of Attribute (sound <span class="InlineEquation" id="IEq62"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq62.png" style="width:0.87em"/></span> Sound 1, vibration <span class="InlineEquation" id="IEq63"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq63.png" style="width:0.87em"/></span> Sine). N <span class="InlineEquation" id="IEq64"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq64.png" style="width:0.87em"/></span> 29: all subjects; N <span class="InlineEquation" id="IEq65"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq65.png" style="width:0.87em"/></span> 19: consistent subjects</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1 align-left"/><col class="tcol2 align-left"/><col class="tcol3 align-left"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Attribute</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Estimate N <span class="InlineEquation" id="IEq66"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq66.png" style="width:0.87em"/></span> 29</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Estimate N <span class="InlineEquation" id="IEq67"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq67.png" style="width:0.87em"/></span> 19</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Preference</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.563</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.604</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Control</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.562</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.594</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Expression</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.594</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.645</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Enjoy</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">0.591</p></td><td style="text-align: left;"><p class="SimplePara">0.628</p></td></tr></tbody></table></div>
</div><p class="Para" id="Par74">As the participants were highly skilled musicians, we believe that the recorded inconsistent responses were not due to the task being too difficult. However, as they were not screened for individual vibrotactile sensitivity, it is possible that they did not feel vibrations equally. On top of that, we argue that rating inconsistency may be linked to the varying perceived vibration strength and audio-tactile congruence, depending on where and how the participants were playing over the interface’s surface. Indeed, vibrotactile intensity perception is affected by vibration amplitude (obviously), spectral content (with a peak in the <span class="InlineEquation" id="IEq68"><img alt="$$200\mathrm {-}300\,\textrm{Hz}$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq68.png" style="width:5.31em"/></span> range [<span class="CitationRef"><a epub:type="biblioref" href="#CR55" role="doc-biblioref">55</a></span>]), and the exerted pressing force [<span class="CitationRef"><a epub:type="biblioref" href="#CR42" role="doc-biblioref">42</a></span>]; also, varying degrees of spectral and temporal similarity between auditory and vibratory feedback may result either in cross-modal perceptual integration or interference [<span class="CitationRef"><a epub:type="biblioref" href="#CR60" role="doc-biblioref">60</a></span>]. However, we specifically chose a free playing task in order to measure the effect of vibrotactile feedback on various aspects of the playing experience.</p><p class="Para" id="Par75">With regard to the coherence of specific audio-tactile combinations, although Noise vibration resulted in very uniform ratings when associated with Sound 1 (<span class="InlineEquation" id="IEq69"><img alt="$$\beta $$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq69.png" style="width:0.94em"/></span> <span class="InlineEquation" id="IEq70"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq70.png" style="width:0.87em"/></span> 0.56, p &lt; 0.001), it produced the lowest rating consistency with Sound 2 (<span class="InlineEquation" id="IEq71"><img alt="$$\beta $$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq71.png" style="width:0.94em"/></span> <span class="InlineEquation" id="IEq72"><img alt="$$=$$" src="../images/478239_1_En_12_Chapter/478239_1_En_12_Chapter_TeX_IEq72.png" style="width:0.87em"/></span> 0.16, p &lt; 0.05). While this was obviously affected by the general tendency of ten participants toward inconsistent ratings, one may also consider the varying degree of similarity between Sound 2 and Noise vibration: at the upper range of the y coordinate Sound 2 was noise-like, while for lower y values it was increasingly sinusoidal; inconsistency might follow from having played once mostly at high y and once mostly at low y. Conversely, Sound 1 retained the same degree of (dis)similarity with Noise vibration, independent of the playing position/style. Overall, the noticed inconsistency of responses sets a future challenge for screening the participants and controlling the playing task.</p></section></section><section class="Section1 RenderAsSection1" id="Sec13"><h2 class="Heading"><span class="HeadingNumber">12.9 </span>Conclusions</h2><p class="Para" id="Par76">Based on the reported results, we suggest that the design of future multisensory interface technologies, especially if applicable to music performance, should take into consideration the addition of advanced vibrotactile feedback. This would enable the re-establishment of a consistent physical exchange between users and their digital devices—similar to the natural relationship that musicians establish with their instrument, where the source of sound and vibration coincides—with the demonstrated potential to enhance the experience and the perceived quality of the interface. Indeed, several participants in the reported musical studies were impressed with the novelty and “aliveness” of haptic interfaces, as opposed to their experience with existing digital musical devices.</p><p class="Para" id="Par77">Ultimately, it is yet to be seen if and how such subjective enhancements may be reflected in the quality of playing, and musical performance altogether. Making objective measurements of these aesthetic aspects however poses a major research challenge, and the present work only scratched the surface in this direction. Instead, this will be the main object of a follow-up experiment currently in the works.</p></section><div class="License LicenseSubType-cc-by"><a href="https://creativecommons.org/licenses/by/4.0"><img alt="Creative Commons" src="../css/cc-by.png"/></a><p class="SimplePara"><strong class="EmphasisTypeBold ">Open Access</strong> This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (<span class="ExternalRef"><a href="http://creativecommons.org/licenses/by/4.0/"><span class="RefSource">http://​creativecommons.​org/​licenses/​by/​4.​0/​</span></a></span>), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p><p class="SimplePara">The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p></div><aside aria-labelledby="Bib1Heading" class="Bibliography" id="Bib1"><div epub:type="bibliography" role="doc-bibliography"><div class="Heading" id="Bib1Heading">References</div><ol class="BibliographyWrapper"><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">1.</div><div class="CitationContent" id="CR1">Ahmaniemi, T.: Effect of Dynamic Vibrotactile Feedback on the Control of Isometric Finger Force. IEEE Trans. on Haptics (2012).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">2.</div><div class="CitationContent" id="CR2">Altinsoy, M. E. in Lect. Notes Comput. Sci. (eds Nordahl, R., Serafin, S., Fontana, F., Brewster, S.) 20–25 (Springer Berlin / Heidelberg, 2010).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">3.</div><div class="CitationContent" id="CR3">Altinsoy, M. E.: Quality of auditory-tactile virtual environments. J. Audio Eng. Soc. <strong class="EmphasisTypeBold ">60</strong>, 38–46 (2012).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">4.</div><div class="CitationContent" id="CR4">Banu, A., Praliyev, N., Evagoras, X.: Effect of Frequency Level on Vibrotactile Sound Detection in Proc. of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications <strong class="EmphasisTypeBold ">2</strong> (SciTePress, Prague, Czech Republic, July 2019), 97–102.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">5.</div><div class="CitationContent" id="CR5">Bensmaïa, S. J., Hollins, M.: Complex tactile waveform discrimination. J. Acoust. Soc. of Am. <strong class="EmphasisTypeBold ">108</strong>, 1236–1245 (2000).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1121/1.1288937"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">6.</div><div class="CitationContent" id="CR6">Brungart, D. S.: Auditory localization of nearby sources. III. Stimulus effects. J. Acoust. Soc. Am. <strong class="EmphasisTypeBold ">106</strong>, 3589–3602 (Dec. 1999).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">7.</div><div class="CitationContent" id="CR7">Brungart, D. S., Durlach, N. I., Rabinowitz, W. M.: Auditory localization of nearby sources. II. Localization of a broadband source. J. Acoust. Soc. Am. <strong class="EmphasisTypeBold ">106</strong>, 1956–1968 (Oct. 1999).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">8.</div><div class="CitationContent" id="CR8">Brungart, D. S., Rabinowitz,W. M.: Auditory localization of nearby sources. Head-related transfer functions. J. Acoust. Soc. Am. <strong class="EmphasisTypeBold ">106</strong>, 1465–1479 (Sept. 1999).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">9.</div><div class="CitationContent" id="CR9">Bürkner, P.-C.: brms: An R Package for Bayesian Multilevel Models Using Stan. J. Stat. Softw. <strong class="EmphasisTypeBold ">80</strong> (2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">10.</div><div class="CitationContent" id="CR10">Caclin, A., Soto-Faraco, S., Kingstone, A., Spence, C.: Tactile "capture" of audition. Perception &amp; Psychophysics <strong class="EmphasisTypeBold ">64</strong>, 616–630 (2002).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.3758/BF03194730"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">11.</div><div class="CitationContent" id="CR11">Caetano, G., Jousmäki, V.: Evidence of vibrotactile input to human auditory cortex. Neuroimage (2006).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">12.</div><div class="CitationContent" id="CR12">Dahl, S., Bresin, R.: Is the player more influenced by the auditory than the tactile feedback from the instrument? in Proc. Int. Conf. on Digital Audio Effects (DAFx) (Limerick, Ireland, Dec. 2001), 194–197.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">13.</div><div class="CitationContent" id="CR13">De Pra, Y., Fontana, F., Järveläinen, H., Papetti, S., Simonato, M.: Does it ping or pong? Auditory and tactile classification of materials by bouncing events. ACM Trans. Applied Perception (TAP) <strong class="EmphasisTypeBold ">17</strong>, 1–17 (2020).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1145/3393898"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">14.</div><div class="CitationContent" id="CR14">Ernst, M. O., Bülthoff, H. H.: Merging the senses into a robust percept. Trends Cogn. Sci. <strong class="EmphasisTypeBold ">8</strong>, 162-9 (2004).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1016/j.tics.2004.02.002"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">15.</div><div class="CitationContent" id="CR15">Falchier, A., Cappe, C., Barone, P., Schroeder, C. E. in The New Handbook of Multisensory Processing (ed Stein, B. E.) chap. 4 (MIT Press, 2012).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">16.</div><div class="CitationContent" id="CR16">Fiebelkorn, I. C., Foxe, J. F., Molholm, S. in The New Handbook of Multisensory Processing (ed Stein, B. E.) chap. 21 (MIT Press, 2012).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">17.</div><div class="CitationContent" id="CR17">Fontana, F. et al.: Rendering and subjective evaluation of real vs. synthetic vibrotactile cues on a digital piano keyboard in Proc. Int. Conf. on Sound and Music Computing (SMC) (Maynooth, Ireland, July 2015), 161–167.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">18.</div><div class="CitationContent" id="CR18">Fontana, F., Paisa, R., Ranon, R., Serafin, S.: Multisensory plucked instrument modeling in Unity3D: From Keytar to accurate string prototyping. App. Sci. <strong class="EmphasisTypeBold ">10</strong>, 1452 (2020).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.3390/app10041452"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">19.</div><div class="CitationContent" id="CR19">Fontana, F., Papetti, S., Järveläinen, H., Avanzini, F.: Detection of keyboard vibrations and effects on perceived piano quality. J. of the Acoust. Soc. Of Am. <strong class="EmphasisTypeBold ">142</strong>, 2953–2967 (2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">20.</div><div class="CitationContent" id="CR20">Fujisaki, W., Nishida, S.: Audio-tactile superiority over visuo-tactile and audio-visual combinations in the temporal resolution of synchrony perception. Exp. Brain Res. <strong class="EmphasisTypeBold ">198</strong>, 245–259 (2009).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1007/s00221-009-1870-x"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">21.</div><div class="CitationContent" id="CR21">Galembo, A., Askenfelt, A.: Quality assessment of musical instruments - Effects of multimodality in Proc. of the 5th triennial conference of the European Society for the Cognitive Sciences of Music (ESCOM) (Hanover, Germany, Sept. 2003), 441–444.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">22.</div><div class="CitationContent" id="CR22">Glennie, E.: Hearing Essay 1993.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">23.</div><div class="CitationContent" id="CR23">Goble, A. K., Hollins, M.: Vibrotactile adaptation enhances frequency discrimination. J. of the Acoust. Soc. of Am. <strong class="EmphasisTypeBold ">96</strong>, 771–780 (1994).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">24.</div><div class="CitationContent" id="CR24">Hopkins, C., Maté-Cid, S., Seiffert, G., Fulford, R., Ginsborg, J.: Inherent and learnt abilities for relative pitch in the vibrotactile domain using the fingertip in 20th Int. Congr. Sound Vib. (ICSV 2013) (Bankok, 2013), 3207–3214.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">25.</div><div class="CitationContent" id="CR25">Huang, J., Gamble, D., Sarnlertsophon, K., Wang, X., Hsiao, S.: Feeling music: integration of auditory and tactile inputs in musical meter perception. PLoS One <strong class="EmphasisTypeBold ">7</strong> (Jan. 2012).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">26.</div><div class="CitationContent" id="CR26">Jack, R. H., Mehrabi, A., Stockman, T., Mcpherson, A.: Action-sound latency and the perceived quality of digital musical instruments: Comparing professional percussionists and amateur musicians. Music Percept. An Interdiscip. J. <strong class="EmphasisTypeBold ">36</strong>, 109–128 (2018).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1525/mp.2018.36.1.109"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">27.</div><div class="CitationContent" id="CR27">Järveläinen, H., Papetti, S., Schiesser, S., Grosshauser, T.: Audio-Tactile Feedback in Musical Gesture Primitives: Finger Pressing in Int. Conf. on Sound and Music Computing (SMC) (2013), 109–114.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">28.</div><div class="CitationContent" id="CR28">Jiang, L., Cutkosky, M. R., Ruutiainen, J., Raisamo, R.: Improving finger force control with vibrational haptic feedback for Multiple Sclerosis in Proc. of the IASTED Int. Conf. on Telehealth/Assistive Technologies (ACTA Press, Baltimore, Maryland, 2008), 110–115.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">29.</div><div class="CitationContent" id="CR29">Kaaresoja, T., Brewster, S., Lantz, V.: Towards the temporally perfect virtual button: touch-feedback simultaneity and perceived quality in mobile touchscreen press interactions. ACM Trans. on Applied Perception <strong class="EmphasisTypeBold ">11</strong>, 1–25 (2014).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1145/2611387"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">30.</div><div class="CitationContent" id="CR30">Kayser, C., Petkov, C. I., Augath, M., Logothetis, N. K.: Integration of Touch and Sound in Auditory Cortex. Neuron <strong class="EmphasisTypeBold ">48</strong>, 373–384 (2005).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1016/j.neuron.2005.09.018"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">31.</div><div class="CitationContent" id="CR31">Kitagawa, N., Spence, C.: Audiotactile multisensory interactions in human information processing. Jpn. Psychol. Res. <strong class="EmphasisTypeBold ">48</strong>, 158–173 (2006).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1111/j.1468-5884.2006.00317.x"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">32.</div><div class="CitationContent" id="CR32">Kruschke, J. K.: Doing Bayesian data analysis - A tutorial with R, JAGS, and Stan 2nd (Academic Press, 2014).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">33.</div><div class="CitationContent" id="CR33">Kuchenbuch, A., Paraskevopoulos, E., Herholz, S. C., Pantev, C.: Audiotactile integration and the influence of musical training. PLoS One <strong class="EmphasisTypeBold ">9</strong>, e85743 (2014).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1371/journal.pone.0085743"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">34.</div><div class="CitationContent" id="CR34">Landry, S. P., Champoux, F.: Musicians react faster and are better multisensory integrators. Brain Cogn. <strong class="EmphasisTypeBold ">111</strong>, 156–162 (2017).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1016/j.bandc.2016.12.001"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">35.</div><div class="CitationContent" id="CR35">Levänen, S., Hamdorf, D.: Feeling vibrations: enhanced tactile sensitivity in congenitally deaf humans. Neuroscience letters <strong class="EmphasisTypeBold ">301</strong>, 75–77 (2001).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1016/S0304-3940(01)01597-X"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">36.</div><div class="CitationContent" id="CR36">Merchel, S., Schwendicke, A., Altinsoy, M. E.: Feeling the sound: audiotactile intensity perception in Proc. 2nd Polish-German Struct. Conf. Acoust. 58th Open Semin. Acoust. (2011).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">37.</div><div class="CitationContent" id="CR37">Merchel, S., Leppin, A., Altinsoy, M. E.: Hearing with your Body: The Influence of Whole-Body Vibrations on Loudness Perception in Proc. 16th Int. Congr. on Sound and Vibration (ICSV16) (Kraków, Poland, July 2009), 5–9.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">38.</div><div class="CitationContent" id="CR38">O’Modhrain, S., Gillespie, R. B. in Musical Haptics (eds Papetti, S., Saitis, C.) 11–27 (Springer International Publishing, Cham, 2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">39.</div><div class="CitationContent" id="CR39">Okazaki, R., Kajimoto, H., Hayward, V.: Vibrotactile Stimulation Can Affect Auditory Loudness: A Pilot Study in Haptics: Perception, Devices, Mobility, and Communication (eds Isokoski, P., Springare, J.) (Springer Berlin Heidelberg, Berlin, Heidelberg, 2012), 103–108.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">40.</div><div class="CitationContent" id="CR40">Papetti, S., Fröhlich, M., Schiesser, S.: The TouchBox: an open-source audiohaptic device for finger-based interaction in IEEE World Haptics Conf. (WHC) (Tokyo, Japan, July 2019), 491–496.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">41.</div><div class="CitationContent" id="CR41">Papetti, S., Järveläinen, H., Schiesser, S.: Interactive vibrotactile feedback enhances the perceived quality of a surface for musical expression and the playing experience. IEEE Trans. on Haptics, 1–1 (2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">42.</div><div class="CitationContent" id="CR42">Papetti, S., Jarvelainen, H., Giordano, B. L., Schiesser, S., Frohlich, M.: Vibrotactile Sensitivity in Active Touch: Effect of Pressing Force. IEEE Trans. on Haptics <strong class="EmphasisTypeBold ">10</strong>, 113–122 (2017).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1109/TOH.2016.2582485"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">43.</div><div class="CitationContent" id="CR43">Papetti, S., Schiesser, S., Fröhlich, M.: Multi-point vibrotactile feedback for an expressive musical interface in Proc. New Interfaces for Musical Expression (NIME) (2015).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">44.</div><div class="CitationContent" id="CR44">Passalenti, A. et al.: No Strings Attached: Force and Vibrotactile Feedback in a Virtual Guitar Simulation in Proc. 2019 IEEE Conf. on Virtual Reality and 3D User Interfaces (VR) (2019), 1116–1117.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">45.</div><div class="CitationContent" id="CR45">Passalenti, A., Fontana, F.: Haptic interaction with guitar and bass virtual strings in Proc. Int. Conf. on Sound and Music Computing (SMC) (Limassol, Cyprus, July 2018), 427–432.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">46.</div><div class="CitationContent" id="CR46">Passalenti, A. et al.: No Strings Attached: Force and Vibrotactile Feedback in a Guitar Simulation in Proc. Int. Conf. Sound and Music Computing (SMC 2019) (Málaga, Spain, 2019), 28–31.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">47.</div><div class="CitationContent" id="CR47">Perrault Jr, T. J., Rowland, B. A. in The New Handbook of Multisensory Processing (ed Stein, B. E.) chap. 6 (MIT Press, 2012).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">48.</div><div class="CitationContent" id="CR48">Romagnoli, M., Fontana, F., Sarkar, R.: Vibrotactile Recognition byWestern and Indian Population Groups of Traditional Musical Scales Played with the Harmonium in Haptic and Audio Interaction Design (eds Cooper, E. W., Kryssanov, V. V., Ogawa, H., Brewster, S.) (Springer Berlin Heidelberg, Berlin, Heidelberg, 2011), 91–100.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">49.</div><div class="CitationContent" id="CR49">Russo, F. A., Ammirante, P., Fels, D. I.: Vibrotactile discrimination of musical timbre. J. Exp. Psychol. Hum. Percept. Perform. <strong class="EmphasisTypeBold ">38</strong> (Aug. 2012).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">50.</div><div class="CitationContent" id="CR50">Sanabria, D., Soto-Faraco, S., Spence, C.: Spatiotemporal interactions between audition and touch depend on hand posture. Exp. Brain Res. <strong class="EmphasisTypeBold ">165</strong>, 505–14 (2005).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1007/s00221-005-2327-5"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">51.</div><div class="CitationContent" id="CR51">Schürmann, M., Caetano, G., Hlushchuk, Y., Jousmäki, V., Hari, R.: Touch activates human auditory cortex. Neuroimage <strong class="EmphasisTypeBold ">30</strong>, 1325–31 (2006).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1016/j.neuroimage.2005.11.020"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">52.</div><div class="CitationContent" id="CR52">Schürmann, M., Caetano, G., Jousmäki, V., Hari, R.: Hands help hearing: facilitatory audiotactile interaction at low sound-intensity levels. J. of the Acoust. Soc. of Am. <strong class="EmphasisTypeBold ">115</strong>, 830–832 (2004).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">53.</div><div class="CitationContent" id="CR53">Shore, S. E., Dehmel, S. in The New Handbook of Multisensory Processing (ed Stein, B. E.) chap. 1 (MIT Press, 2012).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">54.</div><div class="CitationContent" id="CR54">Stanford, T. A. in The New Handbook of Multisensory Processing (ed Stein, B. E.) (MIT Press, 2012).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">55.</div><div class="CitationContent" id="CR55">Verrillo, R. T.: Vibration sensation in humans. Music Perception <strong class="EmphasisTypeBold ">9</strong>, 281–302 (1992).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.2307/40285553"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">56.</div><div class="CitationContent" id="CR56">Wilson, E. C., Reed, C. M., Braida, L. D.: Integration of auditory and vibrotactile stimuli: Effects of phase and stimulus-onset asynchrony. J. Acoust. Soc. Am. <strong class="EmphasisTypeBold ">126</strong>, 1960–74 (2009).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1121/1.3204305"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">57.</div><div class="CitationContent" id="CR57">Wilson, E. C., Reed, C. M., Braida, L. D.: Integration of auditory and vibrotactile stimuli: Effects of frequency. J. Acoust. Soc. Am. <strong class="EmphasisTypeBold ">127</strong>, 3044–59 (2010).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1121/1.3365318"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">58.</div><div class="CitationContent" id="CR58">Wollman, I., Fritz, C., Poitevineau, J., McAdams, S.: Investigating the role of auditory and tactile modalities in violin quality evaluation. PLoS One <strong class="EmphasisTypeBold ">9</strong>, e112552 (2014).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1371/journal.pone.0112552"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">59.</div><div class="CitationContent" id="CR59">Yau, J. M., Olenczak, J. B., Dammann, J. F., Bensmaïa, S. J.: Temporal frequency channels are linked across audition and touch. Curr. Biol. <strong class="EmphasisTypeBold ">19</strong>, 561–6 (2009).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1016/j.cub.2009.02.013"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">60.</div><div class="CitationContent" id="CR60">Yau, J. M., Weber, A. I., Bensmaïa, S. J.: Separate mechanisms for audiotactile pitch and loudness interactions. Frontiers in psychology <strong class="EmphasisTypeBold ">1</strong>, 160 (2010).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.3389/fpsyg.2010.00160"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">61.</div><div class="CitationContent" id="CR61">Young, G. W., Murphy, D., Weeter, J.: Auditory Discrimination of Pure and Complex Waveforms Combined With Vibrotactile Feedback in Proc. New Interfaces for Musical Expression (NIME) (Baton Rouge, LA, 2015).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">62.</div><div class="CitationContent" id="CR62">Young, G. W., Murphy, D., Weeter, J.: Haptics in music: The effects of vibrotactile stimulus in low frequency auditory difference detection tasks. IEEE Trans. on Haptics <strong class="EmphasisTypeBold ">1412</strong>, 135–139 (2017).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1109/TOH.2016.2646370"><span><span>Crossref</span></span></a></span></span></div></li></ol></div></aside><aside aria-label="Footnotes" class="FootnoteSection" epub:type="footnotes"><div class="Heading">Footnotes</div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn1_source">1</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn1" role="doc-footnote"><p class="Para" id="Par29">The relatively low participation number of musicians as well as non-musicians reflects the exploratory character of tactile experiments with pianists as far as one decade ago. Later, they have consolidated into more robust methodologies, including the participation of more musicians when necessary—see, e.g., Sect. <span class="InternalRef"><a href="#Sec8">12.6</a></span>.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn2_source">2</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn2" role="doc-footnote"><p class="Para" id="Par31">performed by <em class="EmphasisTypeItalic ">aligned rank transform</em>, the nonparametric equivalent to factorial within-subjects analysis of variance</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn3_source">3</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn3" role="doc-footnote"><p class="Para" id="Par61"><span class="ExternalRef"><a href="https://tinyurl.com/HS-sounds"><span class="RefSource">https://​tinyurl.​com/​HS-sounds</span></a></span>.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn4_source">4</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn4" role="doc-footnote"><p class="Para" id="Par71">Note that unlike the other studies reported in this chapter, these data were analyzed using Bayesian inference; therefore, we use the term “credible” instead of “significant” of effects.</p></div><div class="ClearBoth"> </div></div></aside></div></div></body></html>