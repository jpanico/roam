<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops"><head><title>Audio Quality Assessment for Virtual Reality</title><meta content="text/html; charset=utf-8" http-equiv="content-type"/><link href="../css/springer_epub.css" rel="styleSheet" type="text/css"/></head><body><div epub:type="chapter" role="doc-chapter"><div class="ChapterContextInformation"><div class="ContextInformation" id="Chap5"><div class="ChapterCopyright">© The Author(s) 2023</div><span class="ContextInformationAuthorEditorNames">M. Geronazzo, S. Serafin<span class="CollaboratorDesignation"> (eds.)</span></span><span class="ContextInformationBookTitles"><span class="BookTitle">Sonic Interactions in Virtual Environments</span></span><span class="ContextInformationSeries"><span class="SeriesTitle" lang="en">Human–Computer Interaction Series</span></span><span class="ChapterDOI"><a href="https://doi.org/10.1007/978-3-031-04021-4_5">https://doi.org/10.1007/978-3-031-04021-4_5</a></span></div></div><!--Begin Abstract--><div class="MainTitleSection"><h1 class="ChapterTitle" lang="en">5. Audio Quality Assessment for Virtual Reality</h1></div><div class="AuthorGroup"><div class="AuthorNames"><span class="Author"><span class="AuthorName">Fabian Brinkmann</span><sup><a href="#Aff34">1</a> <a aria-label="Contact information for this author" href="#ContactOfAuthor1"><span class="ContactIcon"> </span></a></sup> and </span><span class="Author"><span class="AuthorName">Stefan Weinzierl</span><sup><a href="#Aff34">1</a> <a aria-label="Contact information for this author" href="#ContactOfAuthor2"><span class="ContactIcon"> </span></a></sup></span></div><div class="Affiliations"><div class="Affiliation" id="Aff34"><span class="AffiliationNumber">(1)</span><div class="AffiliationText">Audio Communication Group, Technical University of Berlin, Einsteinufer 17, 10587 Berlin, Germany</div></div><div class="ClearBoth"> </div></div><div class="Contacts"><div class="Contact" id="ContactOfAuthor1"><div class="ContactIcon"> </div><div class="ContactAuthorLine"><span class="AuthorName">Fabian Brinkmann</span> (Corresponding author)</div><div class="ContactAdditionalLine"><span class="ContactType">Email: </span><a href="mailto:fabian.brinkmann@tu-berlin.de">fabian.brinkmann@tu-berlin.de</a></div></div><div class="Contact" id="ContactOfAuthor2"><div class="ContactIcon"> </div><div class="ContactAuthorLine"><span class="AuthorName">Stefan Weinzierl</span></div><div class="ContactAdditionalLine"><span class="ContactType">Email: </span><a href="mailto:stefan.weinzierl@tu-berlin.de">stefan.weinzierl@tu-berlin.de</a></div></div></div></div><section class="Abstract" id="Abs1" lang="en" role="doc-abstract"><h2 class="Heading">Abstract</h2><p class="Para" id="Par1">A variety of methods for audio quality evaluation are available ranging from classic psychoacoustic methods like alternative forced-choice tests to more recent approaches such as quality taxonomies and plausibility. This chapter introduces methods that are deemed to be relevant for audio evaluation in virtual and augmented reality. It details in how far these methods can directly be used for testing in virtual reality or have to be adapted with respect to specific aspects. In addition, it highlights new areas, for example, quality of experience and presence that arise from audiovisual interactions and the mediation of virtual reality. After briefly introducing 3D audio reproduction approaches for virtual reality, the quality that these approaches can achieve is discussed along with the aspects that influence the quality. The concluding section elaborates on current challenges and hot topics in the field of audio quality evaluation and audio reproduction for virtual reality. To bridge the gap between theory and practice useful resources, software and hardware for 3D audio production and research are pointed out.</p></section><!--End Abstract--><div class="Fulltext"><section class="Section1 RenderAsSection1" id="Sec1"><h2 class="Heading"><span class="HeadingNumber">5.1 </span>Introduction</h2><p class="Para" id="Par2">Over the past years, an increasing number of virtual and augmented reality (VR/AR) applications emerged due to the advent of mobile devices such as smartphones and head-mounted displays. Audio plays an important role within these applications that is by far not restricted to conveying semantic information, for example, through dialogues or warning sounds. Beyond that, audio holds information about the spaciousness of a scene including the location of sound sources and the reverberance or size of a virtual environment. In this way, audio can be regarded as a channel to provide semantic information and spatial information and improve the sense of presence and immersion at the same time. Due to the key role of audio in VR/AR, this chapter gives an overview of methods for audio quality assessment in Sect. <span class="InternalRef"><a href="#Sec2">5.2</a></span>, followed by a brief introduction of audio reproduction techniques for VR/AR in Sect. <span class="InternalRef"><a href="#Sec12">5.3</a></span>. Readers who are familiar with audio reproduction techniques might skip Sect. <span class="InternalRef"><a href="#Sec12">5.3</a></span> and directly continue with Sect. <span class="InternalRef"><a href="#Sec23">5.4</a></span> that gives an overview of the quality of existing audio reproduction systems.</p></section><section class="Section1 RenderAsSection1" id="Sec2"><h2 class="Heading"><span class="HeadingNumber">5.2 </span>Perceptual Qualities and Their Measurement</h2><p class="Para" id="Par3">Methods and systems for generating virtual and augmented environments can be understood as a special case of (interactive) audio reproduction systems. Thus, in principle, all procedures for the perceptual evaluation of audio systems can also be used for the evaluation of VR systems [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>]. These include the procedures for the evaluation of “Basic Audio Quality”<span id="ITerm1"/>, which are standardized in various ITU recommendations and focus on the technical system properties and signal processing, as well as approaches with a wider focus on the listening situation and the presented audio content, taking into account the “Overall Listening Experience”. In addition, a number of measures have recently been proposed to more specifically determine the extent to which technologies for virtual and augmented environments live up to their claim of providing a convincing equivalent to physical acoustic reality. Finally, in addition to these holistic measures for evaluating VR and AR, there are a number of (VR-specific and VR-nonspecific) quality inventories that can be used to perform a differential diagnosis of VR systems, highlighting the individual strengths and weaknesses of the system and drawing conclusions for the targeted improvement.</p><section class="Section2 RenderAsSection2" id="Sec3"><h3 class="Heading"><span class="HeadingNumber">5.2.1 </span>Generic Measures</h3><section class="Section3 RenderAsSection3" id="Sec4"><h4 class="Heading"><span class="HeadingNumber">5.2.1.1 </span>Basic Audio Quality</h4><p class="Para" id="Par4">Since the mid-1990s, the Radiocommunication Sector of the International Telecommunication Union (ITU-R) has developed a series of recommendations for the “Subjective assessment of sound quality”<span id="ITerm2"/>. The series includes an overview of the areas of application of the recommendations with instructions for the selection of the appropriate standard [<span class="CitationRef"><a epub:type="biblioref" href="#CR35" role="doc-biblioref">35</a></span>] as well as an overview of “general methods” which are applied slightly differently in the different standards [<span class="CitationRef"><a epub:type="biblioref" href="#CR36" role="doc-biblioref">36</a></span>]. They contain instructions for experimental design, selection of the listening panel, test paradigms and scales, reproduction devices, and listening conditions up to the statistical treatment of collected data. Originally, these recommendations were mainly used for the perceptual evaluation of audio codecs, but later, they were also used for the evaluation of multi-channel reproduction systems and 3D audio techniques. The central construct to be evaluated by all ITU procedures is “Basic Audio Quality” (BAQ). It can be evaluated either by direct scaling or by rating the “impairment” relative to an explicit or implicit reference and caused by deficits of the transmission system such as a low-bitrate audio codec or by limitations of the spatial reproduction. By definition BAQ includes “all aspects of the sound quality being assessed”, such as “timbre, transparency, stereophonic imaging, spatial presentation, reverberance, echoes, harmonic distortions, quantisation noise, pops, clicks and background noise” [<span class="CitationRef"><a epub:type="biblioref" href="#CR36" role="doc-biblioref">36</a></span>, p. 7], In studies of impairment, listeners are asked “to judge any and all detected differences between the reference and the object” [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>, p. 7]. In this case, the evaluation of BAQ thus corresponds to a rating of general “similarity” or “difference”.</p><div class="Para" id="Par5">The most popular standards for BAQ are (cf. Fig. <span class="InternalRef"><a href="#Fig1">5.1</a></span>)<figure class="Figure" id="Fig1"><div class="MediaObject" id="MO1"><img alt="" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Fig1_HTML.png" style="width:33.95em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 5.1</span><p class="SimplePara">User interfaces for ABC/HR and MUSHRA tests. Active conditions are indicated by orange buttons; loop range and current playback position by orange boxes and lines. The ABC/HR interface shows only one condition but versions with multiple conditions per rating screen are also possible. If multiple conditions are displayed on a single screen, an additional button to sort the conditions according to the current ratings might help subjects to establish more reliable ratings (CC-BY, Fabian Brinkmann)</p></div></figcaption></figure>
<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par6">ITU-R BS. 1116-3:2016 (Methods for the subjective assessment of small impairments in audio systems) [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>]. Listeners are asked to rate the difference between an audio stimulus and a given reference stimulus using a continuous scale with five labels (“Imperceptible”/“Perceptible, but not annoying”/“Slightly annoying”/ “Annoying”/“Very annoying”) used as “anchors”. Participants are presented with three stimuli (A, B, C). A is the reference, and B and C are rated, with one of the two stimuli again being the hidden reference (double-blind triple-stimulus with hidden reference).</p></li><li><p class="Para" id="Par7">ITU-R BS.1534 (Method for the subjective assessment of intermediate quality level of audio systems) [<span class="CitationRef"><a epub:type="biblioref" href="#CR37" role="doc-biblioref">37</a></span>]. Unlike ITU-R BS. 1116-3, it is a multi-stimulus test where direct comparisons between the different stimuli are possible. Quality is rated on a continuous scale with five labels (“Excellent”/“Good”/“Fair”/“Poor”/“Bad”). Participants are presented with a reference, no more than nine stimuli under test, and two anchor signals (MUlti-Stimulus test with Hidden Reference and Anchor, MUSHRA). The standard anchors are a low-pass filtered version of the original signal with a cut-off frequency of 3.5 kHz (low-quality anchor) and 7 kHz (mid-quality anchor). Alternatively or additionally, further non-standard anchors can be used; they should resemble the character of the systems’ artifacts being tested and indicate how the systems under test compares to well-known audio quality levels. Possible anchors in the context of spatial audio might be conventional mono/stereo recordings or non-individual signals. Since listeners can directly compare the signals under test with the reference and among each other, more reliable ratings can be expected in situations where stimuli differ significantly from the reference, but only slightly from each other.</p></li></ul></div>
</div><p class="Para" id="Par8">Although BAQ is the standard attribute to be tested in both ITU recommendations, other attributes are suggested to test more specific aspects of audio systems such as spatial and timbral qualities. ITU-R BS.1284-2 contains a list of main attributes and sub-attributes, from which one can choose those suitable for a particular test [<span class="CitationRef"><a epub:type="biblioref" href="#CR36" role="doc-biblioref">36</a></span>, Attachment 1]. In this respect, both recommendations are often used only as an experimental paradigm, but applied to qualities other than BAQ, e.g., those developed in various taxonomies on the properties of VR systems (see Sect. <span class="InternalRef"><a href="#Sec10">5.2.2.4</a></span>).</p><p class="Para" id="Par9">A number of issues were raised addressing specific aspects of the ITU recommendations [<span class="CitationRef"><a epub:type="biblioref" href="#CR55" role="doc-biblioref">55</a></span>]. One pertains to the scale labels being multidimensional, which could distort the ratings. This can be avoided by using clearly unidimensional labels at both ends, e.g., “imperceptible”/“very perceptible” for ABC/HR or “good”/“bad” for MUSHRA and additional unlabeled lines for orientation. Another issue points out that data from MUSHRA tests often violate assumptions for conducting an Analysis of Variance (ANOVA), the most common means for statistical analysis of the results. This can be considered by using general linear models for the analysis, that are more flexible than ANOVA and pose less requirements on the input data [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>].</p></section><section class="Section3 RenderAsSection3" id="Sec5"><h4 class="Heading"><span class="HeadingNumber">5.2.1.2 </span>Overall Listening Experience</h4><p class="Para" id="Par10">The construct of “Overall Listening Experience” (OLE) [<span class="CitationRef"><a epub:type="biblioref" href="#CR70" role="doc-biblioref">70</a></span>] was derived from the concept of “Quality of Experience”<span id="ITerm3"/>, which in the context of quality management describes “the degree of delight or annoyance of the user of an application or service” [<span class="CitationRef"><a epub:type="biblioref" href="#CR11" role="doc-biblioref">11</a></span>], considering not only the technical performance of a system but also the expectations and personality and current state of the user as influencing <span id="ITerm4">factors</span>. In contrast to listening tests according to the ITU recommendations, the musical content is thus explicitly part of the judgment that listeners make about the OLE.</p><div class="Para" id="Par11">A measurement of the OLE can be a useful alternative or supplement to purely system-related evaluations insofar as, for example, the difference between different playback systems for music may very well be audible in a direct comparison, but hardly relevant for everyday music consumption, also in comparison to the liking of the music played. In this respect, an evaluation according to ITU may possibly convey a false picture of the general relevance of technical functions. This becomes evident, for example, in a direct comparison between BAQ and OLE ratings of spatial audio systems, where the differences between BAQ ratings are generally larger than between OLE ratings. In a listening test, both BAQ ratings according to ITU-R BS. 1534 with explicit reference and OLE ratings (“Please rate for each audio excerpt how much you enjoyed listening to it”) without explicit reference were collected for three different spatial audio systems (2.0 stereo, 5.0 surround, 22.2 surround [<span class="CitationRef"><a epub:type="biblioref" href="#CR71" role="doc-biblioref">71</a></span>]). While the difference between 2.0 and 5.0 was equally visible in BAQ and OLE, the difference between 5.0 and 22.2 was clearly audible in a direct comparison (BAQ), but did obviously not result in a significant increase in listening pleasure (OLE, Fig. <span class="InternalRef"><a href="#Fig2">5.2</a></span>).<figure class="Figure" id="Fig2"><div class="MediaObject" id="MO2"><img alt="" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Fig2_HTML.png" style="width:21.7em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 5.2</span><p class="SimplePara">Results of a listening test (z-standardized scores) of Basic Audio Quality (BAQ) and Overall Listening Experience (OLE) for three different spatial audio systems (2.0 stereo, 5.0 surround, 22.2 sound referred to as “3D Audio”). BAQ ratings were given according to ITU-R BS.1534 relative to the “3D audio” condition as an explicit reference, whereas OLE ratings were given without a reference stimulus [<span class="CitationRef"><a epub:type="biblioref" href="#CR71" role="doc-biblioref">71</a></span>, p. 84]</p></div></figcaption></figure>
</div></section></section><section class="Section2 RenderAsSection2" id="Sec6"><h3 class="Heading"><span class="HeadingNumber">5.2.2 </span>VR/AR-Specifc Measures</h3><section class="Section3 RenderAsSection3" id="Sec7"><h4 class="Heading"><span class="HeadingNumber">5.2.2.1 </span>Authenticity</h4><p class="Para" id="Par12">A simulation that is indistinguishable from the physical sound field it is intended to simulate could be termed authentic. The term could be used in a physical sense; then it would aim at the identity of sound fields, be it the identity of sound pressures in the ear canal (binaural technology) or the identity of sound fields in an extended spatial area (sound field synthesis). Since no technical system is currently able to guarantee such an identity, and since such a physical identity may also not be required for the users of VR/AR systems, the term <span id="ITerm5">authenticity</span> is mostly used in the psychological sense. In this sense, it denotes a simulation that is perceptually indistinguishable from the corresponding real sound field [<span class="CitationRef"><a epub:type="biblioref" href="#CR8" role="doc-biblioref">8</a></span>].</p><div class="Para" id="Par13">The challenge in determining perceptual authenticity is not to let the presence of a simulation or the physical reference in the listening test become recognizable solely through the environment of the presentation, i.e., by wearing headphones as opposed to listening freely in the physical sound field, or by listening in a studio environment that does not correspond to the simulated space even purely visually. For this reason, a determination of the authenticity of loudspeaker-based <span id="ITerm6">systems</span> such as Wave Field Synthesis (WFS) or Higher-Order Ambisonics (HOA) can hardly be carried out in practice, because even if one were to suppress the visual impression by means of a blindfold, the listener would have to be brought from the playback room of the synthesis into the real reference room, which would no longer allow a direct comparison due to the temporal delay. Setting up a sound field synthesis in the corresponding physical room, on the other hand, would be prohibited, since the <span id="ITerm7">room acoustics</span> of the physical room would influence the sound field of the loudspeaker synthesis.<figure class="Figure" id="Fig3"><div class="MediaObject" id="MO3"><img alt="" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Fig3_HTML.png" style="width:22.15em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 5.3</span><p class="SimplePara">Listening test setup for testing authenticity and plausibility. For seamless switching between audio from the loudspeakers and their binaural simulation, the subject is wearing extra-aural headphones that minimize distortions of exterior sound fields. The head position of the subject is tracked by an electromagnetic sensor pair mounted on the top of the chair and headphones. See also Sect. <span class="InternalRef"><a href="#Sec25">5.4.1.1</a></span> (CC-BY, Fabian Brinkmann)</p></div></figcaption></figure>
</div><p class="Para" id="Par14">A determination of authenticity is simpler for binaural technology systems. By using open headphones that are largely transparent to the external sound field and whose influence can possibly be compensated by an equalization filter, a direct comparison can be made by switching back and forth between a physical sound source and <span id="ITerm8">its</span> binaural simulation [<span class="CitationRef"><a epub:type="biblioref" href="#CR8" role="doc-biblioref">8</a></span>]. The influence of the headphones on the external sound field can be further minimized by using extra-aural headphones suspended a few centimeters in front of the ear [<span class="CitationRef"><a epub:type="biblioref" href="#CR18" role="doc-biblioref">18</a></span>]. Such an influence can also come from other VR devices such as head-mounted displays that are close to the ear canal [<span class="CitationRef"><a epub:type="biblioref" href="#CR27" role="doc-biblioref">27</a></span>]. An example of a listening test setup is shown in Fig. <span class="InternalRef"><a href="#Fig3">5.3</a></span>.</p><p class="Para" id="Par15">As a paradigm for the listening test, classical procedures such as ABX with explicit reference [<span class="CitationRef"><a epub:type="biblioref" href="#CR12" role="doc-biblioref">12</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR44" role="doc-biblioref">44</a></span>] or forced-choice procedures (N-AFC) with non-explicit reference  [<span class="CitationRef"><a epub:type="biblioref" href="#CR21" role="doc-biblioref">21</a></span>] can be used, which have proven suitable for detecting small differences between two stimuli. It should be noted that, especially in the case of minor differences, the presentation mode can have a great influence on the recognition rate, such as the fact whether the two stimuli (simulation and reference) can be heard by the test participants only once or as often as desired [<span class="CitationRef"><a epub:type="biblioref" href="#CR8" role="doc-biblioref">8</a></span>, p. 1793 f]. An example of a user interface is given in Fig. <span class="InternalRef"><a href="#Fig4">5.4</a></span>.</p><div class="Para" id="Par16">Binaural representations can also be used to make comparisons of physical sound fields and simulations based on loudspeaker arrays [<span class="CitationRef"><a epub:type="biblioref" href="#CR85" role="doc-biblioref">85</a></span>]. For this purpose, the measured or numerically simulated sound field of a loudspeaker array at a given listening position can be presented in the listening test as a <span id="ITerm9">binaural synthesis</span>, thus avoiding the problems described above when comparing physical and loudspeaker-simulated sound fields. It should be noted, however, in this case, the simulation (binaural synthesis) of a simulation (sound field synthesis) becomes audible, so it may be difficult to separate the artifacts of the two methods.<figure class="Figure" id="Fig4"><div class="MediaObject" id="MO4"><img alt="" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Fig4_HTML.png" style="width:14.88em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 5.4</span><p class="SimplePara">User interfaces for testing authenticity with an ABX test (also termed 2-interval/2-alternative forced choice, 2i/2AFC) test and testing plausibility with a yes/no paradigm. Responses/active conditions are indicated by orange buttons; loop range and current playback position by orange boxes and lines. In case of the test for plausibility, the audio starts automatically and can only be heard once (CC-BY, Fabian Brinkmann)</p></div></figcaption></figure>
</div></section><section class="Section3 RenderAsSection3" id="Sec8"><h4 class="Heading"><span class="HeadingNumber">5.2.2.2 </span>Plausibility</h4><p class="Para" id="Par17">While the authenticity of virtual environments can be determined by the (physical or perceptual) identity of physical and simulated sound fields, <span id="ITerm10">plausibility</span> has been proposed as a measure of the extent to which a simulation is “in agreement with the listener’s expectation towards a corresponding real event” [<span class="CitationRef"><a epub:type="biblioref" href="#CR47" role="doc-biblioref">47</a></span>]. Plausibility thus does not address the comparison with an external, presented reference, but the consideration against the background of an inner reference that reflects the credibility of the simulation, based on the listener’s experience and expectations of the internal structure of acoustic scenes or environments. The operationalization of this construct thus does not require a comparative evaluation, but a yes–no decision.</p><p class="Para" id="Par18">By analyzing such yes–no decisions with the statistical framework of signal detection theory (SDT, [<span class="CitationRef"><a epub:type="biblioref" href="#CR84" role="doc-biblioref">84</a></span>]), one can separate the response bias, i.e., a general, subjective tendency to consider stimuli as “real” or “simulated”, from the actual impairments of the simulation. Signal detection theory is originally a method for determining threshold values. For example, the absolute hearing threshold of sounds can be determined by the statistical analysis of a 2x2 contingency table in which two correct answers (sound present and heard, sound absent and not heard, i.e., hits and correct rejections) and two incorrect answers (sound present and not heard, sound absent and heard, i.e., misses and false alarms) occur. By contrasting these response frequencies, the response bias, i.e., a general tendency to mark sounds as “heard,” can be separated from actual recognition performance. The latter is represented by the sensitivity <span class="InlineEquation" id="IEq1"><img alt="$$d{'}$$" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Chapter_TeX_IEq1.png" style="width:0.94em"/></span> which can be converted to a corresponding 2AFC detection rate. A number of at least 100 yes–no decisions per subject is considered necessary for obtaining stable individual SDT parameters [<span class="CitationRef"><a epub:type="biblioref" href="#CR40" role="doc-biblioref">40</a></span>].</p><p class="Para" id="Par19">This approach can be applied to the evaluation of virtual realities, in that the artifacts caused by deficits in the simulation take on the role of a stimulus to be discovered, and listeners are asked to identify the environment as “simulated” if they notice them. The prerequisite for such an experiment is, however, that—similar to an experiment on “authenticity”—one can present both physically “real” and simulated sound fields without the nature of the stimulus already being recognizable on the basis of the experimental environment, for example, by providing a visual representation of the physical sound source also in the simulated case, or by conducting the experiment with closed or blindfolded eyes.</p></section><section class="Section3 RenderAsSection3" id="Sec9"><h4 class="Heading"><span class="HeadingNumber">5.2.2.3 </span>Sense of Presence and Immersion</h4><p class="Para" id="Par20">A central function of VR systems is to create a “sense of <span id="ITerm11">presence</span>”, i.e., the feeling of being or acting in a place, even when one is physically situated in another location and the sensory input is known to be technically <span id="ITerm12">mediated</span>. The concept of presence, also called “telepresence” in older literature in reference to teleoperation systems used to manipulate remote physical objects [<span class="CitationRef"><a epub:type="biblioref" href="#CR58" role="doc-biblioref">58</a></span>], has given rise to its own research direction and community in the form of presence research, which is organized in societies such as the International Society for Presence Research (ISPR) and conferences such as the biennial PRESENCE conference.<sup><a epub:type="noteref" href="#Fn1" id="Fn1_source" role="doc-noteref">1</a></sup>
</p><p class="Para" id="Par22">To measure the degree of presence, different questionnaires have been developed. For an overview see [<span class="CitationRef"><a epub:type="biblioref" href="#CR72" role="doc-biblioref">72</a></span>]. The instrument of Whitmer and Singer [<span class="CitationRef"><a epub:type="biblioref" href="#CR87" role="doc-biblioref">87</a></span>], one of the most widely used questionnaires, contains 28 questions such as “How much were you able to control events?”, “How responsive was the environment to actions that you initiated (or performed)?”, “How natural did your interactions with the environment seem?”, or “How completely were all of your senses engaged?”. Analyzing the response patterns in these questionnaires, different dimensions such as “Involvement”, “Sensory fidelity”, “Adaptation/immersion”, and “Interface quality” have emerged in factor analytic studies [<span class="CitationRef"><a epub:type="biblioref" href="#CR86" role="doc-biblioref">86</a></span>].</p><p class="Para" id="Par23">Other approaches to measuring presence include behavioral measurements. If one assumes that presence is given if the reactions to a virtual environment correspond to the behavior in physical environments, then for example, the swaying caused by a moving visual scene or ducking in response to a flying object can be used as an indicator for the degree of presence [<span class="CitationRef"><a epub:type="biblioref" href="#CR19" role="doc-biblioref">19</a></span>]. As a prerequisite for such realistic behavior, Slater considers two aspects: The sensation of being in a real place (“<span id="ITerm13">place illusion</span>”) and the illusion that the scenario being depicted is actually occurring (“<span id="ITerm14">plausibility illusion</span>”) [<span class="CitationRef"><a epub:type="biblioref" href="#CR75" role="doc-biblioref">75</a></span>]. Note, however, that “plausibility” is used here, in comparison with the understanding used in Sect. <span class="InternalRef"><a href="#Sec8">5.2.2.2</a></span>, in a narrower sense with a slightly different meaning.</p><p class="Para" id="Par24">A similar idea is behind the use of psychophysiological measures. If the normal physiological response of a person to a particular situation is replicated in a VR environment, this can be considered as an indicator of presence. Although physiological parameters have been used to measure various functions and applications of VR systems [<span class="CitationRef"><a epub:type="biblioref" href="#CR28" role="doc-biblioref">28</a></span>], they have also been used to measure presence in several studies. Depending on the scenario presented, the Electroencephalogram (EEG) [<span class="CitationRef"><a epub:type="biblioref" href="#CR5" role="doc-biblioref">5</a></span>], heart rate (HR) [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>], or skin conductance and heart rate variability [<span class="CitationRef"><a epub:type="biblioref" href="#CR13" role="doc-biblioref">13</a></span>] were shown to be indicators of different degrees of presence. The exact correlations, however, seem to depend very much on the scenario presented in each case, and in any case, comparative values from a corresponding real-life stimulus are required to calibrate the measurement. Also breaks in presence (BIPs), i.e., moments where the users become aware of the mediatedness of the VR experience due to shortcomings of the system becoming suddenly obvious seem to be associated with <span id="ITerm15">physiological</span> responses [<span class="CitationRef"><a epub:type="biblioref" href="#CR76" role="doc-biblioref">76</a></span>].</p><p class="Para" id="Par25">In general, these approaches seem to be limited to situations in which physiological reactions are sufficiently pronounced, such as anger, fear, or stress [<span class="CitationRef"><a epub:type="biblioref" href="#CR54" role="doc-biblioref">54</a></span>], whereas reactions are less pronounced when the person is predominantly an observer of a scene that has little emotional impact. This may be the reason why manipulations to the level of presence in these studies were almost exclusively realized through changes to the visual display and user interaction, while physiological parameters were hardly used to evaluate the degree of presence in <em class="EmphasisTypeItalic ">acoustic</em> virtual environments.</p><p class="Para" id="Par26">The sense of presence, long used as a measure for evaluating VR and AR systems alone, has recently gained increasing attention as a general neuropsychological phenomenon evolving from biological as well as cultural factors [<span class="CitationRef"><a epub:type="biblioref" href="#CR68" role="doc-biblioref">68</a></span>]. From the perspective of evolutionary psychology, the sense of presence has evolved not to distinguish between real and virtual conditions, but to distinguish the external world from phenomena attributable to one’s own body and mind. On such a theoretical basis, it seems consequent that for achieving a high presence not only the sensory plausibility and the naturalness of the interaction but also the meaning and relevance of the scene for the respective user is essential. The degree of presence in a virtual scene will remain limited if the content is irrelevant to the respective user [<span class="CitationRef"><a epub:type="biblioref" href="#CR66" role="doc-biblioref">66</a></span>].</p><p class="Para" id="Par27">Related to the sense of presence, but less consistently used, is the concept of “<span id="ITerm16">immersion</span>”. In some literature, it is treated as an objective property of VR and AR systems [<span class="CitationRef"><a epub:type="biblioref" href="#CR77" role="doc-biblioref">77</a></span>]. According to this technical understanding, a 5-channel system is considered more “immersive” than a two-channel system, simply because it is able to present a wider range of sound incidence directions to the listener. In other works, however, immersion is treated as psychological construct, i.e., a human response to a technical system [<span class="CitationRef"><a epub:type="biblioref" href="#CR87" role="doc-biblioref">87</a></span>], shifting the meaning of “immersion” closer to the concept of presence [<span class="CitationRef"><a epub:type="biblioref" href="#CR74" role="doc-biblioref">74</a></span>]. Finally, in many works, especially in the field of audio, it remains unclear whether the reasoning about immersion is on a technical or psychological level. Chapter <span class="ExternalRef"><a href="478239_1_En_11_Chapter.xhtml"><span class="RefSource">11</span></a></span> discusses more in depth the aforementioned issue focusing on audiovisual experiences.</p></section><section class="Section3 RenderAsSection3" id="Sec10"><h4 class="Heading"><span class="HeadingNumber">5.2.2.4 </span>Attributes and Taxonomies</h4><p class="Para" id="Par28">With properties such as authenticity, plausibility, or the sense of presence, a global assessment of VR systems is intended. In order to obtain indications of the strengths and weaknesses of these systems and to draw appropriate conclusions for improvement, however, a differential diagnosis is required that separately assesses different qualities of the respective systems. To distinguish these perceptual qualities from technical parameters of the system that may have an influence on them, the former is also referred to as “Quality features” and the latter as “Quality elements” in the Context of Product-<span id="ITerm17">Sound</span> Quality [<span class="CitationRef"><a epub:type="biblioref" href="#CR38" role="doc-biblioref">38</a></span>].</p><div class="Para" id="Par29">For this purpose, different taxonomies for the qualities of virtual acoustic environments, 3D audio or spatial audio systems have been developed. Some of these are based on earlier collections of attributes for sound quality and spatial audio quality [<span class="CitationRef"><a epub:type="biblioref" href="#CR42" role="doc-biblioref">42</a></span>] which were clustered in sound families using semantic analyses such as free categorization or multidimensional scaling (MDS) [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>]. Pedersen and Zacharov (2015) [<span class="CitationRef"><a epub:type="biblioref" href="#CR62" role="doc-biblioref">62</a></span>] developed a sound wheel to present such a lexicon for reproduced sound.<sup><a epub:type="noteref" href="#Fn2" id="Fn2_source" role="doc-noteref">2</a></sup> The wheel format has a longer tradition in the domain of food quality and sensory evaluation [<span class="CitationRef"><a epub:type="biblioref" href="#CR60" role="doc-biblioref">60</a></span>] as a structured and hierarchical form of a lexicon of different sensory characteristics. The selection of the items and the structure of the wheel in [<span class="CitationRef"><a epub:type="biblioref" href="#CR62" role="doc-biblioref">62</a></span>] are based on empirical methods such as hierarchical cluster analysis and measures for discrimination, reliability, and inter-rater agreement of the individual items.<figure class="Figure" id="Fig5"><div class="MediaObject" id="MO5"><img alt="" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Fig5_HTML.png" style="width:29.6em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 5.5</span><p class="SimplePara">SAQI wheel for the evaluation of virtual acoustic environments, structured into informal categories (inner ring) and attributes (outer ring). For definitions and sound examples refer to <span class="ExternalRef"><a href="https://depositonce.tu-berlin.de/handle/11303/157.2"><span class="RefSource">depositonce.​tu-berlin.​de/​handle/​11303/​157.​2</span></a></span> (CC-BY, Fabian Brinkmann)</p></div></figcaption></figure>
</div><div class="Para" id="Par31">While the taxonomies mentioned above were developed for spatial audio systems and product categories such as headphones, loudspeakers, multi-channel sound in general, others were generated with a stronger focus on virtual acoustic environments. Developed by qualitative methods such as expert surveys (DELPHI method [<span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>]) and expert focus groups [<span class="CitationRef"><a epub:type="biblioref" href="#CR48" role="doc-biblioref">48</a></span>], they contain between 7 [<span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>] and 48 attributes [<span class="CitationRef"><a epub:type="biblioref" href="#CR48" role="doc-biblioref">48</a></span>], from which those relevant to the specific experiment can be selected. Examples of a VR/AR specific taxonomy and a rating interface are shown in Figs. <span class="InternalRef"><a href="#Fig5">5.5</a></span> and <span class="InternalRef"><a href="#Fig6">5.6</a></span>.<figure class="Figure" id="Fig6"><div class="MediaObject" id="MO6"><img alt="" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Fig6_HTML.png" style="width:14.78em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 5.6</span><p class="SimplePara">User interface for conducting a SAQI test. The interface is similar to that of a MUSHRA test shown in Fig. <span class="InternalRef"><a href="#Fig1">5.1</a></span> with the difference that the current quality to be rated is given together with the possibility to show its definition (info button) and that the rating scale can also be bipolar. In any case, zero ratings indicated no perceivable difference (CC-BY, Fabian Brinkmann)</p></div></figcaption></figure>
</div></section></section><section class="Section2 RenderAsSection2" id="Sec11"><h3 class="Heading"><span class="HeadingNumber">5.2.3 </span>VR/AR-Specific User Interfaces, Test Procedures, and Toolkits</h3><p class="Para" id="Par32">While the quality measures introduced so far can theoretically be directly transferred for testing in VR and AR, there are specific features that should be addressed: The test method and interface, the technical administration of the test, and the effect of added degrees of freedom on the subjects.</p><div class="Para" id="Par33">First, most of the test methods and user interfaces were developed to be accessed on a computer with a mouse as a pointing and clicking device. The rating procedure and the elements on the user interface might thus not be optimal for testing in VR/AR. This might be less relevant for simple paradigms such as ABX or yes/no tests but can certainly become an issue for rating the quality of multiple test conditions.<figure class="Figure" id="Fig7"><div class="MediaObject" id="MO7"><img alt="" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Fig7_HTML.png" style="width:22.15em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 5.7</span><p class="SimplePara">Interface of the Drag and Drop MUSHRA after [<span class="CitationRef"><a epub:type="biblioref" href="#CR81" role="doc-biblioref">81</a></span>]. The currently playing condition is indicated by the orange button; the loop range and playback position by the orange box and line (CC-BY, Fabian Brinkmann)</p></div></figcaption></figure>
</div><div class="Para" id="Par34">Two approaches were suggested to account for this. Völker et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR81" role="doc-biblioref">81</a></span>] suggested a modified MUSHRA to simplify the rating interface and make it easier to establish an order between test conditions, especially if many test conditions are to be compared against the reference and each other (cf. Fig. <span class="InternalRef"><a href="#Fig7">5.7</a></span>). The idea is to unify playback and rating by making use of drag and drop actions, where the playback is triggered when the subject drags a button corresponding to a test condition, and the rating is achieved by dropping the button on a two-dimensional scale. Ratings obtained with the modified interface were comparable to those obtained with the classic interface in terms of test–retest reliability and discrimination ability. At the same time, the modified interface was preferred by the subjects, and subjects needed less time to complete the rating task. Note that the Drag and Drop MUSHRA could be easily adapted for testing quality taxonomies introduced in Sect. <span class="InternalRef"><a href="#Sec10">5.2.2.4</a></span>.<figure class="Figure" id="Fig8"><div class="MediaObject" id="MO8"><img alt="" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Fig8_HTML.png" style="width:21.85em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 5.8</span><p class="SimplePara">Interface of the elimination task after [<span class="CitationRef"><a epub:type="biblioref" href="#CR67" role="doc-biblioref">67</a></span>]. The currently playing condition is indicated by the orange button (CC-BY, Fabian Brinkmann)</p></div></figcaption></figure>
</div><p class="Para" id="Par35">A VR/AR-tailored approach to further simplify the rating procedure and interface was suggested by Rummukainen et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR67" role="doc-biblioref">67</a></span>]. They designed a simple and easy-to-operate interface, where the subject eliminates the conditions one after another in the order from worst to best (cf. Fig. <span class="InternalRef"><a href="#Fig8">5.8</a></span>). The elimination constitutes a rank order between the stimuli from which interval scaled values—similar to Basic Audio Quality ratings—were obtained by fitting Plackett–Luce models to the ranking vectors. As with the Drag and Drop MUSHRA, the elimination task could be adapted for testing against a reference and using taxonomies.</p><p class="Para" id="Par36">Classic tests of Basic Audio Quality are most often conducted for (static) audio-only conditions and a variety of software solutions is available to conduct such tests [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>, Sect. 9.2.3]. In contrast, tests in VR/AR require the experimental control of complex audiovisual <span id="ITerm18">scenes</span>. In addition, the display of rating interfaces might affect the Quality of Experience (QoE) of interactive environments due to their potentially negative effect on the perceived presence [<span class="CitationRef"><a epub:type="biblioref" href="#CR65" role="doc-biblioref">65</a></span>]. An emerging tool to account for these aspects of AR/VR is the Quality of Experience Evaluation Tool (Q.ExE) currently developed by Raake et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR65" role="doc-biblioref">65</a></span>].</p><p class="Para" id="Par37">A third VR/AR-specific aspect is the possibility of freely exploring an audiovisual scene in <span id="ITerm19">six degrees of freedom (6DoF)</span>. Introducing 6DoF clearly affects the rating behavior of subjects [<span class="CitationRef"><a epub:type="biblioref" href="#CR67" role="doc-biblioref">67</a></span>] and might thus be considered problematic at first glance. An unrestricted 6DoF exploration is, however, the most realistic test condition. While this might introduce additional variance in the results, it might also be argued that results are more comprehensive and reflect more aspects of the audiovisual scene due to free exploration. Whether or not the exploration should be restricted will thus ultimately depend on the aim of an investigation.</p></section></section><section class="Section1 RenderAsSection1" id="Sec12"><h2 class="Heading"><span class="HeadingNumber">5.3 </span>Audio Reproduction Techniques</h2><p class="Para" id="Par38">Two fundamentally different paradigms can be distinguished in audio reproduction for VR/AR that can be illustrated with the help of Fig. <span class="InternalRef"><a href="#Fig9">5.9</a></span>. The picture shows a simple sound field of a point source being reflected by an infinite wall.</p><p class="Para" id="Par39">The first paradigm is to reproduce the entire sound field in a controlled zone, which has two advantages. First, multiple listeners can freely explore the sound field at the same time, and second, the reproduction is already individual as every listener naturally perceives the sound through their own ears. However, there are three disadvantages. First, reproducing the entire sound field requires tens or hundreds of loudspeakers depending on the reproduction algorithm and the size of the listening area. Second, it requires an acoustically treated environment to avoid detrimental effects due to reflections from the reproduction room itself. Third, it is often challenging to achieve a correct reproduction covering the entire hearing range from approximately 20 Hz to 20 kHz. In the following, this reproduction paradigm will be referred to as <em class="EmphasisTypeItalic ">sound field synthesis</em> (SFS).</p><p class="Para" id="Par40">The second paradigm is to only reproduce the sound field at the listeners’ ears. The three advantages of this approach are that it can be realized with a single pair of headphones or loudspeakers, that at least headphone-based reproduction does not pose any demands on the reproduction room, and that a broad frequency range can be correctly reproduced. In turn, two disadvantages arise. First, the position and head orientation of the listeners must be tracked to enable a free exploration of the sound field. Second, the individualization of the ear signals is challenging. Often, the reproduced signals stem from a dummy head, which can cause artifacts such as coloration and increased localization errors in case the ears, head, and torso of the listener differ from the dummy head. This reproduction paradigm will be referred to as <em class="EmphasisTypeItalic "><span id="ITerm20">binaural synthesis</span></em> in the following.</p><div class="Para" id="Par41">It is interesting to see that the advantages and disadvantages of the two paradigms are exactly contrary thus generating a strong bond between the application and reproduction paradigm, whereas binaural synthesis is the apparent option for any application on mobile devices, sound field synthesis is appealing for public or open spaces such as artistic performances and public address systems. The next sections will introduce the two paradigms in more detail. We focus on technical aspects but start with brief theoretical introductions to foster a better understanding of the subject as a whole.<figure class="Figure" id="Fig9"><div class="MediaObject" id="MO9"><img alt="" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Fig9_HTML.png" style="width:20.95em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 5.9</span><p class="SimplePara">Sound field of a point source reflected by an infinite wall. The direct and reflected sound fields are shown as red and blue circles and the direct and reflected sound paths to the listener as red and blue dashed lines. The image of the head in gray denotes the listening position. (CC-BY, Fabian Brinkmann)</p></div></figcaption></figure>
</div><section class="Section2 RenderAsSection2" id="Sec13"><h3 class="Heading"><span class="HeadingNumber">5.3.1 </span>Sound Field Analysis and Synthesis</h3><p class="Para" id="Par42">The idea behind <span id="ITerm21">sound field analysis and synthesis</span> (SFA/SFS) is to reproduce a desired sound field within a defined listening area using a loudspeaker array. The example in Fig. <span class="InternalRef"><a href="#Fig10">5.10</a></span> shows this for the simple case of a plane wave traveling in the normal direction of a linear <span id="ITerm22">array</span>.</p><div class="Para" id="Par43">Two fundamentally different SFA/SFS approaches can be distinguished. <em class="EmphasisTypeItalic ">Physically motivated</em> algorithms aim at capturing and reproducing sound fields physically correct, while <em class="EmphasisTypeItalic ">perceptually motivated</em> methods aim at capturing and synthesizing sound field properties that are deemed to be of high perceptual relevance.<figure class="Figure" id="Fig10"><div class="MediaObject" id="MO10"><img alt="" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Fig10_HTML.png" style="width:20.55em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 5.10</span><p class="SimplePara">Sound field synthesis of a plane wave traveling from bottom to top (red fat lines) by a linear point source array (blue points and blue thin semi-circles) flush-mounted into a sound hard wall (gray line) (CC-BY, Fabian Brinkmann)</p></div></figcaption></figure>
</div><section class="Section3 RenderAsSection3" id="Sec14"><h4 class="Heading"><span class="HeadingNumber">5.3.1.1 </span>Sound Field Acquisition and Analysis</h4><p class="Para" id="Par44">Sound field synthesis requires a sound field that should be reproduced and there are two options for its acquisition: through measurement or simulation. Measured sound fields can have a high degree of realism and can, for example, be used for broadcasting concerts, while simulated sound fields offer more flexibility in the design of the auditory scene and are thus often used in game audio engines (please refer to Chap. <span class="ExternalRef"><a href="478239_1_En_3_Chapter.xhtml"><span class="RefSource">3</span></a></span> for an introduction to interactive auralization)<span id="ITerm23"/>. The description and evaluation of sound field simulation techniques is beyond the scope of the article and the interested reader is kindly referred to related review articles [<span class="CitationRef"><a epub:type="biblioref" href="#CR10" role="doc-biblioref">10</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR79" role="doc-biblioref">79</a></span>].</p><p class="Para" id="Par45">Sound fields are usually measured through microphone arrays, i.e., spatially distributed microphones that are in most cases positioned on the surface of a rigid or imaginary sphere. They can be used to directly record sound scenes such as concerts. In some cases, however, a direct recording will be limiting as it does not allow to change the audio content once the recording is finished. This can be realized if so-called spatial room impulse responses (SRIRs)<span id="ITerm24"/> are measured, i.e., impulse responses that describe the sound propagation between sound sources and each microphone of the array.</p><p class="Para" id="Par46">A common method for physically motivated SFA is the plane wave decomposition (PWD), which applies Fourier Transforms with respect to time and space to the acquired sound field [<span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>, Chap. 2]. It derives a spatially continuous description of the analyzed sound field containing information on the times and directions of arriving plane wave. If the analyzing array has sufficiently many microphones, PWD can yield a physically correct and complete description of the sound field.</p><p class="Para" id="Par47">Popular approaches for perceptually motivates SFA are spatial impulse response rendering (SIRR), <span id="ITerm25">directional audio coding (DirAC)</span>, and the spatial decomposition method (SDM) [<span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR78" role="doc-biblioref">78</a></span>, Chaps. 4–6]. These approaches use a time–frequency analysis to extract the <span id="ITerm26">direction of arrival</span> and in case of SIRR and DirAC also the residual diffuseness for each time–frequency slot. The intention of this is to extract these information from signals recorded with only a few microphones—typically between 4 and 16—and reproduce the signals with an increased resolution using methods introduced in the following sections. SIRR and SDM only work with SRIRs, while PWD and DirAC also work with direct recordings. While SDM uses a broadband frequency analysis and extremely short time windows, the remaining methods use perceptually motivated time and frequency resolutions. SDM is able to extract a single prominent reflection per time window while the PWD and higher order realizations of SIRR and DirAC can detect multiple reflections in each time–frequency slot.</p></section><section class="Section3 RenderAsSection3" id="Sec15"><h4 class="Heading"><span class="HeadingNumber">5.3.1.2 </span>Physically Motivated Sound Field Reproduction</h4><p class="Para" id="Par48">The two methods for physically motivated sound field reproduction are wave field synthesis (WFS, works with linear, planar, rectangular, and cubic loudspeaker arrays)<span id="ITerm27"/> and near-field compensated higher order <span id="ITerm28">Ambisonics</span> (NFC-HOA, works with circular and spherical arrays) [<span class="CitationRef"><a epub:type="biblioref" href="#CR1" role="doc-biblioref">1</a></span>]. Both methods can reproduce plane waves and point sources by filtering and delaying the sounds for each loudspeaker in the array. In the simple case shown in Fig. <span class="InternalRef"><a href="#Fig10">5.10</a></span>, all loudspeakers play identical signals. Because of their high computational demand, WFS and NFC-HOA are rarely used with measured sound fields that consist of hundreds of sources/waves. One possible approach is to use only a few point sources for the direct sound and <span id="ITerm29">early reflections</span>, and a small number of plane waves for the <span id="ITerm30">reverberation</span>.</p></section><section class="Section3 RenderAsSection3" id="Sec16"><h4 class="Heading"><span class="HeadingNumber">5.3.1.3 </span>Perceptually Motivated Sound Field Reproduction</h4><p class="Para" id="Par49">The most common methods for perceptually motivated sound field reproduction are <span id="ITerm31">vector-based amplitude panning (VBAP)</span>, multiple direction amplitude panning (MDAP), and Ambisonics panning, which aim at reproducing point-like sources [<span class="CitationRef"><a epub:type="biblioref" href="#CR89" role="doc-biblioref">89</a></span>, Chaps. 1, 3, and 4]. VBAP is extensions of stereo panning to arbitrary loudspeaker array geometries. It uses one to three speakers that are closest to the position of the virtual source to create a phantom source. MDAP creates a discrete ring of phantom sources—each realized using VBAP—around the position of the virtual source to achieve that the perceived source width becomes almost independent from the position of the virtual source. Ambisonics panning could be thought of as a beamformer that uses all loudspeakers of the array simultaneously to excite circular or spherical sound field modes. In this case, the position of the virtual source is given by the position of the beam. Similar to MDAP, Ambisonics yields virtual sources with an almost position-independent perceived width. In all cases, the degree to which the width of the sources can be controlled increases with the number of <span id="ITerm32">loudspeakers</span>.</p><div class="Para" id="Par50">In many applications, these methods are used as a means to reproduce sound fields that were analyzed using SIRR, SDM, and DirAC. Two reasons for this are their computational efficiency and the fact that they are relatively robust against irregular loudspeaker arrays (non-spherical, missing speakers), which are advantages over physically motivated approaches. VBAP and MDAP are robust to irregular arrays by design (they do not pose any demands on the array geometry). This is not generally true for Ambisonics panning, however, the state-of-the-art All-Round Ambisoncs Decoder (AllRAD, [<span class="CitationRef"><a epub:type="biblioref" href="#CR89" role="doc-biblioref">89</a></span>, Sect. 4.9.6]), which combines VBAP and Ambisonics panning, can well handle irregular arrays.<figure class="Figure" id="Fig11"><div class="MediaObject" id="MO11"><img alt="" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Fig11_HTML.png" style="width:33.95em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 5.11</span><p class="SimplePara">Example of a headphone-based pipeline for binaural synthesis. Dashed lines indicate acoustic signals; black lines indicate digital signals; gray lines indicate movements in 6DoF. <span class="InlineEquation" id="IEq2"><img alt="$$H_c$$" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Chapter_TeX_IEq2.png" style="width:1.38em"/></span> denote compensation filters for the recording (yellow) and reproduction equipment (red, CC-BY, Fabian Brinkmann)</p></div></figcaption></figure>
</div></section></section><section class="Section2 RenderAsSection2" id="Sec17"><h3 class="Heading"><span class="HeadingNumber">5.3.2 </span>Binaural Synthesis</h3><p class="Para" id="Par51">The fundamental theorem of binaural technology is that recording and reproducing the sound pressure signals at a listener’s ears will evoke the same auditory perception as if the listener was exposed to the actual sound field. This is because all acoustic cues that the human auditory system exploits for <span id="ITerm33">spatial hearing</span> are contained in the ear signals. These cues are interaural time and level differences (ITD, ILD), spectral cues (SC), <span id="ITerm34">and</span> <span id="ITerm35">environmental</span> <span id="ITerm36">cues</span>. ITD and ILD stem from the spatial separation of the ears and the acoustic shadow of the head and make it possible to perceive the position of a source in the lateral dimension (left/right). Spectral cues originate from direction-dependent filtering of the outer ear and enable us to perceive the source position in the polar dimension (up/down). The most prominent environmental cue might be reverberation from which information about the source distance and the size of a room can be extracted. For more information please refer to Blauert  [<span class="CitationRef"><a epub:type="biblioref" href="#CR7" role="doc-biblioref">7</a></span>] and to Chap. <span class="ExternalRef"><a href="478239_1_En_4_Chapter.xhtml"><span class="RefSource">4</span></a></span> of this volume.</p><div class="Para" id="Par52">An example of a binaural processing pipeline with <span id="ITerm37">headphone reproduction</span> is shown in Fig. <span class="InternalRef"><a href="#Fig11">5.11</a></span>. The processed binaural signals are stored or directly streamed to the listener whereby the signals are selected and/or processed according to the current position and head orientation of the listener. In any case, a physically correct simulation requires compensating the recording and reproduction equipment (loudspeakers, microphones, headphones) to assure an unaltered reproduction of the binaural signals. These compensation filters are usually separated for signal acquisition and reproduction to maximize the flexibility of the pipeline. For the same reason, anechoic or dry audio content is often convoluted with acquired binaural impulse responses, which makes it possible to change the audio content, without changing the stored binaural signals. The next sections detail the blocks of the introduced reproduction pipeline one by one.<figure class="Figure" id="Fig12"><div class="MediaObject" id="MO12"><img alt="" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Fig12_HTML.png" style="width:22.15em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 5.12</span><p class="SimplePara">HRIR measurement system at the Technical University of Berlin with details of the position procedure using cross line lasers. During the measurement, the subjects are wearing in-ear microphones, are sitting on the chair in the center of the loudspeaker array, and are continuously rotated to measure a full spherical HRIR data set. In addition, the wire frames on the floor are covered with absorbing material (CC-BY, Fabian Brinkmann)</p></div></figcaption></figure>
</div><section class="Section3 RenderAsSection3" id="Sec18"><h4 class="Heading"><span class="HeadingNumber">5.3.2.1 </span>Signal Acquisition and Processing</h4><p class="Para" id="Par53">The most basic technique is to directly record sound events—for example a concert—with a dummy head, i.e., a replica of a human head (and torso) that is equipped with microphones at the positions of the ear channel entrance or inside artificial <span id="ITerm38">ear</span> <span id="ITerm39">channels</span>. This requires a straightforward compensation of the recording microphones by means of an inverse filter, whereas the sources are considered to be a part of the scene and thus remain uncompensated. This approach is, however, very inflexible because the position and orientation of the listener and sources can not be changed during reproduction. It is thus more common to measure or simulate spherical sets of head-related impulse responses (HRIRs)<span id="ITerm40"/> that describe the sound propagation between a free-field sound source and the listeners ears (cf. [<span class="CitationRef"><a epub:type="biblioref" href="#CR88" role="doc-biblioref">88</a></span>, Chaps. 2 and 4] and Fig. <span class="InternalRef"><a href="#Fig12">5.12</a></span>). In this case, the sound source has to be compensated as well. The gain in flexibility stems from the possibility to use anechoic or dry audio content and select the HRIR according to the current source and head position of the listener. While HRIRs are not often directly used because anechoic listening conditions are unrealistic for most applications, they are essential for room acoustic simulations [<span class="CitationRef"><a epub:type="biblioref" href="#CR80" role="doc-biblioref">80</a></span>]. Acoustic simulations can be used to obtain <span id="ITerm41">binaural room impulse responses</span> (BRIRs) that describe the sound propagation between a sound source in a reverberant environment and the listeners ears. BRIRs can also be measured, thereby increasing the degree of realism at the cost of increasing the effort to measure BRIRs for multiple positions and orientations of the listener to enable listener movements during playback.</p></section><section class="Section3 RenderAsSection3" id="Sec19"><h4 class="Heading"><span class="HeadingNumber">5.3.2.2 </span>Head Tracking</h4><p class="Para" id="Par54">Tracking the head position of the listener is required for dynamic binaural reproduction, i.e., a reproduction that accounts for movements of the listener by providing binaural signals according to the angle and distance between the source and the listener’s head. While it will be sufficient for some applications to only track the head orientation, the general VR/AR case requires six degrees of freedom (6DoF, i.e., translation and rotation in x, y, and <span id="ITerm42">z</span>).</p><p class="Para" id="Par55">In general, two tracking approaches <span id="ITerm43">exist</span>. Relative tracking systems track the position of the listener with respect to a potentially unknown starting point, while absolute tracking systems establish a world coordinate system within which the absolute position of the listener is tracked. Relative systems usually use inertial measurement units (IMU) to derive the listener position from combined sensing of a gyroscope, an accelerometer, and possibly a magnetometer. Absolute systems can use optical tracking by deriving the listener position from images of a single or multiple (infrared) cameras, or GPS data.</p><p class="Para" id="Par56">Artifact-free rendering requires a tracking precision of <span class="InlineEquation" id="IEq3"><img alt="$$1^\circ $$" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Chapter_TeX_IEq3.png" style="width:1.06em"/></span> and 1 cm [<span class="CitationRef"><a epub:type="biblioref" href="#CR32" role="doc-biblioref">32</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR46" role="doc-biblioref">46</a></span>], and a total system latency of about 50 ms [<span class="CitationRef"><a epub:type="biblioref" href="#CR45" role="doc-biblioref">45</a></span>]. Note that a significantly lower latency of about 15 ms is required for rendering visual stimuli in AR applications [<span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>]. A challenge for relative tracking systems is to control long-term drift of the IMU unit, while visual occlusion is problematic for optical absolute tracking systems.</p></section><section class="Section3 RenderAsSection3" id="Sec20"><h4 class="Heading"><span class="HeadingNumber">5.3.2.3 </span>Reproduction with Headphones</h4><div class="Para" id="Par57">Headphone reproduction requires a compensation of the headphone transfer function (HpTF) by means of an inverse filter to deliver the binaural signals to the listener’s ear without introducing additional coloration. However, the design of the inverse filter is not straightforward. Two aspects are problematic. First, the HpTF considerably varies across listeners and headphone models, which may require the use of listener and model-specific compensation filters depending on the demands of the application. Second, the low-frequency response and the center frequency and depth of high-frequency notches in the HpTF strongly depend on the fit of the headphone and may considerably change if the listener re-positions the headphones (cf. Fig. <span class="InternalRef"><a href="#Fig13">5.13</a></span>). To account for the variance, the average HpTF can be used to design the inverse filter, and the filter gain at low and high frequencies can be restricted using regularized inversion [<span class="CitationRef"><a epub:type="biblioref" href="#CR24" role="doc-biblioref">24</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR46" role="doc-biblioref">46</a></span>]. Once calculated, the static headphone filter can be applied to the binaural signals by means of convolution.<figure class="Figure" id="Fig13"><div class="MediaObject" id="MO13"><img alt="" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Fig13_HTML.png" style="width:19.8em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 5.13</span><p class="SimplePara">Headphone transfer functions of subject 6 from the HUTUBS HRTF database for the left ear of a Sennheiser HD650 headphone [<span class="CitationRef"><a epub:type="biblioref" href="#CR9" role="doc-biblioref">9</a></span>]. Gray lines show the effect of re-positioning. Black lines show the averaged HpTF (CC-BY, Fabian Brinkmann)</p></div></figcaption></figure>
</div><p class="Para" id="Par58">In addition to this static convolution, a dynamic convolution is often required to render the current HRIR or BRIR. Since real-time audio processing works on blocks of audio, this is simply achieved by using the current HRIR as long as the listener does not move. If the listener moves, the past and current HRIR are both convolved simultaneously and a cross fade with the length of one audio block is applied between the two [<span class="CitationRef"><a epub:type="biblioref" href="#CR82" role="doc-biblioref">82</a></span>].</p></section><section class="Section3 RenderAsSection3" id="Sec21"><h4 class="Heading"><span class="HeadingNumber">5.3.2.4 </span>Reproduction with Loudspeakers</h4><p class="Para" id="Par59">While delivering binaural signals through headphones is the most obvious solution due to the one-to-one correspondence between the two ears and two speakers of the headphone, two approaches for transaural reproduction using loudspeakers are also <span id="ITerm44">available</span>.</p><p class="Para" id="Par60">The first approach uses only two loudspeakers. In analogy to headphone reproduction, there is a one-to-one correspondence between the ear signals and speakers, and the filter for the left loudspeaker compensates for the transfer function between the speaker and the left ear. In contrast to headphone reproduction, however, this requires an additional filter for <span id="ITerm45">cross-talk cancellation (CTC)</span> between the right speaker and the left ear (the filters for the right ear work accordingly). This requires an iterative design of the compensation filters for all possible positions of the head with respect to the loudspeakers and thus a dynamic convolution already for the compensation filters [<span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>]. Optionally, more loudspeakers can be used to optimize the system for different listening positions or frequency ranges.</p><p class="Para" id="Par61">The second approach uses linear or circular loudspeaker arrays. Here, the idea is to shoot two narrow audio beams in the direction of the listener’s ears. Because the beams concentrate most of their energy towards the listener’s ears, a high separation between the left and right ear beams can be achieved depending on the array geometry [<span class="CitationRef"><a epub:type="biblioref" href="#CR20" role="doc-biblioref">20</a></span>]. In this case, a one-to-one correspondence is established between the two beams and the ears, and cross-talk compensation is not required if the beams are sufficiently narrow. In this case, a dynamic convolution is required to update the beamformers according to the listener’s position.</p></section></section><section class="Section2 RenderAsSection2" id="Sec22"><h3 class="Heading"><span class="HeadingNumber">5.3.3 </span>Binaural Reproduction of Synthesized Sound Fields</h3><p class="Para" id="Par62">It is worth to note that SFS approaches can be combined with binaural reproduction, either by virtualizing the loudspeaker array with an array of HRIRs or through binaural processing stages that build upon the sound field analysis (c.f., [<span class="CitationRef"><a epub:type="biblioref" href="#CR2" role="doc-biblioref">2</a></span>], [<span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>, Sect. 6.4.2] and [<span class="CitationRef"><a epub:type="biblioref" href="#CR89" role="doc-biblioref">89</a></span>, Sect. 4.11]). This makes binaural reproduction the prime framework for rendering spatial audio in AR/VR and SFS a versatile tool within the framework: First, SFS makes it possible to efficiently render binaural signals for arbitrary head orientations from a single SRIR (might require pre-processing to achieve a reasonable quality as detailed in Sect. <span class="InternalRef"><a href="#Sec32">5.4.3</a></span>). Second, SFS makes it possible to include listener movements (translation)—to a limited extent—and thus enables rendering with 6DoF. The realization of 6DoF rendering depends on the sound field representation, which strongly differs across SFS approaches. However, the general idea agrees in many cases. Head rotations can be realized by an inverse rotation of the sound field. For perceptually motivated SFS methods, translation can be realized by manipulating the directions and times of arrival that were obtained through SFA according to the listener’s movements (e.g., [<span class="CitationRef"><a epub:type="biblioref" href="#CR41" role="doc-biblioref">41</a></span>]). The possibility of realizing translation with physically motivated SFS approaches and measured sound fields is, however, rather limited as this would require arrays with hundreds if not thousands of microphones.</p></section></section><section class="Section1 RenderAsSection1" id="Sec23"><h2 class="Heading"><span class="HeadingNumber">5.4 </span>System Performance</h2><p class="Para" id="Par63">This section details the quality that can be achieved with the different reproduction paradigms, starting with binaural synthesis. This is the most common approach, and in case it is used in combination with SFS, it also limits the maximally achievable quality of the SFS.</p><section class="Section2 RenderAsSection2" id="Sec24"><h3 class="Heading"><span class="HeadingNumber">5.4.1 </span>Binaural Synthesis</h3><div class="Para" id="Par64">The authenticity and plausibility of a reproduction system are without a doubt the most integral and comprehensive quality measures and are thus discussed first. However, it is also important to shed light on the relevance of individual components in the reproduction pipeline. While there are many small pieces that contribute to the overall quality, the most relevant might be the individualization of binaural signals, head tracking, and audiovisual stimulation, which are discussed separately.<figure class="Figure" id="Fig14"><div class="MediaObject" id="MO14"><img alt="" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Fig14_HTML.png" style="width:22.15em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 5.14</span><p class="SimplePara">Results of the test for authenticity. Top: Range of differences between the sound field of the real and virtual frontal loudspeakers across head-above-torso orientations. Data was measured at the blocked ear channel entrance and is shown as 12th (light blue) and 3rd octave (dark blue) smoothed magnitude spectra. Bottom: 2-Alternative Forced Choice detection rates for all participants, two audio contents, source positions in front (<span class="InlineEquation" id="IEq4"><img alt="$$0^\circ $$" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Chapter_TeX_IEq4.png" style="width:1.06em"/></span>) and to the left (<span class="InlineEquation" id="IEq5"><img alt="$$90^\circ $$" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Chapter_TeX_IEq5.png" style="width:1.56em"/></span>), and three different acoustical environments (cf. Fig. <span class="InternalRef"><a href="#Fig3">5.3</a></span>). The size of the dots and the numbers next to them indicates how many participants scored identical results. Results on or above the dashed line are significantly above chance, indicating that differences between simulated and real sound fields were reliably audible. 50% correct answers denotes guessing (CC-BY, Fabian Brinkmann)</p></div></figcaption></figure>
</div><section class="Section3 RenderAsSection3" id="Sec25"><h4 class="Heading"><span class="HeadingNumber">5.4.1.1 </span>Authenticity and Plausibility</h4><p class="Para" id="Par65">Headphone-based individual dynamic binaural synthesis can be authentic if reverberant environments and real-life signals, such as speech, are simulated. For this typical use case, 66% of the subjects in Brinkmann et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR8" role="doc-biblioref">8</a></span>] could not hear any differences between a real loudspeaker and its binaural simulation (cf. Fig. <span class="InternalRef"><a href="#Fig14">5.14</a></span>, bottom). However, differences such as coloration become audible if simulating anechoic environments or artificial noise signals. Remaining differences stem from accumulated measurement errors in the range of 1 dB mostly related to the positioning of the subject and the in-ear microphones during the experiment (cf. Fig. <span class="InternalRef"><a href="#Fig3">5.3</a></span>, top). Clearly, these differences can be detected more easily with steady broadband signals such as noise. The effect of reverberance might be twofold. First, the <span id="ITerm46">reverberation</span> might be able to mask audible coloration in the direct sound, and second, reverberant parts of the BRIR might be less prone to coloration artifacts because measurement errors could cancel across reflections arriving from multiple directions.</p><p class="Para" id="Par66">Loudspeaker-based individual binaural synthesis by means of CTC can be authentic in anechoic reproduction rooms [<span class="CitationRef"><a epub:type="biblioref" href="#CR59" role="doc-biblioref">59</a></span>]. However, the quality drastically decreases if the CTC system is set up in reverberant environments, thus limiting the usability of this approach. The decrease in quality is caused by undesired reflections from the reproduction room that can not be compensated in practice due to uncertainties in the exact position of the listener [<span class="CitationRef"><a epub:type="biblioref" href="#CR69" role="doc-biblioref">69</a></span>].</p><p class="Para" id="Par67">Non-individual dynamic binaural synthesis is not authentic but can be plausible, i.e., matching the listeners expectation towards the acoustic environment. This means that differences between a real sound field and a non-individual simulation are audible in a direct comparison, but they are not large enough for the simulation to be detected as such in an indirect comparison. Although the plausibility was only shown for headphone base reproduction of reverberant environments [<span class="CitationRef"><a epub:type="biblioref" href="#CR47" role="doc-biblioref">47</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR63" role="doc-biblioref">63</a></span>], it is reasonable to assume that this also holds the simulation of anechoic environments and loudspeaker based reproduction in anechoic environments. Remaining differences between real sound fields and binaural simulations are discussed in the following section.</p><p class="Para" id="Par68">An example setup for testing authenticity and plausibility is shown in Fig. <span class="InternalRef"><a href="#Fig14">5.14</a></span>. It is important to note that authentic simulations can only be achieved under carefully controlled laboratory conditions. Otherwise, the placement of the headphones will already introduce audible artifacts that would be hard to control in any consumer application [<span class="CitationRef"><a epub:type="biblioref" href="#CR61" role="doc-biblioref">61</a></span>]. It can, however, be assumed that such artifacts are irrelevant for the vast majority of VR/AR applications, where plausibility is a sufficient quality criterion.</p></section><section class="Section3 RenderAsSection3" id="Sec26"><h4 class="Heading"><span class="HeadingNumber">5.4.1.2 </span>Effect of Individualization</h4><p class="Para" id="Par69">Binaural signals (binaural recordings, HRIRs, BRIRs) are highly individual, i.e., they differ across listeners due to different shapes of the listeners‘ ears, heads, and bodies. As a consequence, listening to non-individual binaural signals decreases the audio quality and can be thought of as listening through someone else’s ears. While the decrease in quality could already be seen in the integral measures authenticity and plausibility, this section will look at differences in more <span id="ITerm47">detail</span>.</p><div class="Para" id="Par70">The most discussed degradation caused by non-individual signals is increased uncertainty in source localization [<span class="CitationRef"><a epub:type="biblioref" href="#CR57" role="doc-biblioref">57</a></span>]<span id="ITerm48"/>. Using individual head-related transfer functions (HRTFs, the frequency domain HRIRs), median route mean squared localization errors are approximately 27<span class="InlineEquation" id="IEq6"><img alt="$$^\circ $$" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Chapter_TeX_IEq6.png" style="width:0.57em"/></span> for the polar angle, which denotes the up/down source position, and 15<span class="InlineEquation" id="IEq7"><img alt="$$^\circ $$" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Chapter_TeX_IEq7.png" style="width:0.57em"/></span> for the lateral angle, which denotes the left/right position. Quadrant errors, which are a measure for front–back and up–down confusions (and mixtures thereof), occur in only 4% of the cases. A drastic increase of the quadrant error by a factor of 5 to about 20% and the polar error by a factor of 1.5 to about 40<span class="InlineEquation" id="IEq8"><img alt="$$^\circ $$" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Chapter_TeX_IEq8.png" style="width:0.57em"/></span> can be observed if using non-individual signals. Because source localization in the polar dimension relies on high-frequency cues in the binaural signal, the increased errors can be attributed to differences in ear shapes, which have the strongest influence on binaural signals at high frequencies. The lateral error increases by only 2<span class="InlineEquation" id="IEq9"><img alt="$$^\circ $$" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Chapter_TeX_IEq9.png" style="width:0.57em"/></span>. In this case, the auditory system exploits interaural cues (ITD, ILD) for localization, which stems from the overall head <span id="ITerm49">shape</span>. The fact that head shapes differ less between listeners than ear shapes explains the relatively small changes in this case.<figure class="Figure" id="Fig15"><div class="MediaObject" id="MO15"><img alt="" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Fig15_HTML.png" style="width:33.58em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 5.15</span><p class="SimplePara">Perceived differences between a real sound field and the individual (blue, left) and non-individual (red, right) dynamic binaural simulation thereof. Results are pooled across an anechoic, dry, and wet acoustic environment. The horizontal lines show the medians, the boxes the interquartile ranges, and the vertical lines the minimum and maximum perceived differences. Scale labels were omitted for clarity and can be found in [<span class="CitationRef"><a epub:type="biblioref" href="#CR48" role="doc-biblioref">48</a></span>] (CC-BY, Fabian Brinkmann)</p></div></figcaption></figure>
</div><p class="Para" id="Par71">Whereas localization might be one of the most important properties of audio in virtual acoustic realities, it is by far not the only aspect that degrades due to non-individual signals. An extensive qualitative analysis is shown in Fig. <span class="InternalRef"><a href="#Fig15">5.15</a></span>. The results were obtained with pulsed pink noise as audio content in a direct comparison between a frontal loudspeaker- and headphone-based dynamic binaural syntheses using the setup shown in Fig. <span class="InternalRef"><a href="#Fig3">5.3</a></span>. Apart from qualities related to the scene geometry (localization, externalization, etc.), considerable degradations can also be observed for aspects related to the tone color. In sum, this also lead to a larger overall difference and subjects rated the non-individual simulation to be less natural and clear than its individual counterpart. As a result, the individual simulation was generally preferred (attribute <em class="EmphasisTypeItalic ">liking</em>), however, the presence was not affected. Because the similarity between the individual BRIRs and the non-individual BRIRs used in the test depends on the listener, the results for non-individual synthesis have considerably higher variance (indicated by the interquartile ranges).</p><p class="Para" id="Par72">Differences for individual binaural synthesis are small compared to non-individual synthesis. In this case, noteworthy differences only remain for the tone color. These differences stem from measurement uncertainties that arise mostly due to positioning inaccuracies of the subjects and in-ear microphones. As mentioned above, these differences become inaudible if using speech signals instead of pulsed noise.</p><p class="Para" id="Par73">Individualization is not only important for HRIRs and BRIRs but also for the <span id="ITerm50">headphone compensation</span> (HpC). The examples above either used fully individual (individual HRIRs/BRIRs and HpC) or fully non-individual (non-individual HRIRs/BRIRs and HpC) simulations. Combinations of these cases were investigated by Engel et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR15" role="doc-biblioref">15</a></span>] and Gupta et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>]. As expected, fully individual simulations always have the highest quality, and considerable degradations can be observed if using individual signals with a non-individual HpC. If an individual HpC is not feasible, differences between individual and non-individual signals were only significant for the source direction but not for the perceived distance, coloration, and overall similarity. In any case, at least a non-individual HpC should be used because differences are the largest for simulations without HpC.</p><p class="Para" id="Par74">Many individualization approaches are available that mitigate the detrimental effects of non-individual signals to a certain degree [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>]. However, they demand additional action from the listener to obtain individual or individualized signals. It is thus worth noting—and discussed in the next sections—that head tracking and visual stimulation are two means to mitigate some effects that do not require actions from the listener.</p></section><section class="Section3 RenderAsSection3" id="Sec27"><h4 class="Heading"><span class="HeadingNumber">5.4.1.3 </span>Effect of Head Tracking</h4><p class="Para" id="Par75">Without head <span id="ITerm51">tracking</span>, the auditory scene will move if the listeners move their head, which is a very unnatural behavior for most VR/AR applications. Head-tracked dynamic simulations in which the auditory scene remains stable during head movements have thus become the standard. Besides the general improvement of the sense of presence and immersion, this has at least two more benefits.</p><p class="Para" id="Par76">First, localization errors for non-individual signals decrease if head tracking is enabled [<span class="CitationRef"><a epub:type="biblioref" href="#CR52" role="doc-biblioref">52</a></span>]. While the lateral localization errors remain largely unaffected, front–back confusion completely disappears if the listeners rotate their head by 32<span class="InlineEquation" id="IEq10"><img alt="$$^\circ $$" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Chapter_TeX_IEq10.png" style="width:0.57em"/></span> or more to the left or <span id="ITerm52">right</span>. This can be explained by movement-induced dynamic changes in the binaural signals. As listeners move their head to the left, the left ear moves away from the source if it is in front, and the right ear moves towards it. Because this behavior would be exactly reversed for a source behind the listener, the auditory system is able to resolve the front–back confusion through the head motion. Up–down confusion can be resolved in analogy through head nodding to the left or right. Additionally, the elevation error decreases by a third for head rotations of 64<span class="InlineEquation" id="IEq11"><img alt="$$^\circ $$" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Chapter_TeX_IEq11.png" style="width:0.57em"/></span> to the left or right. This can be explained by the fact that dynamic changes in the binaural signals are largest for a frontal source and almost disappear for a source above and below the listener.</p><p class="Para" id="Par77">The second benefit pertains to the <span id="ITerm53">externalization</span> of non-individual virtual sources [<span class="CitationRef"><a epub:type="biblioref" href="#CR31" role="doc-biblioref">31</a></span>]. While sources to the side are well externalized even with non-individual signals, sources to the front and rear were often reported to be perceived as being inside the head. The most likely reason for this is that signals for sources close to the median plane are similar for the left and right ears. In contrast, the ear signals differ in time and level for sources to the side. These differences stem from the spatial separation of the ears and the acoustic shadow of the head and might provide the auditory system with evidence of the presence of an external source. If listeners perform large head rotations to the left and right, dynamic binaural cues are induced and the externalization of frontal and rear sources significantly increases.</p><p class="Para" id="Par78">Despite the positive effects of head tracking, it has to be kept in mind that listeners will not always perform large head movements just because they can. The actual benefit might thus often be smaller than reported above. However, dynamic cues that are similar to those of head movements can also be induced by a moving source, which was shown to have a similarly positive effect on externalization [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>]. An effect of source movements for localization has not yet been extensively investigated. For the case of distance localization, it was already shown that active self-motion is more efficient than passive self-motion and source motion [<span class="CitationRef"><a epub:type="biblioref" href="#CR22" role="doc-biblioref">22</a></span>].</p></section><section class="Section3 RenderAsSection3" id="Sec28"><h4 class="Heading"><span class="HeadingNumber">5.4.1.4 </span>Effect of Visual Stimulation</h4><p class="Para" id="Par79">Because VR/AR applications usually provide congruent audiovisual signals, it is worth to consider the effect of visual stimulation on the audio quality. Interestingly—and in contrast to head tracking—visual stimulation can have positive and negative <span id="ITerm54">effects</span>.</p><p class="Para" id="Par80">The possibly most important positive aspect is the <span id="ITerm55">ventriloquism effect</span>, which describes the phenomenon that a fused audiovisual event is perceived at the location of the visual stimuli even if the position of the auditory event deviates from that of the visual event. Median thresholds below which fusion appears are approximately 15<span class="InlineEquation" id="IEq12"><img alt="$$^\circ $$" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Chapter_TeX_IEq12.png" style="width:0.57em"/></span> in the horizontal plane and 45<span class="InlineEquation" id="IEq13"><img alt="$$^\circ $$" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Chapter_TeX_IEq13.png" style="width:0.57em"/></span> in the median plane if presenting a realistic stereoscopic 3D video of a talker [<span class="CitationRef"><a epub:type="biblioref" href="#CR29" role="doc-biblioref">29</a></span>]. Comparing this to localization errors reported in Sect. <span class="InternalRef"><a href="#Sec26">5.4.1.2</a></span>, it can be hypothesized that localization <span id="ITerm56">errors</span> will drastically decrease if not completely disappear even for non-individual binaural synthesis due to audiovisual fusion and the ventriloquism effect if a source is visible and in the field of view. It has to be kept in mind, however, that the degree of realism of the visual stimulation—termed <em class="EmphasisTypeItalic ">compellingness</em> in [<span class="CitationRef"><a epub:type="biblioref" href="#CR29" role="doc-biblioref">29</a></span>]—affects the strength of the ventriloquism effect. Thus, fusion thresholds can decrease for less realistic visual stimulation.</p><p class="Para" id="Par81">Quality degrading effects can occur if the (expected) acoustics of the visually presented room does not match the acoustics of the auditorily presented room—an effect termed <em class="EmphasisTypeItalic "><span id="ITerm57">room divergence</span></em>. This effect is especially relevant for AR applications where listeners can naturally explore real audiovisual environments to which artificial auditory or audiovisual events are added. However, room divergence can also appear in VR applications for example due to badly parameterized room acoustic simulations. Room divergence is not extensively researched up to date, but it was already shown that it can affect distance perception and externalization [<span class="CitationRef"><a epub:type="biblioref" href="#CR23" role="doc-biblioref">23</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR83" role="doc-biblioref">83</a></span>]<span id="ITerm58"/>. While degradations with respect to these qualities might as well be mitigated by the ventriloquism effect [<span class="CitationRef"><a epub:type="biblioref" href="#CR56" role="doc-biblioref">56</a></span>], the room divergence might also affect higher level qualities such as plausibility and presence.</p></section></section><section class="Section2 RenderAsSection2" id="Sec29"><h3 class="Heading"><span class="HeadingNumber">5.4.2 </span>Sound Field Synthesis</h3><p class="Para" id="Par82">The discussion of SFA/SFS is limited to perceptually motivated approaches because they are predominantly used in VR/AR applications. In-depth evaluations of physically motivated approaches were, for example, conducted by Wierstorf [<span class="CitationRef"><a epub:type="biblioref" href="#CR85" role="doc-biblioref">85</a></span>] and Erbes [<span class="CitationRef"><a epub:type="biblioref" href="#CR17" role="doc-biblioref">17</a></span>].</p><section class="Section3 RenderAsSection3" id="Sec30"><h4 class="Heading"><span class="HeadingNumber">5.4.2.1 </span>Vector-Based and Ambisonics Panning</h4><p class="Para" id="Par83">The most important quality factor for loudspeaker-based reproduction approaches is the number of loudspeakers <em class="EmphasisTypeItalic ">L</em>. In case of Ambisonics, there is a strict dependency between <em class="EmphasisTypeItalic ">L</em> and the achievable spatial resolution, which is determined by the so-called Ambisonics order <span class="InlineEquation" id="IEq14"><img alt="$$N\lesssim (L+1)^2$$" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Chapter_TeX_IEq14.png" style="width:6.06em"/></span>. Intuitively, the spatial resolution increases with increasing <span id="ITerm59">Ambisonics</span> order. For the amplitude panning methods, the fluctuation of the perceived source width across source positions (VBAP)<span id="ITerm60"/> and the minimally achievable source width that is independent of the source position (MDAP) increase with <em class="EmphasisTypeItalic ">L</em>.</p><p class="Para" id="Par84">Both approaches—vector-based and Ambisonics panning—have distinct disadvantages at very low orders <span class="InlineEquation" id="IEq15"><img alt="$$N \lesssim 2$$" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Chapter_TeX_IEq15.png" style="width:3em"/></span>, i.e., for arrays consisting of only about four to nine loudspeakers. In this case, Ambisonics and MDAP have a rather limited spatial resolution and Ambisonics additionally exhibits a dull sound color. For VBAP, on the other hand, the source width heavily depends on the position of the virtual source. Using state-of-the-art Ambisonics decoders, the differences between the approaches decrease at orders <span class="InlineEquation" id="IEq16"><img alt="$$N \gtrsim 3$$" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Chapter_TeX_IEq16.png" style="width:3em"/></span>, i.e., for arrays consisting of 16 loudspeakers or more. For such arrays, all methods are able to produce virtual sources whose width and loudness are independent of the source position. For an in-depth discussion of these properties the interested reader is referred to Zotter and Frank [<span class="CitationRef"><a epub:type="biblioref" href="#CR89" role="doc-biblioref">89</a></span>, Chaps. 1 and 3] and Pulkki et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>, Chap. 5].</p></section><section class="Section3 RenderAsSection3" id="Sec31"><h4 class="Heading"><span class="HeadingNumber">5.4.2.2 </span>SIRR, SDM, and DirAC</h4><p class="Para" id="Par85">Different versions of SIRR and DirAC have been proposed over the past years. The two most advanced versions are the so-called Virtual Microphone DirAC, which improved the rendering of diffuse sound field components over the original DirAC version, and higher order DirAC/SIRR, which make it possible to estimate more than one directional component for each time frame to improve the rendering of challenging acoustic scenes [<span class="CitationRef"><a epub:type="biblioref" href="#CR53" role="doc-biblioref">53</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>, Chaps. 5 and 6]<span id="ITerm61"/>. For an array consisting of 16 loudspeakers that are set up in acoustically treated environments (anechoic or very dry), SIRR and DirAC can achieve a high audio quality of about 80–90% on a MUSHRA-like rating scale (cf. Sect. <span class="InternalRef"><a href="#Sec4">5.2.1.1</a></span>). Best results are obtained for idealized microphone array signals, i.e., if the SIRR/DirAC input signals are synthetically generated instead of recorded with a real microphone. Using a real microphone array decreased the audio quality by about 10% on average.</p><p class="Para" id="Par86">Similar audio qualities were obtained for SDM [<span class="CitationRef"><a epub:type="biblioref" href="#CR78" role="doc-biblioref">78</a></span>] and binaural SDM [<span class="CitationRef"><a epub:type="biblioref" href="#CR2" role="doc-biblioref">2</a></span>]. The latter study showed that binaural SDM has a <em class="EmphasisTypeItalic ">plausibility score</em> similar to sound fields emitted by real loudspeakers. Although the plausibility score differs from the definition of plausibility in Sect. <span class="InternalRef"><a href="#Sec8">5.2.2.2</a></span>, it is reasonable to assume that SDM—and also SIRR and DirAC—can be plausible, however, not authentic.</p><p class="Para" id="Par87">So far, perceptual evaluations were conducted in acoustically treated listening rooms and it is plausible to expect that the quality decreases with an increasing degree of reverberation in the listening environment. Moreover, a comprehensive comparative evaluation of SIRR and SDM is missing to date and existing studies sometimes used test conditions that might have favored one approach over the others.</p><p class="Para" id="Par88">SIRR, SDM, and DirAC might be the most common, but by far, not the only methods for perceptually motivated SFS. Broader overviews are, for example, given by Pulkki et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>, Chap. 4] and Zotter and Frank [<span class="CitationRef"><a epub:type="biblioref" href="#CR89" role="doc-biblioref">89</a></span>, Sect. 5.8].</p></section></section><section class="Section2 RenderAsSection2" id="Sec32"><h3 class="Heading"><span class="HeadingNumber">5.4.3 </span>Binaural Reproduction of Synthesized Sound Fields</h3><div class="Para" id="Par89">As mentioned before, SFS approaches can be reproduced via headphones if virtualizing the loudspeaker array with a set of HRTFs. The virtualization is uncritical if the number of virtual loudspeakers can be freely selected, which often is the case for SIRR, SDM, and DirAC. The situation is more difficult, however, for Ambisonics signals which are typically order limited to <span class="InlineEquation" id="IEq17"><img alt="$$1\lesssim N \lesssim 7$$" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Chapter_TeX_IEq17.png" style="width:4.94em"/></span>. The challenge in this case is to derive an Ambisonics version of the HRTF data set with the same order restriction. Without specifically tailored algorithms, an order of <span class="InlineEquation" id="IEq18"><img alt="$$N \approx 35$$" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Chapter_TeX_IEq18.png" style="width:3.49em"/></span> is required for an authentic Ambisonics representation of HRTFs and simply restricting the order causes clearly audible artifacts [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>].<figure class="Figure" id="Fig16"><div class="MediaObject" id="MO16"><img alt="" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Fig16_HTML.png" style="width:25.2em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 5.16</span><p class="SimplePara">Perceived differences between a reference and order limited binaural renderings of microphone array recordings. For details refer to [<span class="CitationRef"><a epub:type="biblioref" href="#CR49" role="doc-biblioref">49</a></span>] (CC-BY, Tim Lübeck)</p></div></figcaption></figure>
</div><p class="Para" id="Par90">A variety of methods have been proposed to mitigate these artifacts. This comprises a global spectral equalization with or without windowing (tapering) of the <span id="ITerm62">spherical harmonics</span> coefficients or a separate treatment of the HRTF phase by means of (frequency-dependent) time alignment or finding an optimal phase that reduces errors in the HRTF magnitude [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR89" role="doc-biblioref">89</a></span>, Sect. 4.11]. A comparative study of these algorithms was conducted by Lübeck et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR49" role="doc-biblioref">49</a></span>]. As shown in Fig. <span class="InternalRef"><a href="#Fig16">5.16</a></span>, the differences between a reference and binaural renderings are small already for <span class="InlineEquation" id="IEq19"><img alt="$$N=3$$" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Chapter_TeX_IEq19.png" style="width:2.94em"/></span>, at least for the best algorithms.</p><p class="Para" id="Par91">Another benefit of <span id="ITerm63">headphone reproduction</span> is that different reproduction techniques can be combined to fine-tune the trade-off between perceptual quality and computational efficiency. One possible solution is to use HRTFs with a high spatial resolution for direct sound rendering (high computational cost, high quality) combined with Ambisonics-based rendering of reverberant components (cost and quality adjustable by means of the SH order) [<span class="CitationRef"><a epub:type="biblioref" href="#CR16" role="doc-biblioref">16</a></span>]. This exploits the fact, that the spatial resolution of the auditory system is higher for the direct sound than for reverberant components [<span class="CitationRef"><a epub:type="biblioref" href="#CR50" role="doc-biblioref">50</a></span>].</p></section></section><section class="Section1 RenderAsSection1" id="Sec33"><h2 class="Heading"><span class="HeadingNumber">5.5 </span>Conclusion</h2><p class="Para" id="Par92">Section <span class="InternalRef"><a href="#Sec2">5.2</a></span> gave an overview of existing quality measures for evaluating 3D audio content and it became apparent that the underlying concepts can also be used to assess audio quality in audiovisual virtual reality. Good suggestions were made to adapt the application of these measures for AR/VR by simplifying the associated rating interfaces and/or adapting methods for the statistical analysis. Open questions in this field mainly seem to relate to the higher level constructs of QoE and presence. It will be interesting to see how these can be measured with less intrusive user interfaces or—in the best case—with indirect physiological or psychological measures. If such methods would be established, it would also be possible to further investigate how far these higher level constructs are affected by specific aspects of audio quality.</p><p class="Para" id="Par93">Sections <span class="InternalRef"><a href="#Sec12">5.3</a></span> and <span class="InternalRef"><a href="#Sec23">5.4</a></span> introduced selected approaches for generating 3D audio for AR/VR and reviewed their quality. The current best practice of using non-individual binaural synthesis with compensated headphones for audio reproduction can generate plausible simulations and can significantly benefit from additional information provided by 3D visual content. Recent advances in signal processing fostered the combination of SFS and binaural reproduction. This improved the efficiency—a key factor for enabling 3D audio rendering in mobile applications—without introducing significant quality degradations. One current hot topic in the combination of SFS and binaural reproduction is clearly 6DoF rendering. Many algorithms were suggested for this, however, their development and even more so their perceptual evaluation are still under investigation in the majority of cases. The interested reader may have a look at recent articles as a starting point for discovering this field (e.g., [<span class="CitationRef"><a epub:type="biblioref" href="#CR4" role="doc-biblioref">4</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR41" role="doc-biblioref">41</a></span>]). A second hot topic is the individualization of binaural technology. The effects of individualization were discussed and it was shown that this makes it possible to create simulations that are perceptually identical to a real sound field. Approaches for individualization were, however, not detailed and the interested reader is referred to the overview of Guezenoc and Renaud [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>].</p><p class="Para" id="Par94">From the user perspective, it is worth to note that an increasing pool of software and hardware is available for 3D audio reproduction.<sup><a epub:type="noteref" href="#Fn3" id="Fn3_source" role="doc-noteref">3</a></sup>
<span id="ITerm64"/> State-of-the-art audio processing and reproduction methods are available as plug-ins that can easily be integrated into the production workflow as well as in toolboxes that can be used for further research and product development. This is complemented by VR/AR-ready hardware such as microphone arrays as well as head-mounted displays and headphones with build-in head trackers.</p></section><div class="License LicenseSubType-cc-by"><a href="https://creativecommons.org/licenses/by/4.0"><img alt="Creative Commons" src="../css/cc-by.png"/></a><p class="SimplePara"><strong class="EmphasisTypeBold ">Open Access</strong> This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (<span class="ExternalRef"><a href="http://creativecommons.org/licenses/by/4.0/"><span class="RefSource">http://​creativecommons.​org/​licenses/​by/​4.​0/​</span></a></span>), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p><p class="SimplePara">The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p></div><aside aria-labelledby="Bib1Heading" class="Bibliography" id="Bib1"><div epub:type="bibliography" role="doc-bibliography"><div class="Heading" id="Bib1Heading">References</div><ol class="BibliographyWrapper"><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">1.</div><div class="CitationContent" id="CR1">Ahrens, J.: Analytic methods of sound field synthesis 1st Edition (eds Möller, S., Küpper, A., Raake, A.) (Springer, Heidelberg, Germany, 2012).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">2.</div><div class="CitationContent" id="CR2">Amengual Garí, S. V., Arend, J. M., Calamia, P. T., Robinson, P. W.: Optimizations of the Spatial Decomposition Method for Binaural Reproduction. J. Audio Eng. Soc. <strong class="EmphasisTypeBold ">68</strong>, 959–976 (Dec. 2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">3.</div><div class="CitationContent" id="CR3">Arend, J. M., Brinkmann, F., Pörschmann, C.: Assessing Spherical Harmonics Interpolation of Time-Aligned Head-Related Transfer Functions. J. Audio Eng. Soc. <strong class="EmphasisTypeBold ">69</strong>, 104–117 (Feb. 2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">4.</div><div class="CitationContent" id="CR4">Arend, J. M., Garí, S. V. A., Schissler, C., Klein, F., Robinson, P. W.: Six- Degrees-of-Freedom Parametric SpatialAudio Based on One MonauralRoom Impulse Response. Journal of the Audio Engineering Society <strong class="EmphasisTypeBold ">69</strong>, 557–575 (July 2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">5.</div><div class="CitationContent" id="CR5">Athif, M. et al.: Using Biosignals for Objective Measurement of Presence in Virtual Reality Environments in 2020 42nd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC) (2020), 3035–3039.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">6.</div><div class="CitationContent" id="CR6">Bech, S. N. Z.: Perceptual audio evaluation. Theroy, method and application (John Wiley &amp; Sons, West Sussex, England, 2006).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">7.</div><div class="CitationContent" id="CR7">Blauert, J.: Spatial Hearing. The psychophysics of human sound localization Revised (MIT Press, Cambridge, Massachusetts, 1997).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">8.</div><div class="CitationContent" id="CR8">Brinkmann, F., Lindau, A., Weinzierl, S.: On the authenticity of individual dynamic binaural synthesis. J.Acoust. Soc. Am. <strong class="EmphasisTypeBold ">142</strong>, 1784–1795 (Oct. 2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">9.</div><div class="CitationContent" id="CR9">Brinkmann, F. et al.: A cross-evaluated database of measured and simulated HRTFs including 3D head meshes, anthropometric features, and headphone impulse responses. J. Audio Eng. Soc. <strong class="EmphasisTypeBold ">67</strong>, 705–718 (Sept. 2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">10.</div><div class="CitationContent" id="CR10">Brinkmann, F. et al.: A round robin on room acoustical simulation and auralization. J. Acoust. Soc. Am. <strong class="EmphasisTypeBold ">145</strong>, 2746–2760 (Apr. 2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">11.</div><div class="CitationContent" id="CR11">Brunnström, K. et al.: Qualinet white paper on definitions of quality of experience in 5th Qualinet meeting (Novi Sad, Serbia, 2013).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">12.</div><div class="CitationContent" id="CR12">Burstein, H.: Approximation formulas for error risk and sample size in abx testing. Journal of the Audio Engineering Society <strong class="EmphasisTypeBold ">36</strong>, 879–883 (1988).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">13.</div><div class="CitationContent" id="CR13">Deniaud, C., Honnet, V., Jeanne, B., Mestre, D.: An investigation into physiological responses in driving simulators: An objective measurement of presence in 2015 Science and Information Conference (SAI) (2015), 739–748.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">14.</div><div class="CitationContent" id="CR14">Dey, A., Phoon, J., Saha, S., Dobbins, C., Billinghurst, M.: Neurophysiological Effects of Presence in Calm Virtual Environments in 2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW) (2020), 744–745.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">15.</div><div class="CitationContent" id="CR15">Engel, I., Alon, D. L., Robinson, P. W., Mehra, R.: The Effect of Generic Headphone Compensation on Binaural Renderings in AES International Conference on Immersive and Interactive Audio (Audio Engineering Society, York, UK, Mar. 2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">16.</div><div class="CitationContent" id="CR16">Engel, I., Henry, C., Amengual Garí, S. V., Robinson, P. W., Picinali, L.: Perceptual implications of different Ambisonics-based methods for binaural reverberation. J. Acoust. Soc. Am. <strong class="EmphasisTypeBold ">149</strong>, 895–910 (Feb. 2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">17.</div><div class="CitationContent" id="CR17">Erbes,V.:Wave field synthesis in a listening room Doctoral Thesis (University of Rostock, Rostock, Germany, Aug. 2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">18.</div><div class="CitationContent" id="CR18">Erbes, V., Schultz, F., Lindau, A., Weinzierl, S.: An extraaural headphone system for optimized binaural reproduction in Fortschritte der Akustik -DAGA 2012 (Darmstadt, Germany, Mar. 2012), 313–314.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">19.</div><div class="CitationContent" id="CR19">Freeman, J., Avons, S. E., Meddis, R., Pearson, D. E., IJsselsteijn, W.: Using behavioral realism to estimate presence: A study of the utility of postural responses to motion stimuli. Presence: Teleoperators &amp;Virtual Environments <strong class="EmphasisTypeBold ">9</strong>, 149–164 (2000).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">20.</div><div class="CitationContent" id="CR20">Gálvez, M. F. S., Menzies, D., Fazi, F. M.: Dynamic Audio Reproduction with Linear Loudspeaker Arrays. J. Audio Eng. Soc. <strong class="EmphasisTypeBold ">67</strong>, 190–200 (Apr. 2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">21.</div><div class="CitationContent" id="CR21">Gelfand, S. A.: Hearing: An introduction to psychological and physiological acoustics (CRC Press, 2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">22.</div><div class="CitationContent" id="CR22">Genzel, D., Schutte, M., Brimijoin, W. O., MacNeilage, P. R., Wiegrebe, L.: Psychophysical evidence for auditory motion parallax. Proceedings of the National Academy of Sciences of the United States of America <strong class="EmphasisTypeBold ">115</strong>, 4264–4269 (Apr. 2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">23.</div><div class="CitationContent" id="CR23">Gil-Carvajal, J. C., Cubick, J., Santurette, S., Dau, T.: Spatial Hearing with Incongruent Visual or Auditory Room Cues. Scientific Reports <strong class="EmphasisTypeBold ">6</strong>, 37342 EP (Nov. 2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">24.</div><div class="CitationContent" id="CR24">Gomez-Bolaños, J., Mäkivirta, A., Pulkki, V.: Automatic regularization parameter for headphone transfer function inversion. J. Audio Eng. Soc. <strong class="EmphasisTypeBold ">64</strong>, 752–761 (Oct. 2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">25.</div><div class="CitationContent" id="CR25">Guezenoc, C., Séguier, R.: HRTF Individualization: A Survey in 145th AES Convention (New York, NY, USA, Oct. 2018), Paper 10129.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">26.</div><div class="CitationContent" id="CR26">Gupta, R., Ranjan, R., He, J., Gan, W.-S.: Study on differences between individualized and non-indiviudalized hear-thourough equalization for natural CA, USA, Aug. 2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">27.</div><div class="CitationContent" id="CR27">Gupta, R., Ranjan, R., He, J., Woon-Seng, G.: Investigation of effect of VR/AR headgear on Head related transfer functions for natural listening in Audio Engineering Society Conference: 2018 AES International Conference on Audio for Virtual and Augmented Reality (2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">28.</div><div class="CitationContent" id="CR28">Halbig, A., Latoschik, M. E.: A Systematic Review of Physiological Measurements, Factors, Methods, and Applications in Virtual Reality. Frontiers in Virtual Reality <strong class="EmphasisTypeBold ">2</strong>, 89 (2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">29.</div><div class="CitationContent" id="CR29">Hendrickx, E., Paquier, M., Koehl, V., Palacino, J.: Ventriloquism effect with sound stimuli varying in both azimuth and elevation. J. Acoust. Soc. Am. <strong class="EmphasisTypeBold ">138</strong>, 3686–3697 (2015).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">30.</div><div class="CitationContent" id="CR30">Hendrickx, E. et al.: Improvement of Externalization by Listener and Source Movement Using a “Binauralized” Microphone Array. J. Audio Eng. Soc. <strong class="EmphasisTypeBold ">65</strong>, 589–599 (July 2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">31.</div><div class="CitationContent" id="CR31">Hendrickx, E. et al.: Influence of head tracking on the externalization of speech stimuli for non-individualized binaural synthesis. J. Acoust. Soc. Am. <strong class="EmphasisTypeBold ">141</strong>, 2011–2023 (Mar. 2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">32.</div><div class="CitationContent" id="CR32">Hiekkanen, T., Mäkivirta, A., Karjalainen, M.: Virtualized listening tests for loudspeakers. J. Audio Eng. Soc. <strong class="EmphasisTypeBold ">57</strong>, 237–251 (2009).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">33.</div><div class="CitationContent" id="CR33">Hox, J. J.: Multilevel Analysis. Techniques and Apllications Second (ed Marcoulides, G. A.) (Routledge, New York, Hove, 2010).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">34.</div><div class="CitationContent" id="CR34">ITU-R BS.1116-3: Methods for the subjective assessment of small impairments in audio systems (ITU, Geneva, Switzerland, 2015).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">35.</div><div class="CitationContent" id="CR35">ITU-R BS.1283-2: Guidance for the selection of the most appropriate ITU-R Recommendation(s) for subjective assessment of sound quality (ITU, Geneva, Switzerland, 2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">36.</div><div class="CitationContent" id="CR36">ITU-R BS.1284-2: General methods for the subjective assessment of sound quality (ITU, Geneva, Switzerland, 2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">37.</div><div class="CitationContent" id="CR37">ITU-R BS.1534-3: Methods for the subjective assessment of intermediate quality level of audio systems (ITU, Geneva, Switzerland, 2015).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">38.</div><div class="CitationContent" id="CR38">Jekosch, U.: Basic Concepts and Terms of. acta acustica united with Acustica <strong class="EmphasisTypeBold ">90</strong>, 999–1006 (2004).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">39.</div><div class="CitationContent" id="CR39">Jerald, J., Whitton, M.: Relating Scene-Motion Thresholds to Latency Thresholds for Head-Mounted Displays in 2009 IEEE Virtual Reality Conference (Mar. 2009), 211–218.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">40.</div><div class="CitationContent" id="CR40">Kadlec, H.: Statistical properties of <span class="InlineEquation" id="IEq22"><img alt="$$d^{^{\prime }}$$" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Chapter_TeX_IEq22.png" style="width:0.94em"/></span> and <span class="InlineEquation" id="IEq23"><img alt="$$\beta $$" src="../images/478239_1_En_5_Chapter/478239_1_En_5_Chapter_TeX_IEq23.png" style="width:0.94em"/></span> estimates of signal detection theory. Psychological Methods <strong class="EmphasisTypeBold ">4</strong>, 22 (1999).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">41.</div><div class="CitationContent" id="CR41">Kentgens, M., Jax, P.: Comparison of Methods for Plausible Sound Field Translation in Fortschritte der Akustik - DAGA 2021 (Vienna, Austria, Aug. 2021), 302–305.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">42.</div><div class="CitationContent" id="CR42">Le Bagousse, S., Colomes, C., Paquier, M.: State of the art on subjective assessment of spatial sound quality inAudio Engineering Society Conference: 38th International Conference: Sound Quality Evaluation (2010).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">43.</div><div class="CitationContent" id="CR43">Le Bagousse, S., Paquier, M., Colomes, C.: Families of sound attributes for assessment of spatial audio in 129th AES Convention (2010), Convention-Paper.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">44.</div><div class="CitationContent" id="CR44">Leventhal, L.: Type 1 and type 2 errors in the statistical analysis of listening tests. Journal of the Audio Engineering Society <strong class="EmphasisTypeBold ">34</strong>, 437–453 (1986).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">45.</div><div class="CitationContent" id="CR45">Lindau, A.: The perception of system latency in dynamic binaural synthesis in NAG/DAGA 2009, International Conference on Acoustics (Rotterdam, Netherland, 2009), 1063–1066.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">46.</div><div class="CitationContent" id="CR46">Lindau, A., Weinzierl, S.: On the spatial resolution of virtual acoustic environments for head movements on horizontal, vertical and lateral direction in EAA Symposium on Auralization (Espoo, Finland, June 2009).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">47.</div><div class="CitationContent" id="CR47">Lindau, A., Weinzierl, S.: Assessing the plausibility of virtual acoustic environments. Acta Acust. united Ac. <strong class="EmphasisTypeBold ">98</strong>, 804–810 (Sept. 2012).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">48.</div><div class="CitationContent" id="CR48">Lindau, A. et al.: A Spatial Audio Quality Inventory (SAQI). Acta Acust. united Ac. <strong class="EmphasisTypeBold ">100</strong>, 984–994 (Sept. 2014).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">49.</div><div class="CitationContent" id="CR49">Lübeck, T., Helmholz, H., Arend, J. M., Pörschmann, C., Ahrens, J.: Perceptual Evaluation of Mitigation Approaches of Impairments due to Spatial Undersampling in Binaural Rendering of Spherical Microphone Array Data. J. Audio Eng. Soc. <strong class="EmphasisTypeBold ">68</strong>, 428–440 (June 2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">50.</div><div class="CitationContent" id="CR50">Lübeck, T., Pörschmann, C., Arend, J. M.: Perception of direct sound, early reflections, andreverberation in auralizations of sparsely measuredbinaural room impulse responses in AES Int. Conf. Audio for Virtual and Augmented Reality (AVAR) (Aug. 2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">51.</div><div class="CitationContent" id="CR51">Majdak, P., Masiero, B., Fels, J.: Sound localization in individualized and non-individualized crosstalk cancellation systems. J. Acoust. Soc. Am. <strong class="EmphasisTypeBold ">133</strong>, 2055–2068 (Apr. 2013).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">52.</div><div class="CitationContent" id="CR52">McAnally, K. I., Martin, R. L.: Sound localization with head movement: implications for 3-d audio displays. Frontiers in Neuroscience <strong class="EmphasisTypeBold ">8</strong>, 210 (2014).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">53.</div><div class="CitationContent" id="CR53">McCormack, L., Pulkki, V., Politis, A., Scheuregger, O., Marschall, M.: Higher-Order Spatial Impulse Response Rendering: Investigating the Perceived Effects of Spherical Order, Dedicated Diffuse Rendering, and Frequency Resolution. J. Audio Eng. Soc. <strong class="EmphasisTypeBold ">68</strong>, 338–354 (May 2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">54.</div><div class="CitationContent" id="CR54">Meehan, M., Insko, B., Whitton, M., Brooks Jr, F. P.: Physiological measures of presence in stressful virtual environments. Acm transactions on graphics (tog) <strong class="EmphasisTypeBold ">21</strong>, 645–652 (2002).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">55.</div><div class="CitationContent" id="CR55">Mendonça, C., Delikaris-Manias, S.: Statistical Tests with MUSHRA Data in 144th AES Convention (Milan, Italy, May 2018), Paper 10006.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">56.</div><div class="CitationContent" id="CR56">Mendonça, C., Mandelli, P., Pulkki, V.: Modeling the perception of audiovisual distance: Bayesian causal inference and other models. PLoS ONE <strong class="EmphasisTypeBold ">11</strong>, e0165391 (2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">57.</div><div class="CitationContent" id="CR57">Middlebrooks, J. C.: Virtual localization improved by scaling nonindividualized external-ear transfer functions in frequency. J. Acoust. Soc. Am. <strong class="EmphasisTypeBold ">106</strong>, 1493–1510 (Sept. 1999).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">58.</div><div class="CitationContent" id="CR58">Minsky, M.: Telepresence. Omni, 45–51 (1980).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">59.</div><div class="CitationContent" id="CR59">Moore, A. H., Tew, A. I., Nicol, R.: An initial validation of individualised crosstalk cancellation filters for binaural perceptual experiments. J. Audio Eng. Soc. <strong class="EmphasisTypeBold ">58</strong>, 36–45 (Jan. 2010).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">60.</div><div class="CitationContent" id="CR60">Noble, A. C. et al.: Modification of a standardized system of wine aroma terminology. American journal of Enology and Viticulture <strong class="EmphasisTypeBold ">38</strong>, 143–146 (1987).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">61.</div><div class="CitationContent" id="CR61">Paquier, M., Koehl, V.: Discriminability of the placement of supra-aural and circumaural headphones. Applied Accoustics <strong class="EmphasisTypeBold ">93</strong>, 130–139 (2015).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">62.</div><div class="CitationContent" id="CR62">Pedersen, T. H., Zacharov, N.: The development of a sound wheel for reproduced sound in Audio Engineering Society Convention 138 (2015).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">63.</div><div class="CitationContent" id="CR63">Pike, C., Melchior, F.,Tew,T.: Assessing the plausibility of non-individualised dynamic binaural synthesis in a small room in AES 55th International Conference (Helsinki, Finland, 2014).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">64.</div><div class="CitationContent" id="CR64">Parametric time-frequency domain spatial audio First (eds Pulkki, V., Delikaris-Manias, S., Politis, A.) (Wiley, Hoboken, NJ, USA, 2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">65.</div><div class="CitationContent" id="CR65">Raake, A., Rummukainen, O. S., Habets, E. A. P., Robotham, T., Singla, A.: QoEvaVE - QoE Evaluation of Interactive Virtual Environments with Audiovisual Scenes in Fortschritte der Akustik - DAGA 2021 (Vienna, Austria, Aug. 2021), 1332–1335.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">66.</div><div class="CitationContent" id="CR66">Riva, G., Waterworth, J. A., Waterworth, E. L.: The layers of presence: a bio-cultural approach to understanding presence in natural and mediated environments. CyberPsychology &amp; Behavior <strong class="EmphasisTypeBold ">7</strong>, 402–416 (2004).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">67.</div><div class="CitationContent" id="CR67">Rummukainen, O. et al.: Audio Quality evaluation in virtual reality: Multiple stimulus ranking with behaviour tracking in AES Int. Conf. on Audio for Virtual and Augmented Reality (AVAR) (Redmond, USA, Aug. 2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">68.</div><div class="CitationContent" id="CR68">Sanchez-Vives, M. V., Slater, M.: From presence to consciousness through virtual reality. Nature Reviews Neuroscience <strong class="EmphasisTypeBold ">6</strong>, 332–339 (2005).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">69.</div><div class="CitationContent" id="CR69">Schlenstedt, G., Brinkmann, F., Pelzer, S., Weinzierl, S.: Perceptual evaluation of transaural binaural synthesis under consideration of the playback room [German: Perzeptive Evaluation transauraler Binauralsynthese unter Berücksichtigung des Wiedergaberaums] in Fortschritte der Akustik - DAGA 2016 (Aachen, Germany, Mar. 2016), 561–564.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">70.</div><div class="CitationContent" id="CR70">Schoeffler, M., Herre, J.: About the different types of listeners for rating the overall listening experience in Proceedings of the ICMC|SMC (Athens, Greece, 2014), 886–892.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">71.</div><div class="CitationContent" id="CR71">Schoeffler, M., Silzle, A., Herre, J.: Evaluation of spatial/3D audio: Basic audio quality versus quality of experience. IEEE Journal of Selected Topics in Signal Processing <strong class="EmphasisTypeBold ">11</strong>, 75–88 (2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">72.</div><div class="CitationContent" id="CR72">Schwind, V., Knierim, P., Haas, N., Henze, N.: Using presence questionnaires in virtual reality in Proceedings of the 2019 CHI conference on human factors in computing systems (2019), 1–12.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">73.</div><div class="CitationContent" id="CR73">Silzle, A.: Quality taxonomies for auditory virtual environments in Audio Engineering Society Convention 122 (2007).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">74.</div><div class="CitationContent" id="CR74">Slater, M.: Measuring presence: A response to the Witmer and Singer presence questionnaire. Presence <strong class="EmphasisTypeBold ">8</strong>, 560–565 (1999).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">75.</div><div class="CitationContent" id="CR75">Slater, M.: Place illusion and plausibility can lead to realistic behaviour in immersive virtual environments. Phil. Trans. R. Soc. B <strong class="EmphasisTypeBold ">364</strong>, 3549–3557 (2009).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">76.</div><div class="CitationContent" id="CR76">Slater, M., Brogni, A., Steed, A.: Physiological responses to breaks in presence: A pilot study in Presence 2003: The 6th annual international workshop on presence <strong class="EmphasisTypeBold ">157</strong> (2003).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">77.</div><div class="CitationContent" id="CR77">Slater, M., Wilbur, S.: A framework for immersive virtual environments (FIVE): Speculations on the role of presence in virtual environments. Presence: Teleoperators &amp; Virtual Environments <strong class="EmphasisTypeBold ">6</strong>, 603–616 (1997).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">78.</div><div class="CitationContent" id="CR78">Tervo, S., Pätynen, J., Kuusinen, A., Lokki, T.: Spatial decomposition method for room impulse responses. J. Audio Eng. Soc. <strong class="EmphasisTypeBold ">61</strong>, 17–28 (Jan. 2013).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">79.</div><div class="CitationContent" id="CR79">Välimäki,V., Parker, J., Savioja, L., Smith, J. O.,Abel, J.: More Than 50Years of ArtificialReverberation in 60th Int. AES Conf.DREAMS(Dereverberation and Reverberation of Audio, Music, and Speech) (Leuven, Belgium, Feb. 2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">80.</div><div class="CitationContent" id="CR80">Välimäki, V., Parker, J. D., Savioja, L., Smith, J. O., Abel, J. S.: Fifty Years of Artificial Reverberation. IEEE Transactions on Audio, Speech, and Language Processing <strong class="EmphasisTypeBold ">20</strong>, 1421–1448 (July 2012).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">81.</div><div class="CitationContent" id="CR81">Völker, C., Bisitz, T., Huber, R., Kollmeier, B., Ernst, S. M. A.: Modifications of the MUlti stimulus test with Hidden Reference and Anchor (MUSHRA) for use in audiology. Int. J. Audiology (2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">82.</div><div class="CitationContent" id="CR82">Wefers, F.: Partitioned convolution algorithms for real-time auralization PhD thesis (RWTH Aachen University, Aachen, Germany, Sept. 2014).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">83.</div><div class="CitationContent" id="CR83">Werner, S., Klein, F., Mayenfels, T., Brandenburg, K.: Asummary on acoustic room divergence and its effect on externalization of auditory events in 8th Int. Conf. Quality of Multimedia Experience (QoMEX) (Lisbon, Portugal, June 2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">84.</div><div class="CitationContent" id="CR84">Wickens, T. D.: Elementary Signal Detection Theory (Oxford University Press, Oxford et al., 2002).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">85.</div><div class="CitationContent" id="CR85">Wierstorf, H.: Perceptual assessment of sound field synthesis Doctoral Thesis (Technical University of Berlin, Berlin, Germany, Sept. 2014).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">86.</div><div class="CitationContent" id="CR86">Witmer, B. G., Jerome, C. J., Singer, M. J.: The factor structure of the presence questionnaire. Presence: Teleoperators &amp; Virtual Environments <strong class="EmphasisTypeBold ">14</strong>, 298–312 (2005).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">87.</div><div class="CitationContent" id="CR87">Witmer, B. G., Singer, M. J.: Measuring presence in virtual environments: A presence questionnaire. Presence <strong class="EmphasisTypeBold ">7</strong>, 225–240 (1998).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">88.</div><div class="CitationContent" id="CR88">Xie, B.: Head-related transfer function and virtual auditory display Second (J. Ross Publishing, Plantation, FL, USA, 2013).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">89.</div><div class="CitationContent" id="CR89">Zotter, F., Frank, M.: Ambisonics. A practical 3D audio theroy for recording, studio production, sound reinforcement, and virtual reality (Springer Open, Cham, Switzerland, 2019).</div></li></ol></div></aside><aside aria-label="Footnotes" class="FootnoteSection" epub:type="footnotes"><div class="Heading">Footnotes</div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn1_source">1</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn1" role="doc-footnote"><p class="Para" id="Par21"><span class="ExternalRef"><a href="https://ispr.info"><span class="RefSource">https://​ispr.​info</span></a></span> (last access 2022/06/17).</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn2_source">2</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn2" role="doc-footnote"><p class="Para" id="Par30">Currently maintained under <span class="ExternalRef"><a href="https://forcetechnology.com/en/articles/gated-content-senselab-sound-wheel"><span class="RefSource">https://​forcetechnology.​com/​en/​articles/​gated-content-senselab-sound-wheel</span></a></span> (last access 2022/06/17).</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn3_source">3</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn3" role="doc-footnote"><p class="Para" id="Par95">A list of available tools can, for example, be found at <span class="ExternalRef"><a href="https://www.audio-technology.info/"><span class="RefSource">https://​www.​audio-technology.​info/​</span></a></span> under the <em class="EmphasisTypeItalic ">Resources</em> section of the <em class="EmphasisTypeItalic ">Binaural Technology</em> chapter (last access 2022/06/17).</p></div><div class="ClearBoth"> </div></div></aside></div></div></body></html>