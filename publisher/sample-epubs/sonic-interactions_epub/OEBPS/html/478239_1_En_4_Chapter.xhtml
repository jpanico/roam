<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops"><head><title>System-to-User and User-to-System Adaptations in Binaural Audio</title><meta content="text/html; charset=utf-8" http-equiv="content-type"/><link href="../css/springer_epub.css" rel="styleSheet" type="text/css"/></head><body><div epub:type="chapter" role="doc-chapter"><div class="ChapterContextInformation"><div class="ContextInformation" id="Chap4"><div class="ChapterCopyright">© The Author(s) 2023</div><span class="ContextInformationAuthorEditorNames">M. Geronazzo, S. Serafin<span class="CollaboratorDesignation"> (eds.)</span></span><span class="ContextInformationBookTitles"><span class="BookTitle">Sonic Interactions in Virtual Environments</span></span><span class="ContextInformationSeries"><span class="SeriesTitle" lang="en">Human–Computer Interaction Series</span></span><span class="ChapterDOI"><a href="https://doi.org/10.1007/978-3-031-04021-4_4">https://doi.org/10.1007/978-3-031-04021-4_4</a></span></div></div><!--Begin Abstract--><div class="MainTitleSection"><h1 class="ChapterTitle" lang="en">4. System-to-User and User-to-System Adaptations in Binaural Audio</h1></div><div class="AuthorGroup"><div class="AuthorNames"><span class="Author"><span class="AuthorName">Lorenzo Picinali</span><sup><a href="#Aff34">1</a> <a aria-label="Contact information for this author" href="#ContactOfAuthor1"><span class="ContactIcon"> </span></a></sup> and </span><span class="Author"><span class="AuthorName">Brian F. G. Katz</span><sup><a href="#Aff35">2</a> <a aria-label="Contact information for this author" href="#ContactOfAuthor2"><span class="ContactIcon"> </span></a></sup></span></div><div class="Affiliations"><div class="Affiliation" id="Aff34"><span class="AffiliationNumber">(1)</span><div class="AffiliationText">Imperial College London, South Kensington Campus, London, SW7 2AZ, UK</div></div><div class="Affiliation" id="Aff35"><span class="AffiliationNumber">(2)</span><div class="AffiliationText">Sorbonne Université, CNRS, UMR 7190, Institut Jean Le Rond d’Alembert, Lutheries - Acoustique - Musique, Paris, France</div></div><div class="ClearBoth"> </div></div><div class="Contacts"><div class="Contact" id="ContactOfAuthor1"><div class="ContactIcon"> </div><div class="ContactAuthorLine"><span class="AuthorName">Lorenzo Picinali</span> (Corresponding author)</div><div class="ContactAdditionalLine"><span class="ContactType">Email: </span><a href="mailto:l.picinali@imperial.ac.uk">l.picinali@imperial.ac.uk</a></div></div><div class="Contact" id="ContactOfAuthor2"><div class="ContactIcon"> </div><div class="ContactAuthorLine"><span class="AuthorName">Brian F. G. Katz</span></div><div class="ContactAdditionalLine"><span class="ContactType">Email: </span><a href="mailto:brian.katz@sorbonne-universite.fr">brian.katz@sorbonne-universite.fr</a></div></div></div></div><section class="Abstract" id="Abs1" lang="en" role="doc-abstract"><h2 class="Heading">Abstract</h2><p class="Para" id="Par1">This chapter concerns concepts of adaption in a binaural audio context (i.e. headphone-based three-dimensional audio rendering and associated spatial hearing aspects), considering first the adaptation of the rendering system to the acoustic and perceptual properties of the user, and second the adaptation of the user to the rendering quality of the system. We start with an overview of the basic mechanisms of human sound source localisation, introducing expressions such as localisation cues and interaural differences, and the concept of the Head-Related Transfer Function (HRTF), which is the basis of most 3D spatialisation systems in VR. The chapter then moves to more complex concepts and processes, such as HRTF selection (system-to-user adaptation) and HRTF accommodation (user-to-system adaptation). State-of-the-art HRTF modelling and selection methods are presented, looking at various approaches and at how these have been evaluated. Similarly, the process of HRTF accommodation is detailed, with a case study employed as an example. Finally, the potential of these two approaches are discussed, considering their combined use in a practical context, as well as introducing a few open challenges for future research.</p></section><!--End Abstract--><div class="Fulltext"><section class="Section1 RenderAsSection1" id="Sec1"><h2 class="Heading"><span class="HeadingNumber">4.1 </span>Introduction</h2><p class="Para" id="Par2">Binaural technology is the solution for sound <span id="ITerm1">spatialisation</span> which is the closest to real-life listening. It attempts to mimic the entirety of acoustic cues associated with the human localisation of sounds, reproducing the corresponding acoustic pressure signal at the entrance of the two ear canals of the listener (binaural literally means “related to two ears”). These two signals should be a complete and sufficient representation of the sound scene, since they are the only information that the auditory system requires in order to identify the 3D location of a sound source. Thus, <span id="ITerm2">binaural rendering</span>of spatial information is fundamentally based on the production (either through recording or synthesis) of localisation cues that are the consequence of the incident sound upon the listener’s torso, head, and ears on the way to the ear canal, and subsequently to the eardrums. These cues are, namely, the ITD (interaural time difference), <span id="ITerm3">the</span> ILD (interaural level difference) <span id="ITerm4">and</span> <span id="ITerm5">spectral</span> cues [<span class="CitationRef"><a epub:type="biblioref" href="#CR48" role="doc-biblioref">48</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR68" role="doc-biblioref">68</a></span>]. Their combined effects are represented by the Head-Related Transfer Function (HRTF)<span id="ITerm6"/>, which characterises the spectro-temporal filtering of a locus of source positions around a given head.<sup><a epub:type="noteref" href="#Fn1" id="Fn1_source" role="doc-noteref">1</a></sup>
</p><section class="Section2 RenderAsSection2" id="Sec2"><h3 class="Heading"><span class="HeadingNumber">4.1.1 </span>Localisation Cues and Their Individual Nature</h3><div class="Para" id="Par5">The ILD and ITD as a function of source position are determined principally by the size and shape of the head, as well as the position of the ears on the two sides. In order to better understand these localisation cues, Fig. <span class="InternalRef"><a href="#Fig1">4.1</a></span> shows how ITD and ILD vary as a function of both distance (1.5–10 m) and azimuth. This comparison highlights potential effects of ITD/ILD mismatch, especially if they occur near the interaural axis where they can affect distance perception. The results were obtained by Boundary Element Method (BEM) simulation of the HRTF using the open-source <span class="EmphasisFontCategoryNonProportional ">mesh2hrtf</span> software [<span class="CitationRef"><a epub:type="biblioref" href="#CR110" role="doc-biblioref">110</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR111" role="doc-biblioref">111</a></span>]. The mesh employed was obtained from an MRI scan of a Neumann dummy recording head (model KU-100)<span id="ITerm7"/>, previously used in HRTF computation [<span class="CitationRef"><a epub:type="biblioref" href="#CR32" role="doc-biblioref">32</a></span>] and measurement [<span class="CitationRef"><a epub:type="biblioref" href="#CR4" role="doc-biblioref">4</a></span>] comparisons. These cues vary as a function of frequency. For this example, the ITD was calculated using the <em class="EmphasisTypeItalic ">Threshold lp –30 dB</em> method (for a summary of various ITD estimation methods see [<span class="CitationRef"><a epub:type="biblioref" href="#CR50" role="doc-biblioref">50</a></span>]), which detects the first onset using a –30 dB relative threshold on a 3 kHz low-pass filtered version of the HRIR, as this has been shown to be the most perceptually relevant method for ITD estimation among 32 different estimation methods and variants [<span class="CitationRef"><a epub:type="biblioref" href="#CR7" role="doc-biblioref">7</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR50" role="doc-biblioref">50</a></span>]. The ILD was calculated as the difference of left and right HRIR RMS values, after applying a 3kHz high-pass filter. The use of low-pass and high-pass filters for the two different acoustic cues is based on previous studies showing the frequency dependence of the different auditory cues [<span class="CitationRef"><a epub:type="biblioref" href="#CR101" role="doc-biblioref">101</a></span>], with ITD being dominated by low-frequency content (with interpretation of phase information being inconclusive for frequencies smaller than head dimensions) and ILD varying more significantly with high-frequency content (where the wavelength is less than the dimensions of the head). The application of a 2–3 kHz filter can be used to generally separate the contributions of the pinnae in the HRIR [<span class="CitationRef"><a epub:type="biblioref" href="#CR50" role="doc-biblioref">50</a></span>]. One can observe that ITD varies little over the simulated distance range, while becoming more vague and ambiguous near the interaural axis. In contrast, the ILD varies with distance in the same interaural axis range of 70<span class="InlineEquation" id="IEq1"><img alt="$$^\circ $$" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Chapter_TeX_IEq1.png" style="width:0.57em"/></span>–110<span class="InlineEquation" id="IEq2"><img alt="$$^\circ $$" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Chapter_TeX_IEq2.png" style="width:0.57em"/></span>.<figure class="Figure" id="Fig1"><div class="MediaObject" id="MO1"><img alt="" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Fig1_HTML.png" style="width:33.08em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 4.1</span><p class="SimplePara">Isocontours for ITD (left) and ILD (right) as a function of azimuth (in degrees) and radial distance (from 1.5 to 10 m) obtained via numerical simulation of the HRTF of a dummy head (not shown to scale). ITD (3 kHz low-pass Head-Related Impulse Response—HRIR, <em class="EmphasisTypeItalic ">Threshold</em>, –30 dB first onset method) 50 <span class="InlineEquation" id="IEq3"><img alt="$$\upmu $$" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Chapter_TeX_IEq3.png" style="width:0.75em"/></span>s contours. ILD (3 kHz high-pass HRIR, RMS difference) 1 dB contours</p><div class="Credit"><p class="SimplePara">(from [<span class="CitationRef"><a epub:type="biblioref" href="#CR48" role="doc-biblioref">48</a></span>])</p></div></div></figcaption></figure></div><p class="Para" id="Par6">Other physical interactions between the sound wave and the torso, head, and pinnae (the external parts of the ear) introduce a range of spectral cues (principally through series of peaks and notches) which can be used to judge whether a sound source is e.g. above or below, to the front or rear of the listener, while ITD and ILD remain relatively unchanged. Considering the various morphological regions of the pinnae, as indicated later in Sect. <span class="InternalRef"><a href="#Sec5">4.2.1</a></span>—Fig. <span class="InternalRef"><a href="#Fig2">4.2</a></span>a, each of these is potentially related to specific characteristic of the HRTF filters. As such, individual morphological variations will result in different HRTFs. When reproducing binaural audio, it has been experimentally demonstrated that using an HRTF that does not match the one of the listener has a detrimental effect on the accuracy and realism of virtual sound perception. For example, it has been noted that listeners are able to localise virtual sounds that have been spatialized using their own HRTFs with a similar accuracy to free field listening, though some studies have shown poorer elevation judgements and increased front-back confusions [<span class="CitationRef"><a epub:type="biblioref" href="#CR67" role="doc-biblioref">67</a></span>], which may be due to the idealised anechoic nature of HRTFs and the importance of slight head movements and associated dynamic cues [<span class="CitationRef"><a epub:type="biblioref" href="#CR37" role="doc-biblioref">37</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR102" role="doc-biblioref">102</a></span>]. These errors can significantly increase when using someone else’s HRTF [<span class="CitationRef"><a epub:type="biblioref" href="#CR99" role="doc-biblioref">99</a></span>]. Furthermore, using non-individual HRTFs (see Sect. <span class="InternalRef"><a href="#Sec3">4.1.2</a></span>) has been shown to affect various perceptual attributes when considering <span id="ITerm8">complex scenes</span>, in addition to those associated with source localisation: i.e. Coloration, Externalisation, Immersion, Realism and Relief/Depth [<span class="CitationRef"><a epub:type="biblioref" href="#CR87" role="doc-biblioref">87</a></span>]. In this chapter, the primary focus is on <span id="ITerm9">localisationas</span> the perceptual evaluation metric. Chapter <span class="ExternalRef"><a href="478239_1_En_5_Chapter.xhtml"><span class="RefSource">5</span></a></span> introduces and discusses other relevant metrics.</p></section><section class="Section2 RenderAsSection2" id="Sec3"><h3 class="Heading"><span class="HeadingNumber">4.1.2 </span>Minimising HRTF Mismatch Between the System and the Listener</h3><div class="Para" id="Par7">Various means have been investigated to minimise erroneous or conflicting binaural acoustic localisation cues relative to the natural cues delivered to the auditory system and, as such, improve the quality of the resulting binaural rendering. Majority of research has focused on improving the similarity between the rendering systems’ localisation cues and those of the individual listener. This is generally termed “<span id="ITerm10">individualisation</span>” or “individualised” binaural rendering. To clarify questions of nomenclature, we propose the following terms:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par8"><em class="EmphasisTypeItalic ">individual</em> to identify the HRTF of the user;</p></li><li><p class="Para" id="Par9"><em class="EmphasisTypeItalic ">individualised</em> or <em class="EmphasisTypeItalic ">personalised</em> to indicated an HRTF modified or selected to best accommodate the user;</p></li><li><p class="Para" id="Par10"><em class="EmphasisTypeItalic ">non-individual</em> or <em class="EmphasisTypeItalic ">non-individualised</em> to indicate an HRTF that has not been tailored to the user and</p></li><li><p class="Para" id="Par11"><em class="EmphasisTypeItalic ">dummy head</em> or so-called <em class="EmphasisTypeItalic ">generic</em> HRTF sets are specific instances of non-individual HRTFs, often designed with the goal of representing a certain pool of subjects.</p></li></ul></div>
</div><p class="Para" id="Par12">While not exhaustive, a general overview of individualisation methods is discussed here.</p><p class="Para ParaOneEmphasisChild" id="Par13"><strong class="EmphasisTypeBold ">Binaural Recordings and Synthesis</strong></p><p class="Para" id="Par14">The first and most direct method to create an individual rendering is to perform the recording with binaural microphones placed in the ear canal of the listener. This is however, in most cases, an impractical solution. The second still rather direct method is to measure the HRTF of an individual for a collection of spatial positions and to then use this individual HRTF to produce an individual binaural synthesis rendering through convolution of the sound source with the relevant incident direction HRTF [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR105" role="doc-biblioref">105</a></span>]. While this is the most common method employed to date, it is generally limited to those with the facilities and equipment to carry out such measurements [<span class="CitationRef"><a epub:type="biblioref" href="#CR4" role="doc-biblioref">4</a></span>].</p><p class="Para" id="Par15">The general pros and cons between <span id="ITerm11">binaural recordings</span>and <span id="ITerm12">binaural synthesis</span>merit mention. While individual binaural recordings provide arguably the most accurate 3D audio capture/reproduction method, they require the sonic environment and the individual to be situated accordingly. For any reasonable production, this would resemble a theatrical piece being performed around the individual in a first person context. The recording would capture the acoustic detail of the soundscape, including reflections from various surfaces, diffraction and scattering effects. However, the head orientation of the individual would be encoded into the recording, imposed on the listener at playback. If presented to another individual, the issues of HRTF mismatch are introduced, degrading the spatial <span id="ITerm13">audio quality</span>to an unknown degree for each individual. In laboratory conditions, this method suffers additional difficulty, as the individual takes part in the recording, making the presentation of unfamiliar material difficult. In contrast, binaural synthesis allows for the scripting, manipulation and mixing of 3D scenarios without the intended listener present. With real-time synthesis, head tracking can be incorporated allowing freedom of movement by the individual, a basic requirement for VR applications. HRTF mismatch is alleviated through the use of individual HRTFs. However, the quality of the production is affected by the <span id="ITerm14">level of detail</span>in the acoustic simulation of the environment, including elements such as source and surface properties. Highly complex scenes and acoustic environments can require significant computational resources (the interested reader can refer to Chap. <span class="ExternalRef"><a href="478239_1_En_3_Chapter.xhtml"><span class="RefSource">3</span></a></span> for further details on this topic). Spatial synthesis using HRTF data is also affected by the measurement conditions of the employed HRTF, predominantly the measurement distance. If sound sources are to be rendered at various distances, this requires either multiple HRTF datasets, or deformation of the individual HRTF data to approximate such changes in distance. Further discussion of these details is beyond the scope of this chapter. In continuing, the focus will be limited to questions concerning the individual nature of the HRTF as integrated into an auditory VR environment through binaural synthesis.</p><p class="Para ParaOneEmphasisChild" id="Par16"><strong class="EmphasisTypeBold ">Introduction to System-to-User and User-to-System adaptation</strong></p><p class="Para" id="Par17">A variety of alternative methods exist in order to improve the match between the HRTF used for the rendering and the specific HRTF of the listener. It is the aim of this chapter to present an overview of those approaches that have been evaluated and validated through experimental research. In order to map the various methods and at the same time simplify the narrative and facilitate the reading, the text has been organised in two separate sections. Section <span class="InternalRef"><a href="#Sec4">4.2</a></span> presents research which looks at matching the rendering system to the specific <span id="ITerm15">listener</span>(system-to-user adaptation), thus aiming to provide every individual with the best HRTF possible. Section <span class="InternalRef"><a href="#Sec7">4.3</a></span> looks at the problem from a diametrically opposite point of view, introducing studies where the listener is trained in order to adapt to the rendering system (user-to-system adaptation)<span id="ITerm16"/>, therefore aiming at improving the performance of a specific individual when using non-individual HRTFs.</p><p class="Para" id="Par18">While a rather extensive number of studies exist on the topic of system-to-user adaptation, a more limited amount of research has been carried out focusing on user-to-system adaptation. For this reason, while Sect. <span class="InternalRef"><a href="#Sec4">4.2</a></span> is presented as an extensive review of several research projects, Sect. <span class="InternalRef"><a href="#Sec7">4.3</a></span>, after an initial overview, then dives more in depth into one specific study carried out by this chapter’s authors, giving details of the methodology and briefly discussing the results. Section <span class="InternalRef"><a href="#Sec11">4.5</a></span> concludes by presenting a brief overview of open challenges on this topic.</p></section></section><section class="Section1 RenderAsSection1" id="Sec4"><h2 class="Heading"><span class="HeadingNumber">4.2 </span>System-to-User Adaptation: HRTF Synthesis and Selection</h2><p class="Para" id="Par19">Two main approaches exist for obtaining individual (or at least personalised) HRTFs without having to measure them acoustically. The first one focuses on <span id="ITerm17">numerical simulations</span>, therefore using mathematical methods to generate an HRTF for a given individual from 3D models of the head, torso, and pinnae. Techniques such as the Boundary Element Method (BEM), Finite Element Method (FEM), and Finite Difference Time Domain (FDTD) method which are commonly employed in diffraction, scattering, and resonance problems allow one to calculate the HRTF of a given individual based on precise geometrical data (e.g. coming from a <span id="ITerm18">3D scanof</span> the head and pinnae), which have been used for this purpose since the late 1990s, and have shown increased uptake and success in the past years thanks to technological advancements in domains such as high-performance computing and high-resolution 3D scanning. An example of such a resulting 3D mesh from a Neumann KU-100 <span id="ITerm19">dummy head</span>can be seen in Fig. <span class="InternalRef"><a href="#Fig2">4.2</a></span>b. The second one relies on using HRTFs from available datasets, either transforming them in order to provide a better fit for a given listener or selecting a best fit considering, for example, preference or performance, e.g. using a sound localisation task or signal metric. Due to the relative independence between the ITD and the Spectral Cues, the HRTF can be decomposed and different elements addressed by different methods, e.g. an ITD structural model can be used with best fit selected Spectra Cues [<span class="CitationRef"><a epub:type="biblioref" href="#CR22" role="doc-biblioref">22</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR78" role="doc-biblioref">78</a></span>].</p><div class="Para" id="Par20">As can be expected, each of these approaches comes with specific challenges. Moreover, the success in employing one or the other depends significantly on factors such as the available data (quantity and quality), the time constraints in order to run the tests and the calculations, and the context for which the rendering is needed (i.e. the requirements in terms of quality, interactivity, etc.). An overview of the various techniques and related challenges, including solutions found through state-of-the art research studies, is presented in the following sections.<figure class="Figure" id="Fig2"><div class="MediaObject" id="MO2"><img alt="" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Fig2_HTML.png" style="width:33.95em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 4.2</span><p class="SimplePara">Pinna morphology nomenclature and example BEM mesh</p><div class="Credit"><p class="SimplePara">(from [<span class="CitationRef"><a epub:type="biblioref" href="#CR91" role="doc-biblioref">91</a></span>])</p></div></div></figcaption></figure>
</div><section class="Section2 RenderAsSection2" id="Sec5"><h3 class="Heading"><span class="HeadingNumber">4.2.1 </span>HRTF Modelling</h3><p class="Para" id="Par21">Various attempts have been made to investigate the function of the <span id="ITerm20">pinna</span>, linking HRTFs to its morphology as well as that of the head <span id="ITerm21">and</span> torso. Early work by Teranishi and Shaw [<span class="CitationRef"><a epub:type="biblioref" href="#CR93" role="doc-biblioref">93</a></span>] looked at creating a physical model of the pinnae and analysing the various excitation modes generated by a nearby point source. The model, based on very simple geometries, showed responses similar to those of real data, and represented one of the first steps towards better understanding the spatially varying acoustic role of the pinna. Similar work was done by Batteau [<span class="CitationRef"><a epub:type="biblioref" href="#CR12" role="doc-biblioref">12</a></span>], who created a mathematical representation of the acoustical transformation performed by the pinna and produced the first mathematically described theory of sound source localisation based on a reflection-diffraction model. These studies were the baseline of research carried out 30 and more years later, when the available computational power allowed to create more complex models, and to validate those by comparing them with experimental measures (e.g.  [<span class="CitationRef"><a epub:type="biblioref" href="#CR58" role="doc-biblioref">58</a></span>]). Further modelling work was carried out looking at simplified models and approximations. Notable examples are those of Genuit [<span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>] based on a structural simplification model of the pinnae, Algazi and colleagues [<span class="CitationRef"><a epub:type="biblioref" href="#CR1" role="doc-biblioref">1</a></span>] based on an approximation of the head and the torso using ellipsoidal and spherical models, and Spagnol and colleagues [<span class="CitationRef"><a epub:type="biblioref" href="#CR89" role="doc-biblioref">89</a></span>] looking at ray-tracing analysis of pinna reflection patterns. It is relevant to note that many of the early studies focused on models for understanding the various phenomena and principles involved, rather than models for binaural audio rendering. For these early studies, much of the research on spatial perception was carried out independently from acoustical/morphological studies regarding the details of the pinnae.</p><p class="Para ParaOneEmphasisChild" id="Par22"><strong class="EmphasisTypeBold ">Structural Modelling</strong></p><p class="Para" id="Par23">One of the first experiments using these techniques applied to HRTFs (including pinnae) was carried out by Katz [<span class="CitationRef"><a epub:type="biblioref" href="#CR49" role="doc-biblioref">49</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR52" role="doc-biblioref">52</a></span>]. This work focused on using BEM to calculate HRTFs by modifying various aspects of the geometrical models, for example, eliminating the pinna, changing the size and shape of the head, and accounting for hair acoustic impedance. Results from numerical simulations were then compared with experimental measures, validating the technique and improving our understanding of the role of the pinnae in modifying the incoming sound in a direction-dependent manner. Similar work was carried out in the same period by Kahana [<span class="CitationRef"><a epub:type="biblioref" href="#CR44" role="doc-biblioref">44</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR46" role="doc-biblioref">46</a></span>]. Such simulations were initially limited, due to computational resources, to an upper frequency of 6 kHz, then extended to 10 and 20 kHz in later studies [<span class="CitationRef"><a epub:type="biblioref" href="#CR32" role="doc-biblioref">32</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR45" role="doc-biblioref">45</a></span>]. Even in these cases the validation was performed comparing the numerical model results with experimental measurements showing a good match between the two, also in light of the variances observed between different HRTF measurement systems for the same individual [<span class="CitationRef"><a epub:type="biblioref" href="#CR4" role="doc-biblioref">4</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR47" role="doc-biblioref">47</a></span>]. The computational complexity of these numerical methods was a major limitation in the early years of using this technique for generating HRTFs. Various optimisation techniques are being proposed [<span class="CitationRef"><a epub:type="biblioref" href="#CR35" role="doc-biblioref">35</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR55" role="doc-biblioref">55</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR70" role="doc-biblioref">70</a></span>], allowing significantly faster computation times with reasonable processing resources (i.e. no longer needing super computers). This led to the development of easy-to-use and open-source tools for the numerical calculation of HRTFs. A notable example is <span class="EmphasisFontCategoryNonProportional ">mesh2hrtf</span> [<span class="CitationRef"><a epub:type="biblioref" href="#CR110" role="doc-biblioref">110</a></span>], a software package centred on a BEM solver, as well as tools for the pre-processing of geometry data, generation of evaluation grids and post-processing of calculation results. It is essential here to consider a major challenge to be tackled when approaching HRTF synthesis from geometrical models, which is the acquisition and processing of the 3D models from which the HRTFs are computed. Evaluations of various 3D scanning methods, specifically looking at capturing the geometry of the pinnae, have been carried out [<span class="CitationRef"><a epub:type="biblioref" href="#CR44" role="doc-biblioref">44</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR69" role="doc-biblioref">69</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR80" role="doc-biblioref">80</a></span>].</p><p class="Para" id="Par24">Numerical simulations also brought significant benefits with regard to repeatability, replicability and reproducibility. A comparison of different numerical tools for simulating an HRTF from scan data by Greff and Katz [<span class="CitationRef"><a epub:type="biblioref" href="#CR32" role="doc-biblioref">32</a></span>] (here employing the high-resolution scan of a Neumann KU-100 shown in Fig. <span class="InternalRef"><a href="#Fig2">4.2</a></span>b) showed little variance. In contrast, a similar comparison of acoustical HRTF measurements using the same head at different laboratories [<span class="CitationRef"><a epub:type="biblioref" href="#CR4" role="doc-biblioref">4</a></span>] showed significant variations between resulting HRTFs. Another significant advantage of numerically modelling HRTFs rather than measuring them is that with physical measurements on human subjects it is difficult or impossible to isolate the influence of different morphological characteristics on the actual HRTF filters.</p><p class="Para ParaOneEmphasisChild" id="Par25"><strong class="EmphasisTypeBold ">Morphological Relationships</strong></p><div class="Para" id="Par26">Exploring and modelling the relationship between geometrical features and filter characteristics is indeed a very important step for advancing our understanding of the <span id="ITerm22">spatial hearing</span>processes. Research in this area was strongly advanced with the distribution of the CIPIC HRTF database [<span class="CitationRef"><a epub:type="biblioref" href="#CR2" role="doc-biblioref">2</a></span>], which included associated morphological parameter data for most subjects. This effort was followed with the LISTEN HRTF database [<span class="CitationRef"><a epub:type="biblioref" href="#CR98" role="doc-biblioref">98</a></span>], providing similar data. Benefiting from the power of numerical simulation and controlled geometrical models, Katz and Stitt [<span class="CitationRef"><a epub:type="biblioref" href="#CR91" role="doc-biblioref">91</a></span>] investigated the effect of morphological changes by varying specific <span id="ITerm23">morphological parameters</span>, an extension of the CIPIC set of morphological parameters to provide more unique solutions. In order to do this, they created a Parametric Pinna Model (PPM)<span id="ITerm24"/> and with BEM they investigated the sensitivity of the HRTF to specific morphological alterations. Examples of pinnae created using this PPM can be seen in Fig. <span class="InternalRef"><a href="#Fig3">4.3</a></span>. Evaluations included the use of auditory models [<span class="CitationRef"><a epub:type="biblioref" href="#CR88" role="doc-biblioref">88</a></span>]<span id="ITerm25"/> to identify those morphological changes most likely to affect spatial hearing perception. In line with previous studies, morphological features near to the rear of the helix were found to have little influence on HRTF objective metrics, while the dimension of the concha had a much more relevant impact, both looking at the directional and diffuse HRTF spectral components. <sup><a epub:type="noteref" href="#Fn2" id="Fn2_source" role="doc-noteref">2</a></sup> Other relevant findings include the importance of the region around the triangular fossa, which is often not considered when looking at HRTF personalisation, and the fact that the relief (or depth, directions parallel to the interaural axis) parameters were found to be at least as important as side-facing parameters, which are more frequently cited in morphological/HRTF studies.<figure class="Figure" id="Fig3"><div class="MediaObject" id="MO3"><img alt="" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Fig3_HTML.png" style="width:28.05em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 4.3</span><p class="SimplePara">Two pinna created with the parametric model developed in [<span class="CitationRef"><a epub:type="biblioref" href="#CR91" role="doc-biblioref">91</a></span>]</p></div></figcaption></figure>
</div><p class="Para" id="Par28">Such interest in binaural audio, combined with major advancements in terms of available technologies, has encouraged the publication of large datasets of BEM-generated HRTFs and correspondent high-accuracy 3D geometrical models. An example is the Sydney York Morphological and Acoustic Recordings of Ears (SYMARE) database [<span class="CitationRef"><a epub:type="biblioref" href="#CR42" role="doc-biblioref">42</a></span>], which was then followed by other examples of either head-related or more reduced complexity pinnae-related datasets [<span class="CitationRef"><a epub:type="biblioref" href="#CR18" role="doc-biblioref">18</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>]. The availability of such large datasets opened the door to the use of <span id="ITerm27">machine learningapproaches</span> to tackle the issue of morphology-based HRTF personalisation. An example is the work by Grijalva and colleagues [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>], where a non-linear dimensionality reduction technique is used to decompose and reconstruct the HRTF for individualisation, focusing on elements which vary the most between positions and across individuals. Results may offer improved performance over linear methods, such as principal component analysis (e.g.  [<span class="CitationRef"><a epub:type="biblioref" href="#CR81" role="doc-biblioref">81</a></span>]).</p><p class="Para ParaOneEmphasisChild" id="Par29"><strong class="EmphasisTypeBold ">HRTFs, Binaural Models and Perceptual Evaluations</strong></p><p class="Para" id="Par30">It is evident that since the 1990s a large amount of work has been carried out looking at synthesising HRTFs and better understanding the relationship between these and morphological features of the pinnae, head and torso. Nevertheless, it must be reiterated that very few of the reviewed studies have included perceptual evaluations on the modelled HRTFs [<span class="CitationRef"><a epub:type="biblioref" href="#CR18" role="doc-biblioref">18</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR56" role="doc-biblioref">56</a></span>], and that in no case such subject-based validations were extensive enough to fully support the use of synthesised HRTFs instead of measured ones. It is therefore clear that significant research is still needed in order to develop and validate models that can describe, classify and ultimately generate individual HRTFs from a reduced set of parameters.</p><p class="Para" id="Par31">While numerical assessments can be very useful when trying to better explain experimental results, they cannot be the only way to explore and validate the quality of the rendering choices. Binaural models (e.g.  [<span class="CitationRef"><a epub:type="biblioref" href="#CR88" role="doc-biblioref">88</a></span>]) could become an invaluable tool to help overcome such limitations, as they offer a computational simulation of binaural auditory processing and, in certain cases, also allow to predict listeners’ responses to binaural signals. Using them, it is possible to rapidly perform comprehensive evaluations that would be too time-consuming to implement as actual auditory experiments (e.g.  [<span class="CitationRef"><a epub:type="biblioref" href="#CR17" role="doc-biblioref">17</a></span>]).</p><p class="Para" id="Par32">An example of this approach can be found in [<span class="CitationRef"><a epub:type="biblioref" href="#CR29" role="doc-biblioref">29</a></span>], where an anthropometry-based mismatch function between HRTF pairs, looking at the relationship between pinna geometry and localisation cues, was used to select an optimal HRTF for a given individual, specifically looking at vertical localisation. The outcome of the selection was then evaluated using an auditory model which computed a mapping between HRTF spectra and perceived spatial locations. While this study outlined that the best fitting HRTF selected with the proposed method was predicted to yield a significantly improved vertical localisation when compared to a selected generic HRTF, it must be reiterated that the reliability of perceptual models is still to be thoroughly validated, and potential biases can be identified and dealt with only through actual perceptual evaluations. Another similar application of binaural models has been recently published, focusing on the comparison between different Ambisonics-based binaural rendering methods [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>]. The very large number of independent variables (e.g. each method was tested with Ambisonics orders from 1 to 44), as well as the complexity of the interactions between such variables, would make it very challenging to run perceptual evaluations with subjects. This study showed not only that models’ predictions were consistent with previous perceptual data, but also contributed to validate the models’ ability to predict user responses to binaural signals.</p><p class="Para" id="Par33">It is likely that models will never be able to provide 100% accurate assessments near to the zone of perfect reproduction, in part due to the difficulties in modelling processes such as cognitive loading and procedural/perceptual learning. However, it is reasonable to expect them to provide broadly correct predictions for larger errors. This means that they could be particularly useful when prototyping rendering algorithms and designing HRTF personalisation experiments, in order to rapidly reduce the number of conditions and variables which are subsequently assessed through real subject-based perceptual evaluations.</p><p class="Para" id="Par34">Artificial intelligence and machine learning should play an important role in such future research, looking at improving both HRTF synthesis and selection processes, as well as perceptual <span id="ITerm28">models</span> accuracy and reliability.</p></section><section class="Section2 RenderAsSection2" id="Sec6"><h3 class="Heading"><span class="HeadingNumber">4.2.2 </span>HRTF Selection</h3><p class="Para" id="Par35">A different approach for obtaining individual (or at least personalised) HRTFs without having to acoustically measure them is to rely on available HRTF databases, either transforming/tuning the transfer function according to certain subjective criteria, or designing a process for selecting the best fitting HRTF for a given subject. Regarding the first option, as mentioned at the beginning of this section, it is generally known that frequency-independent ITDs from a given HRTF can be modified and personalised according to e.g. the head circumference of a <span id="ITerm29">given</span> listener [<span class="CitationRef"><a epub:type="biblioref" href="#CR9" role="doc-biblioref">9</a></span>]. Such a technique is implemented in a few binaural spatialisers [<span class="CitationRef"><a epub:type="biblioref" href="#CR22" role="doc-biblioref">22</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR78" role="doc-biblioref">78</a></span>]. However, the personalisation of other HRTF features, such as monoaural and interaural Spectral Cues, presents more significant challenges. Early works in this direction looked at improving vertical localisation by scaling the HRTF in frequency [<span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR65" role="doc-biblioref">65</a></span>]. Other “simpler” approaches to tuning were found to be effective, for example, by manually modifying frequency and phase for every HRTF direction, for the left and right ears independently [<span class="CitationRef"><a epub:type="biblioref" href="#CR86" role="doc-biblioref">86</a></span>]. Hwang and colleagues [<span class="CitationRef"><a epub:type="biblioref" href="#CR40" role="doc-biblioref">40</a></span>] carried out a principal component analysis on the CIPIC HRTFs and used the output components to develop a customisation method based on subjective tuning of a generalised HRTF. Such customisation allowed listeners to perform significantly better in vertical perception and front-back discrimination tasks. The same approach was used to modify and personalise a KEMAR HRTF, resulting also in this case in significantly improved vertical localisation abilities [<span class="CitationRef"><a epub:type="biblioref" href="#CR84" role="doc-biblioref">84</a></span>].</p><p class="Para ParaOneEmphasisChild" id="Par36"><strong class="EmphasisTypeBold ">HRTF Selection Methods</strong></p><p class="Para" id="Par37">Methods for selecting a best fit HRTF based on subjective criteria can be grouped into two general categories: physical measurement-based matching and perceptual selection. The first pertains to selecting an HRTF from an existing set based on morphological measurements or sparse acoustical measurements. Of importance is the determination of the relevant morphological <span id="ITerm30">features</span>, as they pertain to spatial hearing and HRTF-related cues, as examined by [<span class="CitationRef"><a epub:type="biblioref" href="#CR91" role="doc-biblioref">91</a></span>]. Zotkin and colleagues [<span class="CitationRef"><a epub:type="biblioref" href="#CR112" role="doc-biblioref">112</a></span>] looked at a selection strategy based on matching certain anthrophometric pinnae parameters of the specific subject with those of HRTFs within a dataset, while providing associated low-frequency information using a “head-and-torso” model. Comparison between a non-personalised HRTF and the selected HRTF via this method showed heightened localisation accuracy and improved subjective perception of the virtual auditory scene when using the latter. A similar approach was used by [<span class="CitationRef"><a epub:type="biblioref" href="#CR81" role="doc-biblioref">81</a></span>], where advanced statistical methods were employed to create a subset of morphological parameters, which were then employed for predicting what might be the subject’s preferred HRTF based on measurement matching. HRTFs selected using this method performed better than randomly selected ones. An alternate selection perspective was proposed in [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>], where a reflection model was applied to the picture of the pinnae of the subject, facilitating the extraction of relevant anthropometric parameters which were then used for selecting one or more HRTFs from an existing database. This selection method resulted in a significant improvement in elevation localisation performances, as well as an enhancement of the perceived externalisation of the simulated sources. The relationship between features of the pinna shape and HRTF notches, focusing specifically on elevation perception, was successfully used in [<span class="CitationRef"><a epub:type="biblioref" href="#CR27" role="doc-biblioref">27</a></span>] for selecting a best fitting HRTF from pinna images. Interestingly, studies on Spectral Cues have suggested the importance of notches over peaks in the HRTF [<span class="CitationRef"><a epub:type="biblioref" href="#CR31" role="doc-biblioref">31</a></span>]. Another work from Geronazzo and colleagues [<span class="CitationRef"><a epub:type="biblioref" href="#CR28" role="doc-biblioref">28</a></span>] introduced a rather original approach by developing the <em class="EmphasisTypeItalic ">Mixed Structural Modelling</em> (MSM)<span id="ITerm31"/>, a framework for HRTF individualisation which combines structural modelling and HRTF selection. The level of flexibility of this solution, which allows to mix modelled and recorded components (therefore HRTF selection and synthesis), is particularly promising when looking at the HRTF personalisation process.</p><p class="Para ParaOneEmphasisChild" id="Par38"><strong class="EmphasisTypeBold ">HRTF Evaluation</strong></p><p class="Para" id="Par39">It must be highlighted that whether selection is based on measured or perceptual data, the evaluation of said method is necessarily perceptual as the final application is a human-centred experience. With this in mind, a fundamental yet unanswered question is: “What determines the suitability of an HRTF for a given subject?” [<span class="CitationRef"><a epub:type="biblioref" href="#CR48" role="doc-biblioref">48</a></span>]. When establishing whether an HRTF is a good fit, should one look at how precisely sound sources can be localised using that HRTF (direct approaches), or should other subjective metrics (e.g. realism, spatial quality or overall preference) be employed [<span class="CitationRef"><a epub:type="biblioref" href="#CR87" role="doc-biblioref">87</a></span>]? In employing perceptual selection, the choice of protocol becomes more critical. In addition, as was observed with acoustical measurements, the repeatability of the measurement apparatus (here the response of human subjects) must be examined and taken into account. As an example, past studies using binaural audio rendering for applications other than spatial hearing research (e.g.  [<span class="CitationRef"><a epub:type="biblioref" href="#CR74" role="doc-biblioref">74</a></span>]) relied on simple perceptually based HRTF selection procedures which, at a later stage, resulted in being less repeatable than originally thought [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>]. Without extensive training as seen in some of the principal earlier studies, the reliability of naive listeners (those situations which are also more representative of applied uses of binaural audio rather than studies on fundamental auditory processing) must be taken into account. Early studies on HRTF selection through ratings [<span class="CitationRef"><a epub:type="biblioref" href="#CR53" role="doc-biblioref">53</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR74" role="doc-biblioref">74</a></span>] assumed innate reliability in quality judgements. More recently, studies have shown that such reliability cannot be assumed, but must be evaluated, with some <span id="ITerm32">listeners</span> being highly repeatable while others are not [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>].</p><div class="Para" id="Par40">It can be assumed that different HRTFs will, for a given subject, result in different performances in a sound source localisation task. From this we can infer that an optimal HRTF could be selected looking at such performances, for example, using metrics such as localisation errors and front-back and up-down confusion rates (see Sect. <span class="InternalRef"><a href="#Sec9">4.3.2</a></span> for metric definitions). This assumption has been the baseline of several studies where an HRTF selection procedure was designed and evaluated based on localisation performances [<span class="CitationRef"><a epub:type="biblioref" href="#CR41" role="doc-biblioref">41</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR83" role="doc-biblioref">83</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR96" role="doc-biblioref">96</a></span>]. Such methods previously required specialised hardware, though current consumer Virtual Reality (VR) devices, thanks to their increasingly higher performance in terms of tracking capabilities (e.g.  [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>]), can now be employed for rendering and reporting the perceived direction of the sound source. However, these methods still remain rather time-consuming, as a large number of positions across the whole sphere should be evaluated in order to obtain reliable results.<figure class="Figure" id="Fig4"><div class="MediaObject" id="MO4"><img alt="" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Fig4_HTML.png" style="width:22.45em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 4.4</span><p class="SimplePara">Trajectory graphic description reference for HRTF quality ratings: horizontal (left) and median (right) plane trajectories indicating the start/stop position and trajectory direction (<span class="InlineEquation" id="IEq4"><img alt="$$\bullet \dashrightarrow $$" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Chapter_TeX_IEq4.png" style="width:2.56em"/></span>)</p><div class="Credit"><p class="SimplePara">(from [<span class="CitationRef"><a epub:type="biblioref" href="#CR92" role="doc-biblioref">92</a></span>])</p></div></div></figcaption></figure>
</div><p class="Para" id="Par41">Alternatively, HRTF selection can be the result of subjective evaluations based on indirect quality judgement approaches. Several research works have looked at asking listeners to rate HRTFs based on the perceived quality of some descriptive attributes, from the overall impression [<span class="CitationRef"><a epub:type="biblioref" href="#CR106" role="doc-biblioref">106</a></span>] to how well the auditory presentation matched specifically described locations or movements of the virtual source [<span class="CitationRef"><a epub:type="biblioref" href="#CR53" role="doc-biblioref">53</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR83" role="doc-biblioref">83</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR85" role="doc-biblioref">85</a></span>] (e.g.  Fig. <span class="InternalRef"><a href="#Fig4">4.4</a></span>). Several methods have been introduced for ultimately being able to select one or more best performing HRTFs; these include ranking [<span class="CitationRef"><a epub:type="biblioref" href="#CR83" role="doc-biblioref">83</a></span>], rating on scales [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR53" role="doc-biblioref">53</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR82" role="doc-biblioref">82</a></span>], multiple selection-elimination rounds [<span class="CitationRef"><a epub:type="biblioref" href="#CR97" role="doc-biblioref">97</a></span>] and pairwise comparisons [<span class="CitationRef"><a epub:type="biblioref" href="#CR85" role="doc-biblioref">85</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR106" role="doc-biblioref">106</a></span>]. In general, there seems to be an agreement on the fact that expert assessors (as defined by [<span class="CitationRef"><a epub:type="biblioref" href="#CR107" role="doc-biblioref">107</a></span>]) perform significantly better (i.e. in a more reliable and repeatable manner) if compared with initiated assessors [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR54" role="doc-biblioref">54</a></span>]. To gain further insight into indirect method results, some work has been carried out to develop global perceptual distance metrics with the aim to describe both HRTF and listener similarities [<span class="CitationRef"><a epub:type="biblioref" href="#CR8" role="doc-biblioref">8</a></span>]. In addition to proposing and evaluating a set of perceptual metrics, this work encourages further research into novel experiment design which could help in minimising the need for data normalisation and, more importantly, outlines the need for further investigations on the stability of these perceptual experiments/<span id="ITerm33">evaluations</span>, specifically looking at repeatability and training.</p><p class="Para ParaOneEmphasisChild" id="Par42"><strong class="EmphasisTypeBold ">Methods Comparison</strong></p><p class="Para" id="Par43">Few studies have examined the similarity between direct (i.e. localisation performances) and indirect HRTF selection methods. Using an immersive VR reporting system for the localisation test, results from [<span class="CitationRef"><a epub:type="biblioref" href="#CR108" role="doc-biblioref">108</a></span>] indicated a significant and positive mean correlation between HRTF selection based on localisation performance and HRTF ranking/selection based on quality judgement; the best HRTF selected according to one method had significantly better rating according to metrics in the other method. In contrast, using a <span id="ITerm34">gestaltreporting</span> method through the use of an avatar representation of the listener’s head, results from [<span class="CitationRef"><a epub:type="biblioref" href="#CR54" role="doc-biblioref">54</a></span>] showed no significant correlations. A number of protocol differences exist between these two studies, including the type of tasks used for both methods, the user interface (see [<span class="CitationRef"><a epub:type="biblioref" href="#CR10" role="doc-biblioref">10</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR11" role="doc-biblioref">11</a></span>] regarding localisation reporting method effects), the stimuli signals, as well as the metrics evaluated in the quality judgement task.</p></section></section><section class="Section1 RenderAsSection1" id="Sec7"><h2 class="Heading"><span class="HeadingNumber">4.3 </span>User-to-System Adaptation: HRTF Accommodation</h2><p class="Para" id="Par44">The previous section examined HRTF selection and individualisation methods in the signal domain. While such methods aim to provide every individual user with the best HRTF possible, such approaches are not always available in all conditions. <span id="ITerm35"/> However, evidence is increasingly available showing that the adult brain is adaptable to environmental changes. It has been demonstrated that this <span id="ITerm36">adaptability</span>(or <span id="ITerm37">plasticity</span>) regarding spatial auditory processing can lead to a reduction in localisation error over time in the case when a listener’s normal localisation cues are significantly modified.</p><p class="Para" id="Par45">It has been established that one can adapt to modified HRTFs over time, with ear moulds inserted in the pinnae [<span class="CitationRef"><a epub:type="biblioref" href="#CR19" role="doc-biblioref">19</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR38" role="doc-biblioref">38</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR94" role="doc-biblioref">94</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR95" role="doc-biblioref">95</a></span>], or with non-individual HRTFs through binaural rendering  [<span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR77" role="doc-biblioref">77</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR90" role="doc-biblioref">90</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR92" role="doc-biblioref">92</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR99" role="doc-biblioref">99</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR109" role="doc-biblioref">109</a></span>]. Studies have shown that one can adapt to distorted HRTFs, e.g. in [<span class="CitationRef"><a epub:type="biblioref" href="#CR60" role="doc-biblioref">60</a></span>] where participants suffering from hearing loss learned to use HRTFs whose spectrum had been warped to move audio cues back into frequency bands they could perceive. <span id="ITerm38">HRTF learningis</span> not only possible, but lasting in time [<span class="CitationRef"><a epub:type="biblioref" href="#CR62" role="doc-biblioref">62</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR92" role="doc-biblioref">92</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR109" role="doc-biblioref">109</a></span>]: users have been shown to retain performance improvements up to 4 months after training [<span class="CitationRef"><a epub:type="biblioref" href="#CR109" role="doc-biblioref">109</a></span>]. Given enough time, participants using non-individual <span id="ITerm39">HRTFs</span> may achieve localisation performance on par with participants using their own individual HRTFs [<span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR77" role="doc-biblioref">77</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR92" role="doc-biblioref">92</a></span>].</p><p class="Para" id="Par46">This concept has been successfully used to improve user localisation performance within virtual auditory environments when using non-individual HRTFs. Readers are referred to [<span class="CitationRef"><a epub:type="biblioref" href="#CR61" role="doc-biblioref">61</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR104" role="doc-biblioref">104</a></span>] for more general reviews on the broader topic of HRTF learning.</p><section class="Section2 RenderAsSection2" id="Sec8"><h3 class="Heading"><span class="HeadingNumber">4.3.1 </span>Training Protocol Parameters</h3><p class="Para" id="Par47">Learning methods explored in previous studies are often based on a localisation task. This type of learning is referred to as <em class="EmphasisTypeItalic ">explicit</em> learning [<span class="CitationRef"><a epub:type="biblioref" href="#CR61" role="doc-biblioref">61</a></span>], as opposed to <em class="EmphasisTypeItalic ">implicit</em> learning where the <span id="ITerm40">training</span>task does not immediately focus participant attention on localisation cues [<span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR92" role="doc-biblioref">92</a></span>]. Performance-wise, there is no evidence to suggest either type is better than the other. Implicit learning gives more leeway for task design <em class="EmphasisTypeItalic "><span id="ITerm41">gamification</span>
</em>. The technique is more and more applied to the design of HRTF learning methods [<span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR90" role="doc-biblioref">90</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR92" role="doc-biblioref">92</a></span>], and while its impact on HRTF learning rates remains uncertain [<span class="CitationRef"><a epub:type="biblioref" href="#CR90" role="doc-biblioref">90</a></span>], its benefit for learning, in general, is, however, well established [<span class="CitationRef"><a epub:type="biblioref" href="#CR36" role="doc-biblioref">36</a></span>]. On the other hand, explicit learning more readily produces training protocols where participants are <em class="EmphasisTypeItalic ">consciously</em> focusing on the learning process [<span class="CitationRef"><a epub:type="biblioref" href="#CR63" role="doc-biblioref">63</a></span>], potentially helping with the unconscious re-adjustment of auditory spatial mapping.</p><p class="Para" id="Par48">As much as the nature of the task, providing feedback can play an important role during learning. VR technologies are more and more relied upon to increase feedback density in the hope of increasing HRTF learning rates (in Chap. <span class="ExternalRef"><a href="478239_1_En_10_Chapter.xhtml"><span class="RefSource">10</span></a></span>, the interested reader can find further insights on multisensory feedback in VR). While results encourage the use of a visual virtual environment [<span class="CitationRef"><a epub:type="biblioref" href="#CR60" role="doc-biblioref">60</a></span>], it has been reported that proprioceptive <span id="ITerm42">feedback</span> alone can be used to improve learning rates [<span class="CitationRef"><a epub:type="biblioref" href="#CR16" role="doc-biblioref">16</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>]. Direct comparison of experimental results suggests that active learning with direct feedback is more efficient (i.e. leads to faster improvement) than passive learning from sound exposure [<span class="CitationRef"><a epub:type="biblioref" href="#CR61" role="doc-biblioref">61</a></span>]. There is also a growing consensus on the use of adaptive (i.e. head-tracked) binaural rendering during training to improve learning rates [<span class="CitationRef"><a epub:type="biblioref" href="#CR19" role="doc-biblioref">19</a></span>], despite the generalised use of static head-locked localisation tasks to assess performance evolution [<span class="CitationRef"><a epub:type="biblioref" href="#CR61" role="doc-biblioref">61</a></span>]. It is not trivial to ascertain whether the benefit of head-tracked rendering comes from continuous situated feedback improving audio cue recalibration, or from unbalanced comparison, as static head-locked rendering creates user frustration and results in less sound exposure [<span class="CitationRef"><a epub:type="biblioref" href="#CR90" role="doc-biblioref">90</a></span>]).</p><p class="Para" id="Par49">Studies on the training stimulus indicate that learning extends to more than the signals used during learning [<span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR90" role="doc-biblioref">90</a></span>]. This result is likely dependent on specific characteristics of the stimuli and how these relate to auditory localisation mechanisms, i.e. whether they present the transient energy and broad frequency content necessary for auditory spatial discrimination [<span class="CitationRef"><a epub:type="biblioref" href="#CR24" role="doc-biblioref">24</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR57" role="doc-biblioref">57</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR72" role="doc-biblioref">72</a></span>].</p><p class="Para" id="Par50">There is no clear cut result on optimum training session duration and scheduling. Training session duration reported in previous studies ranges from <span class="InlineEquation" id="IEq5"><img alt="$$\approx $$" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Chapter_TeX_IEq5.png" style="width:1.06em"/></span>8 min [<span class="CitationRef"><a epub:type="biblioref" href="#CR66" role="doc-biblioref">66</a></span>] to <span class="InlineEquation" id="IEq6"><img alt="$$\approx $$" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Chapter_TeX_IEq6.png" style="width:1.06em"/></span>2h [<span class="CitationRef"><a epub:type="biblioref" href="#CR60" role="doc-biblioref">60</a></span>]. Comparative analysis argues in favour of several short training sessions over long ones [<span class="CitationRef"><a epub:type="biblioref" href="#CR61" role="doc-biblioref">61</a></span>]. Training session spread is also widely distributed in the literature, ranging from all sessions in one day [<span class="CitationRef"><a epub:type="biblioref" href="#CR57" role="doc-biblioref">57</a></span>] versus one every week or every other week [<span class="CitationRef"><a epub:type="biblioref" href="#CR92" role="doc-biblioref">92</a></span>]. Where results suggest spreading training over time benefits learning (all in 1 day versus spread over 7 days) [<span class="CitationRef"><a epub:type="biblioref" href="#CR57" role="doc-biblioref">57</a></span>] outcomes from [<span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR92" role="doc-biblioref">92</a></span>] indicate that weekly sessions and daily sessions result in the same overall performance improvement (for equal total training duration). There is some example of latent learning (improvement between sessions) in the literature [<span class="CitationRef"><a epub:type="biblioref" href="#CR66" role="doc-biblioref">66</a></span>], naturally encouraging the spread of training sessions. Regardless of duration and spread, studies have shown that learning saturation occurs after a while. In [<span class="CitationRef"><a epub:type="biblioref" href="#CR59" role="doc-biblioref">59</a></span>], most of the training effect took place within the first 400 trials (<span class="InlineEquation" id="IEq7"><img alt="$$\approx $$" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Chapter_TeX_IEq7.png" style="width:1.06em"/></span>160 min), a result comparable to that reported by [<span class="CitationRef"><a epub:type="biblioref" href="#CR20" role="doc-biblioref">20</a></span>] where saturation was reached after 252 to 324 trials.</p><p class="Para" id="Par51">One of the critical questions not fully answered to date is the role of the HRTF fit in the training process or how similar the training HRTF is to the actual HRTF of the individual. It would appear that a certain degree of affinity between a participant and the training HRTF facilitates learning [<span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR92" role="doc-biblioref">92</a></span>]. In contrast, lack of adaptation can occur if the HRTF to be learned is too different from one’s own HRTF. This is evidenced by mixed adaptation results in studies where ill-suited HRTF matches were tested.</p></section><section class="Section2 RenderAsSection2" id="Sec9"><h3 class="Heading"><span class="HeadingNumber">4.3.2 </span>HRTF Accommodation Example</h3><p class="Para" id="Par52">We present here as an example HRTF learning study by Stitt et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR92" role="doc-biblioref">92</a></span>], which examined the effect of adaptation to non-individual HRTFs. This study was chosen for this example as it provides a controlled study over a significant number of training sessions. As a “<em class="EmphasisTypeItalic ">worst-case</em>” real-world scenario, perceptually worst-rated non-individual HRTFs were chosen by each subject to allow for maximum potential for improvement, another factor of interest in its design. This study is part of a series of studies on the subject of user-to-system adaptation, providing continuity of comparisons [<span class="CitationRef"><a epub:type="biblioref" href="#CR15" role="doc-biblioref">15</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR77" role="doc-biblioref">77</a></span>]. The methodology consisted of a training game and a localisation test to evaluate performance carried out over 10 sessions. Subjects using non-individual HRTFs (group <strong class="EmphasisTypeBold ">W10</strong>) were tested alongside control subjects using their own individual measured HRTFs (group <strong class="EmphasisTypeBold ">C10</strong>).</p><p class="Para" id="Par53">Prior to any training, subjects were assigned non-individual HRTFs based on quality judgements of rendered sound object trajectories for 7 HRTF sets, taken as “<em class="EmphasisTypeItalic ">perceptually orthogonal</em>” [<span class="CitationRef"><a epub:type="biblioref" href="#CR53" role="doc-biblioref">53</a></span>]. These trajectories, shown in Fig. <span class="InternalRef"><a href="#Fig4">4.4</a></span>, were presented to subjects as a reference. Following the results of [<span class="CitationRef"><a epub:type="biblioref" href="#CR8" role="doc-biblioref">8</a></span>], which examined the reliability and repeatability of HRTF judgements by naive and experienced subjects, this rating task was performed three times, leading to a total of six ratings per subject, counting the two trajectories, with the overall judgement rating taken as the overall mean. The lowest rated HRTF for each subject was then used as that subject’s <em class="EmphasisTypeItalic ">worst</em>-match HRTF. This method is an improvement over alternate methods which are either uncontrolled (e.g. a single HRTF used by all listeners) or limited in the extent of relative spectral changes presented to subjects when compared to their individual HRTFs.</p><p class="Para" id="Par54">The training procedure for the 10 sessions was devised as a simple game with a searching task in which the listener had to find a target at a hidden position in some direction (<span class="InlineEquation" id="IEq8"><img alt="$$\theta ,\phi $$" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Chapter_TeX_IEq8.png" style="width:1.94em"/></span>), ignoring radial distance. Subjects searched for the hidden target by moving the motion-tracked hand-held object around their head (see concept in Fig. <span class="InternalRef"><a href="#Fig5">4.5</a></span>). For the duration of the search, alternating pink/white noise (50–20000 Hz) with an overall level of approximately 55 dBA measured at the ear was presented to the listener, positioned at the location of the tracked hand-held object relative to the subject’s head. This provided a link between the proprioceptively known position of the subject’s own hand and spatial cues in the binaural rendering. The alternation <em class="EmphasisTypeItalic ">rate</em> of the pink/white noise bursts increased with increasing angular proximity to the target direction using a Geiger counter metaphor [<span class="CitationRef"><a epub:type="biblioref" href="#CR71" role="doc-biblioref">71</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR79" role="doc-biblioref">79</a></span>]. Once the subject reached the intended target direction, a success sound would play, spatialised at the target’s location. The training game lasted 12 min and subjects were instructed to find as many targets as possible in the time available. Sessions 1–4 occurred at 1-week interval, while the remaining sessions occurred at 2-week interval.</p><div class="Para" id="Par55">It should be emphasised that no auditory localisation on the part of the subject was actually required to accomplish this task, only tempo judgements of the alternation <em class="EmphasisTypeItalic ">rate</em> of the pink/white noise bursts and proprioceptive knowledge of one’s hand position. HRTF adaptation was therefore an implicit result of game play, but not the task of the game as far as the participant was aware. This task was designed to facilitate learning with source positions outside of the visual field of view, as well as to function for individuals with visual impairments.<figure class="Figure" id="Fig5"><div class="MediaObject" id="MO5"><img alt="" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Fig5_HTML.png" style="width:17.38em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 4.5</span><p class="SimplePara">Training game concept design</p></div></figcaption></figure>
</div><p class="Para ParaOneEmphasisChild" id="Par56"><strong class="EmphasisTypeBold ">Performance Evaluation Metrics</strong></p><p class="Para" id="Par57">The HRTF accommodation was evaluated via <span id="ITerm43">localisation tests</span>. Subjects were presented a brief burst of noise (to limit the influence of any possible head movement during playback) and would subsequently point in the perceived direction of the sound using the hand-held object. No feedback was given to subjects regarding the target position. The noise burst consisted of a train of three, 40ms Gaussian broadband noise pulses (20000 Hz) with 2 ms raised cosine window applied at onset and offset and 30ms of silence between each burst [<span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>]. There were 25 target directions with 5 repetitions of each target, resulting in the tested sphere including a full 360<span class="InlineEquation" id="IEq9"><img alt="$$^{\circ }$$" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Chapter_TeX_IEq9.png" style="width:0.57em"/></span> of azimuth, and –40–90<span class="InlineEquation" id="IEq10"><img alt="$$^{\circ }$$" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Chapter_TeX_IEq10.png" style="width:0.57em"/></span> of elevation.</p><div class="Para" id="Par58">Two types of metrics were used to analyse localisation errors: <em class="EmphasisTypeItalic ">angular</em> and <em class="EmphasisTypeItalic ">confusion</em> metrics. The interaural coordinate system defines a lateral and polar angle Fig. <span class="InternalRef"><a href="#Fig6">4.6</a></span>a. The lateral angle is the angle between the interaural axis and the line between the origin and the source. The lateral angle approaches cones-of-confusion along which the interaural cues (ITD and ILD) are approximately equal. A cone-of-confusion is defined by the contour around the listener for a given ITD or ILD (see Fig. <span class="InternalRef"><a href="#Fig1">4.1</a></span>). For ITD, these contours can be generally represented by a hyperbolic function, where the difference in arrival time to the two ears is constant and the vertex is on the interaural axis, between the two ears. The intersection of the ITD and ILD cones-of-confusion for a given stimulus prescribes a closed curve (approaching a circle). The ITD and ILD are insufficient to resolve the localisation ambiguity, requiring further information, such as from Spectral Cues or head movements. The polar angle is the angle between the horizontal plane and a perpendicular line from the interaural axis to the point, such that the polar angle prescribes the source location on the cone-of-confusion. The polar angle is primarily linked with the monaural, Spectral Cues in the HRTF. This independence of binaural and Spectral Cues makes the interaural coordinate system a natural choice when looking at localisation performance. If the perceived ILD, ITD and Spectral Cues of a given source do not adequately coincide with the expectations of the auditory system for a single point in space, uncertainty in localisation response ensues. The most commonly referenced uncertainties are polar angle confusions.<figure class="Figure" id="Fig6"><div class="MediaObject" id="MO6"><img alt="" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Fig6_HTML.png" style="width:33.98em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 4.6</span><p class="SimplePara">Interaural polar coordinate system and associated polar angle cone-of-confusion zone definitions</p></div></figcaption></figure>
</div><p class="Para" id="Par59">Polar angle confusions are classified using a traditional segmentation of the cone-of-confusion [<span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR92" role="doc-biblioref">92</a></span>], revised in [<span class="CitationRef"><a epub:type="biblioref" href="#CR108" role="doc-biblioref">108</a></span>]. The classification results in three potential confusion types, front-back, up-down and combined, with a fourth type corresponding to precision errors, represented schematically in Fig. <span class="InternalRef"><a href="#Fig6">4.6</a></span>b. The <em class="EmphasisTypeItalic ">precision</em> category designates any response close enough to the real target so as not to be associated to the other confusion types. In short, responses classified under <em class="EmphasisTypeItalic ">precision</em> are for those within ±45<span class="InlineEquation" id="IEq11"><img alt="$$^{\circ }$$" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Chapter_TeX_IEq11.png" style="width:0.57em"/></span> of the target angle, <em class="EmphasisTypeItalic ">front-back</em> classified errors are responses reflected in the frontal plane, and those classified <em class="EmphasisTypeItalic ">up-down</em> are for those reflected in the transverse plane. Any responses that fall outside of these regions are classified as <em class="EmphasisTypeItalic ">combined</em> type errors.</p><p class="Para ParaOneEmphasisChild" id="Par60"><strong class="EmphasisTypeBold ">Performance Evaluation Results</strong></p><div class="Para" id="Par61">Results examined the evolution of polar angle error and confusion rates. As a measure of accommodation, the <em class="EmphasisTypeItalic ">rate of improvement</em> was defined as the gradient of the linear regression of polar angle error. The rates of improvement for the 8 subjects spanned values of 0.5<span class="InlineEquation" id="IEq12"><img alt="$$^{\circ }$$" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Chapter_TeX_IEq12.png" style="width:0.57em"/></span> to 4.6<span class="InlineEquation" id="IEq13"><img alt="$$^{\circ }$$" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Chapter_TeX_IEq13.png" style="width:0.57em"/></span>/session over sessions 5–10 (as results for initial sessions have been shown to be influenced by procedural learning effects [<span class="CitationRef"><a epub:type="biblioref" href="#CR59" role="doc-biblioref">59</a></span>]). In contrast, results for the control group over the same sessions spanned 0<span class="InlineEquation" id="IEq14"><img alt="$$^{\circ }$$" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Chapter_TeX_IEq14.png" style="width:0.57em"/></span> to 2.2<span class="InlineEquation" id="IEq15"><img alt="$$^{\circ }$$" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Chapter_TeX_IEq15.png" style="width:0.57em"/></span>/session. A clustering analysis of the test group relative to the control group, <strong class="EmphasisTypeBold ">C10</strong>, separated those whose rate of improvement exceeded that of the control group (subgroup <strong class="EmphasisTypeBold ">W10+</strong>) and the remaining subjects (<strong class="EmphasisTypeBold ">W10–</strong>) who did not. This second group failed to exhibit clear HRTF adaptation results over and above that of the control group whose improvement can be considered primarily as procedural learning.<figure class="Figure" id="Fig7"><div class="MediaObject" id="MO7"><img alt="" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Fig7_HTML.png" style="width:34.03em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 4.7</span><p class="SimplePara">Result analysis by subgroup. <strong class="EmphasisTypeBold ">a</strong> Mean absolute polar angle error and 95% confidence intervals for groups <strong class="EmphasisTypeBold ">W10+</strong>, <strong class="EmphasisTypeBold ">W10–</strong> and <strong class="EmphasisTypeBold ">C10</strong> across sessions 1–10. <strong class="EmphasisTypeBold ">b</strong> Response classification analysis: Mean classification of results for group <strong class="EmphasisTypeBold ">W10</strong> by type (<em class="EmphasisTypeItalic ">precision</em> (<span class="InlineEquation" id="IEq16"><img alt="$$\times $$" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Chapter_TeX_IEq16.png" style="width:1.18em"/></span>), <em class="EmphasisTypeItalic ">front-back</em> error (<span class="InlineEquation" id="IEq17"><img alt="$$\bigcirc $$" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Chapter_TeX_IEq17.png" style="width:1.44em"/></span>), <em class="EmphasisTypeItalic ">up-down</em> error (<span class="InlineEquation" id="IEq18"><img alt="$$\bigtriangledown $$" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Chapter_TeX_IEq18.png" style="width:1em"/></span>) and <em class="EmphasisTypeItalic ">combined</em> error (<span class="InlineEquation" id="IEq19"><img alt="$$\bigtriangleup $$" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Chapter_TeX_IEq19.png" style="width:1em"/></span>)) for groups <strong class="EmphasisTypeBold ">W10+</strong> (<strong class="EmphasisTypeBold ">—</strong>, 3 subjects) and <strong class="EmphasisTypeBold ">W10–</strong> (<strong class="EmphasisTypeBold ">- -</strong>, 5 subjects) over sessions 1–10</p><div class="Credit"><p class="SimplePara">(from [<span class="CitationRef"><a epub:type="biblioref" href="#CR92" role="doc-biblioref">92</a></span>])</p></div></div></figcaption></figure>
</div><p class="Para" id="Par62">The polar errors are shown in Fig. <span class="InternalRef"><a href="#Fig7">4.7</a></span>a for groups <strong class="EmphasisTypeBold ">W10+</strong>, <strong class="EmphasisTypeBold ">W10–</strong> and <strong class="EmphasisTypeBold ">C10</strong>. Group <strong class="EmphasisTypeBold ">W10+</strong> approached a similar level of absolute performance to <strong class="EmphasisTypeBold ">C10</strong>. This demonstrates that these subjects were able to adapt well to their <em class="EmphasisTypeItalic ">worst</em>-rated HRTF to a level approaching subjects using their individually measured one. It also shows clearly that, despite continuous training, some subjects (<strong class="EmphasisTypeBold ">W10–</strong>) exhibited little or no improvement beyond the procedural learning seen in <strong class="EmphasisTypeBold ">C10</strong>.</p><p class="Para" id="Par63">The response classification results for groups <strong class="EmphasisTypeBold ">W10+</strong> and <strong class="EmphasisTypeBold ">W10–</strong> are shown in Fig. <span class="InternalRef"><a href="#Fig7">4.7</a></span>b. At the outset of the study, it can be observed that <em class="EmphasisTypeItalic ">up-down</em> and <em class="EmphasisTypeItalic ">front-back</em> type error <span id="ITerm44">rates</span> are comparable between the two subgroups, with <strong class="EmphasisTypeBold ">W10–</strong> exhibiting more <em class="EmphasisTypeItalic ">combined</em> type errors. This metric could be a potential indicator for identifying poor HRTF adaptation conditions. Subsequently, it can be clearly seen that group <strong class="EmphasisTypeBold ">W10+</strong> exhibits a steady increase in <em class="EmphasisTypeItalic ">precision</em> classified responses, with reductions in <em class="EmphasisTypeItalic ">front-back</em> errors over sessions 3–5 and subsequent reductions in <em class="EmphasisTypeItalic ">combined</em> errors. In contrast, group <strong class="EmphasisTypeBold ">W10–</strong> exhibits generally consistent response classifications across sessions, with only small increases in <em class="EmphasisTypeItalic ">precision</em> classification mirrored by a decreasing trend in <em class="EmphasisTypeItalic ">front-back</em> errors. For all subjects, it can be noted that the occurrence of <em class="EmphasisTypeItalic ">up-down</em> errors is quite rare.</p><p class="Para" id="Par64">Results of this accommodation study show that adaptation to an individual’s perceptually worst-rated HRTF can continue as long as training is provided, though the rate of improvement decreases after a certain amount of training. A subgroup achieving localisation performance levels approaching the control group with individual HRTFs. These performance levels were comparable to those observed in [<span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>] with identical test protocol, where subjects performed only three training sessions using their perceptually <em class="EmphasisTypeItalic ">best rated</em> HRTF.</p></section></section><section class="Section1 RenderAsSection1" id="Sec10"><h2 class="Heading"><span class="HeadingNumber">4.4 </span>Discussion</h2><p class="Para" id="Par65">It is clear that, while various methods and tools are available for selecting a best fit HRTF for a given listener, there is no established evaluation protocol to determine how well these methods work and compare with each other. While some work is advancing in proposing common methodologies and metrics [<span class="CitationRef"><a epub:type="biblioref" href="#CR75" role="doc-biblioref">75</a></span>], the lack of established methods raises some very relevant questions about the feasibility of a unique HRTF selection task which performs reliably and independently from factors such as the listeners expertise, the signals employed, the user interface, the context where the tests are carried out and, more in general, the task for which the final quality is judged. It seems evident that any major leap forward in this field is limited until two primary issues are addressed: (1) the establishment of pertinent metrics to perceptually assess HRTFs and (2) the relationship between these metrics and specific characteristics of the signal domain HRTF filters.</p><p class="Para" id="Par66">The use of HRTF adaptation, in examining the results of this and previous studies, has been shown to be a viable option to improve spatial audio rendering, at least with regard to localisation. The level of adaptation achievable is related to the initial suitability (perceptual similarity) between the system HRTF and the user’s individual HRTF, with more suitable HRTFs showing more rapid adaptation. No significant effect has been found regarding the specific training intervals, though spreading out sessions is better than multiple sessions on the same day. The adaptation method could be integrated into a stand-alone game application, or as part of device setup and personalization configurations, typical of most VR devices to some degree. The major limitation, once the training HRTF is chosen, is the need for repeated training sessions, and this must be made clear to users so that they do not expect ideal results from the start.</p><p class="Para" id="Par67">The combination of user-to-system and system-to-user adaptation is a promising solution. While user-to-system adaptation appears limited by the initial training HRTF employed, system-to-user adaptation methods provide various means of providing, if not a perfect individual HRTF, a reasonable near approximation. As such, selection of a <em class="EmphasisTypeItalic ">pretty-good</em> HRTF match followed by user training could be a viable real-world solution.</p><div class="Para" id="Par68">An example of such a tailored HRTF training has been tested in [<span class="CitationRef"><a epub:type="biblioref" href="#CR77" role="doc-biblioref">77</a></span>]. In this work, as compared to the previous mentioned study in Sect. <span class="InternalRef"><a href="#Sec9">4.3.2</a></span>, the subject was aware of the goal of the training, with specific HRTF-based localisation difficulties presented with increasing difficulty (see Fig. <span class="InternalRef"><a href="#Fig8">4.8</a></span>). In addition, a best match HRTF condition was employed using an interactive exploration method, rather than the general ranking described in Sect. <span class="InternalRef"><a href="#Sec6">4.2.2</a></span> and a worst-case selection scenario. Results indicated that the proposed training program led to improved learning rates compared to that of previous studies. A further addition of this study was the inclusion of a simulated room acoustic response, moving from the typical anechoic conditions of previous studies to a more natural acoustic for the user. Results showed that the addition of the <span id="ITerm45">room acoustics</span>improved HRTF adaptation rate across sessions.<figure class="Figure" id="Fig8"><div class="MediaObject" id="MO8"><img alt="" src="../images/478239_1_En_4_Chapter/478239_1_En_4_Fig8_HTML.png" style="width:25.42em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 4.8</span><p class="SimplePara">Example active HRTF learning training game. Training setup: (top-left) participant in the experiment room, (bottom-left) third person view of the training platform, (right) participant viewpoint during the training</p><div class="Credit"><p class="SimplePara">(from [<span class="CitationRef"><a epub:type="biblioref" href="#CR77" role="doc-biblioref">77</a></span>])</p></div></div></figcaption></figure>
</div></section><section class="Section1 RenderAsSection1" id="Sec11"><h2 class="Heading"><span class="HeadingNumber">4.5 </span>Conclusions and Future Directions</h2><p class="Para" id="Par69">While binaural audio and spatial hearing have been studied for over 100 years, major advancements in these fields have occurred in the last two to three decades, possibly thanks to progress in real-time computing technologies. It has been extensively shown that everyone perceives spatial sound differently thanks to the particular shape of their ears, head and torso. For this reason, either high-quality simulations need to be uniquely tailored to each individual listener, or the listener needs to adapt to the configuration (i.e. the HRTF on offer) of the rendering system, or again some combination of both using individualised HRTFs. This chapter has provided an overview of research aimed at systematically exploring, assessing and validating various aspects of these two approaches. But while there is a good level of agreement on certain notions and principles, e.g. that using non-individual HRTFs can result in impaired localisation performance which can however be improved through perceptual training, there are still open challenges in need of further investigation.</p><p class="Para" id="Par70">A rather general but very important question that has yet to be addressed is how we can measure whether a simulated immersive audio experience is suitable and of sufficient quality for a given individual. Previous work has established a certain level of standardisation for assessing general <span id="ITerm46">audio quality</span>(e.g. related to telecommunication and audio compression algorithms), but equivalent work has yet to be carried out in the field of immersive audio. Objective and subjective metrics for assessing HRTF similarity have been explored and evaluated in the past [<span class="CitationRef"><a epub:type="biblioref" href="#CR5" role="doc-biblioref">5</a></span>], and recently published research suggests that additional metrics might exist, e.g. looking at speech understanding performance [<span class="CitationRef"><a epub:type="biblioref" href="#CR21" role="doc-biblioref">21</a></span>] or machine learning artificial localization tests [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR13" role="doc-biblioref">13</a></span>]. Nevertheless, extensive research is still needed in order to understand and model low-level psychophysical (sensory) as well as high-level psychological (cognitive) spatial hearing perception.</p><p class="Para" id="Par71">Factors other than choices related to binaural audio processing could also have an impact on the overall perception of the rendered scenes. The fact that high-quality, albeit non-interactive, immersive audio rendering can be achieved through recordings done with a simple binaural microphone, which by definition do not account for individualised HRTFs, can be considered an example of the major complexity and dimensionality of the problem. Matters such as the choice of audio content, the context of the rendered scene, as well as the experience of the listener (e.g. whether they have previously participated in immersive audio assessments) have been shown to be relevant when assessing the perceived quality of the immersive audio rendering [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR54" role="doc-biblioref">54</a></span>]. Such a discussion found a natural continuation in Chap. <span class="ExternalRef"><a href="478239_1_En_5_Chapter.xhtml"><span class="RefSource">5</span></a></span>.</p><p class="Para" id="Par72">Looking more in depth at the need to quantify the individually perceived quality of the rendering, the understanding of the perceptual weighting of morphological factors contributing to spatial hearing becomes an essential target to be achieved. Data-based machine learning approaches may be a useful tool when tackling this, as well as challenges related to user-to-system adaptation. Examples include allowing a certain level of customisation of the training by individually and adaptively varying the difficulty of the challenge, maximising learning and at the same time avoiding an overload of sensory and cognitive capabilities. Further explorations on spatial hearing adaptation shall focus on exploring the transferability of the acquired training between different hearing skills (e.g.  [<span class="CitationRef"><a epub:type="biblioref" href="#CR100" role="doc-biblioref">100</a></span>]) and examining to what extent spatial auditory training performed in VR is transferable to real-life tasks.</p><p class="Para" id="Par73">Another very relevant yet still under-explored area of research is employing cognitive and psycho-physiological measurements when trying to assess both the quality of rendered spatial hearing cues and the cognitive effort during HRTF training. In the first case, measures related with behavioural performance, as well as electroencephalographic markers of <span id="ITerm47">selective attention</span>, could be used to assess the suitability of immersive rendering choices [<span class="CitationRef"><a epub:type="biblioref" href="#CR23" role="doc-biblioref">23</a></span>], possibly opening the path towards passive perceptual-based HRTF selection. In the second case, similar metrics, with the addition of other measures of listening effort such as <span id="ITerm48">pupil dilation</span> [<span class="CitationRef"><a epub:type="biblioref" href="#CR103" role="doc-biblioref">103</a></span>], could be employed for customising spatial hearing training routines, maximising outcomes while maintaining engagement and feasibility of the proposed tasks.</p><p class="Para ParaOneEmphasisChild" id="Par74"><strong class="EmphasisTypeBold ">Final Thoughts</strong></p><p class="Para" id="Par75">While most studies have focused on laboratory conditions to isolate specific perception elements, recent context-relevant studies have begun to examine the impact of spatial audio quality on task accomplishment. For example, [<span class="CitationRef"><a epub:type="biblioref" href="#CR76" role="doc-biblioref">76</a></span>] compared performance in a first-person-shooter VR game context with different HRTF <span id="ITerm49">conditions</span>. Results showed performance for extreme elevation target positions was affected by the quality of HRTF matching. In addition, a subgroup of participants showed higher sensitivity to HRTF choice than others. At the same time, low-level sensory perception is only one of the dimensions where immersive audio simulations can have a significant impact. In order to significantly advance our understanding of the impact of HRTF personalisation in virtually rendered scenes and tasks, research needs to move beyond the evaluation of individual immersive audio tasks and metrics (e.g. sound localisation and/or perceived quality of the rendering), moving towards the evaluation of full experiences. The impact of immersive audio beyond perceptual metrics such as localisation, externalisation and immersion [<span class="CitationRef"><a epub:type="biblioref" href="#CR87" role="doc-biblioref">87</a></span>] is an as yet unexplored area of research, specifically when related with social interaction, entering the behavioural and cognitive realms.</p><p class="Para" id="Par76">In the past, several studies have been published in which auditory-based AR/VR interactions were created and evaluated without considering HRTF choice or using HRTF personalisation approaches that had not previously been appropriately validated from a perceptual point of view, or again ignoring the effects of HRTF accommodation, or blaming them in order to justify unexpected results. Considering our current knowledge and experience in immersive audio research, we are keen to recommend carrying out some level of personalisation of the spatial rendering when performing studies which involve auditory-based or multimodal interactions in AR/VR. As a baseline, ITDs can easily be customised to match the head circumference of the specific listener (as mentioned above, this function is already implemented in a few spatialisers, such as [<span class="CitationRef"><a epub:type="biblioref" href="#CR22" role="doc-biblioref">22</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR78" role="doc-biblioref">78</a></span>]). Furthermore, HRTF selection routines, both perceptual and morphology based, could be very beneficial if carried out before the experiment, albeit it is important for the repeatability of such choices to be assessed with the specific subject (i.e. repeating the selection several times in order to verify the consistency across the trials, and possibly discard subjects/methods which do not show a sufficient level of repeatability). Regarding the use of synthesised HRTFs, until these are validated through extensive perceptual studies our advice is to use measured ones, possibly coming from the same dataset in order to avoid measurement-based differences.</p><p class="Para" id="Par77">In addition to these recommendations, it is important to emphasize that the future of immersive audio research will need to include studies focusing on different contexts (e.g. AR/VR interactions, virtual museum explorations and virtual assistant avatars), exploring the impact (and need) of HRTF personalisation on complex tasks such as interpersonal exchanges and distance learning in VR. Furthermore, in order to ensure a sufficient level of standardisation and consistently advance the achievements of research in this area, it seems evident that a concerted and coordinated effort across disciplines and research groups is highly desirable.</p></section><div class="Acknowledgments" epub:type="acknowledgments" role="doc-acknowledgments"><div class="Heading">Acknowledgements</div><p class="SimplePara">Preparation of the chapter was made possible by support from SONICOM (<span class="ExternalRef"><a href="https://www.sonicom.eu/"><span class="RefSource">www.​sonicom.​eu</span></a></span>), a project that has received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement No. 101017743.</p></div><div class="License LicenseSubType-cc-by"><a href="https://creativecommons.org/licenses/by/4.0"><img alt="Creative Commons" src="../css/cc-by.png"/></a><p class="SimplePara"><strong class="EmphasisTypeBold ">Open Access</strong> This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (<span class="ExternalRef"><a href="http://creativecommons.org/licenses/by/4.0/"><span class="RefSource">http://​creativecommons.​org/​licenses/​by/​4.​0/​</span></a></span>), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p><p class="SimplePara">The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p></div><aside aria-labelledby="Bib1Heading" class="Bibliography" id="Bib1"><div epub:type="bibliography" role="doc-bibliography"><div class="Heading" id="Bib1Heading">References</div><ol class="BibliographyWrapper"><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">1.</div><div class="CitationContent" id="CR1">Algazi, V. R., Duda, R. O., Duraiswami, R., Gumerov, N. A., Tang, Z.: Approximating the head-related transfer function using simple geometric models of the head and torso. J Acoust Soc Am <strong class="EmphasisTypeBold ">112</strong>, 2053–2064 (2002).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">2.</div><div class="CitationContent" id="CR2">Algazi, V. R., Duda, R. O., Thompson, D. M., Avendano, C.: The cipic hrtf database in Proceedings of the 2001 IEEE Workshop on the Applications of Signal Processing to Audio and Acoustics (Cat. No. 01TH8575) (2001), 99–102.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">3.</div><div class="CitationContent" id="CR3">Ananthabhotla, I., Ithapu, V. K., Brimijoin, W. O.: A framework for designing head-related transfer function distance metrics that capture localization perception. JASA Express Letters <strong class="EmphasisTypeBold ">1</strong>, 044401:1–6 (2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">4.</div><div class="CitationContent" id="CR4">Andreopoulou, A., Begault, D. R., Katz, B. F.: Inter-Laboratory Round Robin HRTF Measurement Comparison. IEEE J Selected Topics in Signal Processing <strong class="EmphasisTypeBold ">9</strong>, 895–906 (2015).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">5.</div><div class="CitationContent" id="CR5">Andreopoulou, A., Katz, B. F. G.: On the use of subjective HRTF evaluations for creating global perceptual similarity metrics of assessors and assessees in Intl Conf on Auditory Display (2015), 13–20.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">6.</div><div class="CitationContent" id="CR6">Andreopoulou, A., Katz, B. F. G.: Investigation on Subjective HRTF Rating Repeatability in Audio Eng Soc Conv <strong class="EmphasisTypeBold ">140</strong> (Paris, June 2016), 9597:1–10.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">7.</div><div class="CitationContent" id="CR7">Andreopoulou, A., Katz, B. F.: Identification of perceptually relevant methods of inter-aural time difference estimation. J Acoust Soc Am <strong class="EmphasisTypeBold ">142</strong>, 588–598 (2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">8.</div><div class="CitationContent" id="CR8">Andreopoulou, A., Katz, B. F.: Subjective HRTF evaluations for obtaining global similarity metrics of assessors and assessees. Journal on Multimodal User Interfaces <strong class="EmphasisTypeBold ">10</strong>, 259–271 (2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">9.</div><div class="CitationContent" id="CR9">Aussal, M., Alouges, F., Katz, B. F.: ITD Interpolation and Personalization for Binaural Synthesis Using Spherical Harmonics in Audio Eng Soc UK Conf (York, UK, Mar. 2012), 04:01–10.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">10.</div><div class="CitationContent" id="CR10">Bahu, H.: Localisation auditive en contexte de synthèse binaurale nonindividuelle PhD thesis (Université Pierre et Marie Curie-Paris VI, 2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">11.</div><div class="CitationContent" id="CR11">Bahu, H., Carpentier, T., Noisternig, M.,Warusfel, O.: Comparison of different egocentric pointing methods for 3D sound localization experiments. Acta Acust <strong class="EmphasisTypeBold ">102</strong>, 107–118 (2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">12.</div><div class="CitationContent" id="CR12">Batteau, D. W.: The role of the pinna in human localization. Proceedings of the Royal Society of London. Series B. Biological Sciences <strong class="EmphasisTypeBold ">168</strong>, 158–180 (1967).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">13.</div><div class="CitationContent" id="CR13">Baumgartner, R., Majdak, P., Laback, B.: Modeling sound-source localization in sagittal planes for human listeners. J Acoust Soc Am <strong class="EmphasisTypeBold ">136</strong>, 791–802 (2014).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">14.</div><div class="CitationContent" id="CR14">Begault, D. R.: 3-D Sound for Virtual Reality and Multimedia (Academic Press, Cambridge, 1994).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">15.</div><div class="CitationContent" id="CR15">Blum, A., Katz, B., Warusfel, O.: Eliciting adaptation to non-individual HRTF spectral cues with multi-modal training in 7ème Cong de la Soc Française d’Acoustique et 30ème congrès de la Soc Allemande d’Acoustique (CFA/DAGA) (Strasbourg, 2004), 1225–1226.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">16.</div><div class="CitationContent" id="CR16">Bouchara, T., Bara, T.-G., Weiss, P.-L., Guilbert, A.: Influence of vision on short-termsound localization training with non-individualized HRTF in EAA Spatial Audio Signal Processing Symp (2019), 55–60.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">17.</div><div class="CitationContent" id="CR17">Brinkmann, F., Weinzierl, S.: Comparison of Head-Related Transfer Functions Pre-Processing Techniques for Spherical Harmonics Decomposition English. in (Audio Engineering Society, Aug. 2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">18.</div><div class="CitationContent" id="CR18">Brinkmann, F. et al.: A cross-evaluated database of measured and simulated HRTFs including 3D head meshes, anthropometric features, and headphone impulse responses. J Audio Eng Soc <strong class="EmphasisTypeBold ">67</strong>, 705–718 (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">19.</div><div class="CitationContent" id="CR19">Carlile, S., Balachandar, K., Kelly, H.: Accommodating to new ears: the effects of sensory and sensory-motor feedback. J Acous Soc America <strong class="EmphasisTypeBold ">135</strong>, 2002–2011 (2014).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">20.</div><div class="CitationContent" id="CR20">Carlile, S., Leong, P., Hyams, S.: The nature and distribution of errors in sound localization by human listeners. Hearing Research <strong class="EmphasisTypeBold ">114</strong>, 179–196 (1997).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">21.</div><div class="CitationContent" id="CR21">Cuevas-Rodriguez, M., Gonzalez-Toledo, D., Reyes-Lecuona, A., Picinali, L.: Impact of non-individualised head related transfer functions on speechin- noise performances within a synthesised virtual environment. The JAcoust Soc Am <strong class="EmphasisTypeBold ">149</strong>, 2573–2586 (2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">22.</div><div class="CitationContent" id="CR22">Cuevas-Rodríguez, M. et al.: 3D Tune-In Toolkit: An open-source library for real-time binaural spatialisation. PloS one <strong class="EmphasisTypeBold ">14</strong>, e0211899 (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">23.</div><div class="CitationContent" id="CR23">Deng, Y., Choi, I., Shinn-Cunningham, B., Baumgartner, R.: Impoverished auditory cues limit engagement of brain networks controlling spatial selective attention. NeuroImage <strong class="EmphasisTypeBold ">202</strong>, 116151 (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">24.</div><div class="CitationContent" id="CR24">Dramas, F., Katz, B., Jouffrais, C.: Auditory-guided reaching movements in the peripersonal frontal space in Acoustics’08. 9e Congrèès Français d’Acoustique of the SFA. <strong class="EmphasisTypeBold ">123</strong> (Acoustical Society of America, 2008), 3723.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">25.</div><div class="CitationContent" id="CR25">Engel, I., Goodman, D. F. M., Picinali, L.: Assessing HRTF preprocessing methods for Ambisonics rendering through perceptual models. en. Acta Acustica <strong class="EmphasisTypeBold ">6</strong>, 4 (2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">26.</div><div class="CitationContent" id="CR26">Genuit, K.: A model for the description of outer-ear transmission characteristics PhD thesis (Rhenish-Westphalian Technical University, Düsseldorf, 1984), 220.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">27.</div><div class="CitationContent" id="CR27">Geronazzo, M., Peruch, E., Prandoni, F.,Avanzini, F.: Applying a single-notch metric to image-guided head-related transfer function selection for improved vertical localization. Journal of the Audio Engineering Society <strong class="EmphasisTypeBold ">67</strong>, 414–428 (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">28.</div><div class="CitationContent" id="CR28">Geronazzo, M., Spagnol, S., Avanzini, F.: Mixed structural modeling of headrelated transfer functions for customized binaural audio delivery in 2013 18th International Conference on Digital Signal Processing (DSP) (2013), 1–8.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">29.</div><div class="CitationContent" id="CR29">Geronazzo, M., Spagnol, S., Avanzini, F.: Do we need individual head-related transfer functions for vertical localization? The case study of a spectral notch distance metric. IEEE/ACM Transactions on Audio, Speech, and Language Processing <strong class="EmphasisTypeBold ">26</strong>, 1247–1260 (2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">30.</div><div class="CitationContent" id="CR30">Geronazzo, M., Spagnol, S., Bedin, A.,Avanzini, F.: Enhancing vertical localization with image-guided selection of non-individual head-related transfer functions in 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (2014), 4463–4467.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">31.</div><div class="CitationContent" id="CR31">Greff, R., Katz, B.: Perceptual evaluation of HRTF notches versus peaks for vertical localisation in Intl Cong on Acoustics <strong class="EmphasisTypeBold ">19</strong> (Madrid, Spain, 2007), 1–6.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">32.</div><div class="CitationContent" id="CR32">Greff, R., Katz, B.: Round Robin comparison of HRTF simulation results : preliminary results. in Audio Eng Soc Conv <strong class="EmphasisTypeBold ">123</strong> (New York, USA, 2007), 1–5.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">33.</div><div class="CitationContent" id="CR33">Grijalva, F., Martini, L., Florencio, D., Goldenstein, S.: A manifold learning approach for personalizing HRTFs from anthropometric features. IEEE/ACM Transactions on Audio, Speech, and Language Processing <strong class="EmphasisTypeBold ">24</strong>, 559–570 (2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">34.</div><div class="CitationContent" id="CR34">Guezenoc, C., Seguier, R.: A wide dataset of ear shapes and pinna-related transfer functions generated by random ear drawings. J Acoust Soc Am <strong class="EmphasisTypeBold ">147</strong>, 4087–4096 (2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">35.</div><div class="CitationContent" id="CR35">Gumerov, N. A., O’Donovan, A. E., Duraiswami, R., Zotkin, D. N.: Computation of the head-related transfer function via the fast multipole accelerated boundary element method and its spherical harmonic representation. JAcoust Soc Am <strong class="EmphasisTypeBold ">127</strong>, 370–386 (2010).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">36.</div><div class="CitationContent" id="CR36">Hamari, J., Koivisto, J., Sarsa, H.: Does gamification work? A literature review of empirical studies on gamification in Intl Conf on System Sciences (2014), 3025–3034.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">37.</div><div class="CitationContent" id="CR37">Hendrickx, E. et al.: Influence of head tracking on the externalization of speech stimuli for non-individualized binaural synthesis. J Acoust Soc Am <strong class="EmphasisTypeBold ">141</strong>, 2011–2023 (2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">38.</div><div class="CitationContent" id="CR38">Hofman, P. M., Van Riswick, J. G., Van Opstal, A. J.: Relearning sound localization with new ears. Nature Neuroscience <strong class="EmphasisTypeBold ">1</strong>, 417–421 (1998).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">39.</div><div class="CitationContent" id="CR39">Honda, A. et al.: Transfer effects on sound localization performances from playing a virtual three-dimensional auditory game. Applied Acoustics <strong class="EmphasisTypeBold ">68</strong>, 885–896 (2007).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">40.</div><div class="CitationContent" id="CR40">Hwang, S., Park, Y., Park, Y.-s.: Modeling and customization of head-related impulse responses based on general basis functions in time domain. Acta Acustica united with Acustica <strong class="EmphasisTypeBold ">94</strong>, 965–980 (2008).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">41.</div><div class="CitationContent" id="CR41">Iwaya,Y.: Individualization of head-related transfer functions with tournamentstyle listening test: Listening with other’s ears. Acoustical Science &amp; Technology <strong class="EmphasisTypeBold ">27</strong>, 340–343 (2006).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">42.</div><div class="CitationContent" id="CR42">Jin, C. T. et al.: Creating the Sydney York morphological and acoustic recordings of ears database. IEEE Transactions on Multimedia <strong class="EmphasisTypeBold ">16</strong>, 37–46 (2013).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">43.</div><div class="CitationContent" id="CR43">Jost, T. A., Nelson, B., Rylander, J.: Quantitative analysis of the Oculus Rift S in controlled movement. Disability and Rehabilitation: Assistive Technology, 1–5 (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">44.</div><div class="CitationContent" id="CR44">Kahana, Y.: Numerical modelling of the head-related transfer function PhD thesis (University of Southampton, 2000).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">45.</div><div class="CitationContent" id="CR45">Kahana, Y., Nelson, P. A.: Boundary element simulations of the transfer function of human heads and baffled pinnae using accurate geometric models. Journal of Sound and Vibration <strong class="EmphasisTypeBold ">300</strong>, 552–579 (2007).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">46.</div><div class="CitationContent" id="CR46">Kahana, Y., Nelson, P. A., Petyt, M., Choi, S.: Boundary element simulation of HRTFs and sound fields produced by virtual acoustic imaging systems in Audio Engineering Society Convention 105 (1998).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">47.</div><div class="CitationContent" id="CR47">Katz, B., Begault, D.: Round robin comparison of HRTF measurement systems : preliminary results. in Intl Cong on Acoustics <strong class="EmphasisTypeBold ">19</strong> (Madrid, Spain, 2007), 1–6.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">48.</div><div class="CitationContent" id="CR48">Katz, B., Nicol, R. in Sensory Evaluation of Sound (ed Zacharov, N.) 349–388 (CRC Press, Boca Raton, 2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">49.</div><div class="CitationContent" id="CR49">Katz, B. F. G.: Measurement and Calculation of Individual Head-Related Transfer Functions Using a Boundary Element Model Including the Measurement and Effect of Skin and Hair Impedance PhD thesis (The Pennsylvania State University, 1998).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">50.</div><div class="CitationContent" id="CR50">Katz, B. F. G., Noisternig, M.: A comparative study of interaural time delay estimation methods. J Acoust Soc Am <strong class="EmphasisTypeBold ">135</strong>, 3530–3540 (2014).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">51.</div><div class="CitationContent" id="CR51">Katz, B. F.: Boundary element method calculation of individual head-related transfer function. I. Rigid model calculation. J Acoust Soc Am <strong class="EmphasisTypeBold ">110</strong>, 2440–2448 (2001).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">52.</div><div class="CitationContent" id="CR52">Katz, B. F.: Boundary element method calculation of individual head-related transfer function. II. Impedance effects and comparisons to real measurements. J Acoust Soc Am <strong class="EmphasisTypeBold ">110</strong>, 2449–2455 (2001).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">53.</div><div class="CitationContent" id="CR53">Katz, B. F., Parseihian, G.: Perceptually based head-related transfer function database optimization. J Acoust Soc Am <strong class="EmphasisTypeBold ">131</strong>, EL99–EL105 (2012).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">54.</div><div class="CitationContent" id="CR54">Kim, C., Lim, V., Picinali, L.: Investigation Into Consistency of Subjective and Objective Perceptual Selection of Non-individual Head-Related Transfer Functions. J Audio Eng Soc <strong class="EmphasisTypeBold ">68</strong>, 819–831 (2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">55.</div><div class="CitationContent" id="CR55">Kreuzer, W., Majdak, P., Chen, Z.: Fast multipole boundary element method to calculate head-related transfer functions for a wide frequency range. J Acoust Soc Am <strong class="EmphasisTypeBold ">126</strong>, 1280–1290 (2009).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">56.</div><div class="CitationContent" id="CR56">Kreuzer, W., Majdak, P., Haider, A.: A boundary element model to calculate HRTFs. Comparison between calculated and measured data in Proceedings of the NAG-DAGA International Conference 2009 (2009), 196–199.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">57.</div><div class="CitationContent" id="CR57">Kumpik, D. P., Kacelnik, O., King, A. J.: Adaptive reweighting of auditory localization cues in response to chronic unilateral earplugging in humans. J of Neuroscience <strong class="EmphasisTypeBold ">30</strong>, 4883–4894 (2010).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">58.</div><div class="CitationContent" id="CR58">Lopez-Poveda, E. A., Meddis, R.: A physical model of sound diffraction and reflections in the human concha. J Acoust Soc Am <strong class="EmphasisTypeBold ">100</strong>, 3248–3259 (1996).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">59.</div><div class="CitationContent" id="CR59">Majdak, P., Goupell, M. J., Laback, B.: 3-D localization of virtual sound sources: Effects of visual environment, pointing method, and training. Attention, Perception, &amp; Psychophysics <strong class="EmphasisTypeBold ">72</strong>, 454–469 (2010).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">60.</div><div class="CitationContent" id="CR60">Majdak, P., Walder, T., Laback, B.: Effect of long-term training on sound localization performance with spectrally warped and band-limited head-related transfer functions. J Acous Soc America <strong class="EmphasisTypeBold ">134</strong>, 2148–2159 (2013).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">61.</div><div class="CitationContent" id="CR61">Mendonça, C.: A review on auditory space adaptations to altered head-related cues. Frontiers in Neuroscience <strong class="EmphasisTypeBold ">8</strong>, 219:1–14 (2014).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">62.</div><div class="CitationContent" id="CR62">Mendonça, C., Campos, G., Dias, P., Santos, J. A.: Learning auditory space: Generalization and long-term effects. PloS One <strong class="EmphasisTypeBold ">8</strong>, 1–14 (2013).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">63.</div><div class="CitationContent" id="CR63">Mendonça, C. et al.: On the improvement of localization accuracy with nonindividualized HRTF-based sounds. J Audio Eng Soc <strong class="EmphasisTypeBold ">60</strong>, 821–830 (2012).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">64.</div><div class="CitationContent" id="CR64">Middlebrooks, J. C.: Individual differences in external-ear transfer functions reduced by scaling in frequency. J Acoust Soc Am <strong class="EmphasisTypeBold ">106</strong>, 1480–1492 (1999).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">65.</div><div class="CitationContent" id="CR65">Middlebrooks, J. C., Macpherson, E. A., Onsan, Z. A.: Psychophysical customization of directional transfer functions for virtual sound localization. J Acoust Soc Am <strong class="EmphasisTypeBold ">108</strong>, 3088–3091 (2000).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">66.</div><div class="CitationContent" id="CR66">Molloy, K., Moore, D. R., Sohoglu, E., Amitay, S.: Less is more: latent learning is maximized by shorter training sessions in auditory perceptual learning. PloS One <strong class="EmphasisTypeBold ">7</strong>, 1–13 (2012).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">67.</div><div class="CitationContent" id="CR67">Morimoto, M., Aokata, H.: Localization cues of sound sources in the upper hemisphere. J Acous Soc Japan <strong class="EmphasisTypeBold ">5</strong>, 165–173 (1984).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">68.</div><div class="CitationContent" id="CR68">Nicol, R.: Binaural Technology 77 (Audio Engineering Society, New York 2010).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">69.</div><div class="CitationContent" id="CR69">Ospina, F. R., Emerit, M., Katz, B. F.: The 3D morphological database for spatial hearing research of the BiLi project in Proc. of Meetings on Acoustics <strong class="EmphasisTypeBold ">23</strong> (Pittsburg, May 2015), 1–17.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">70.</div><div class="CitationContent" id="CR70">Otani, M., Ise, S.: Fast calculation system specialized for head-related transfer function based on boundary element method. J Acoust Soc Am <strong class="EmphasisTypeBold ">119</strong>, 2589–2598 (2006).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">71.</div><div class="CitationContent" id="CR71">Parseihian, G., Katz, B., Conan, S.: Sound effect metaphors for near field distance sonification in Intl Conf on Auditory Display (Atlanta, June 2012), 6–13.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">72.</div><div class="CitationContent" id="CR72">Parseihian, G., Katz, B. F. G.: Morphocons: A New Sonification Concept Based on Morphological Earcons. J Audio Eng Soc <strong class="EmphasisTypeBold ">60</strong>, 409–418 (2012).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">73.</div><div class="CitationContent" id="CR73">Parseihian, G., Katz, B. F. G.: Rapid head-related transfer function adaptation using a virtual auditory environment. J Acous Soc America <strong class="EmphasisTypeBold ">131</strong>, 2948–2957 (2012).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">74.</div><div class="CitationContent" id="CR74">Picinali, L., Afonso, A., Denis, M., Katz, B. F.: Exploration of architectural spaces by blind people using auditory virtual reality for the construction of spatial knowledge. International Journal of Human-Computer Studies <strong class="EmphasisTypeBold ">72</strong>, 393–407 (2014).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">75.</div><div class="CitationContent" id="CR75">Poirier-Quinot, D., Stitt, P., Katz, B. in Advances in Fundamental and Applied Research on Spatial Audio (eds Katz, B., Majdak, P.) (InTech, 2022).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">76.</div><div class="CitationContent" id="CR76">Poirier-Quinot, D., Katz, B. F.: Assessing the impact of Head-Related Transfer Function individualization on task performance: Case of a virtual reality shooter game. J. Audio Eng. Soc <strong class="EmphasisTypeBold ">68</strong>, 248–260 (2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">77.</div><div class="CitationContent" id="CR77">Poirier-Quinot, D., Katz, B. F.: On the improvement of accommodation to non-individual HRTFs via VR active learning and inclusion of a 3D room response. Acta Acustica <strong class="EmphasisTypeBold ">5</strong>, 1–17 (2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">78.</div><div class="CitationContent" id="CR78">Poirier-Quinot, D., Katz, B. F.: The Anaglyph binaural audio engine in Audio Engineering Society Convention 144 (2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">79.</div><div class="CitationContent" id="CR79">Poirier-Quinot, D., Parseihian, G., Katz, B. F.: Comparative study on the effect of Parameter Mapping Sonification on perceived instabilities, efficiency, and accuracy in real-time interactive exploration of noisy data streams. Displays <strong class="EmphasisTypeBold ">47</strong>, 2–11 (2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">80.</div><div class="CitationContent" id="CR80">Reichinger, A., Majdak, P., Sablatnig, R., Maierhofer, S.: Evaluation of methods for optical 3-D scanning of human pinnas in 2013 International Conference on 3D Vision-3DV 2013 (2013), 390–397.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">81.</div><div class="CitationContent" id="CR81">Schonstein, D., Katz, B. F.: HRTF selection for binaural synthesis from a database using morphological parameters in International Congress on Acoustics (ICA) (2010).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">82.</div><div class="CitationContent" id="CR82">Schönstein, D., Katz, B. F.: Variability in perceptual evaluation of HRTFs. Journal of the Audio Engineering Society <strong class="EmphasisTypeBold ">60</strong>, 783–793 (2012).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">83.</div><div class="CitationContent" id="CR83">Seeber, B. U., Fastl, H.: Subjective selection of non-individual head-related transfer functions in Proceedings of the 2003 Intl Conf on Auditory Display (ICAD) (2003), 259–262.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">84.</div><div class="CitationContent" id="CR84">Shin, K. H., Park, Y.: Enhanced vertical perception through head-related impulse response customization based on pinna response tuning in the median plane. IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences <strong class="EmphasisTypeBold ">91</strong>, 345–356 (2008).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">85.</div><div class="CitationContent" id="CR85">Shukla, R., Stewart, R., Roginska, A., Sandler, M.: User selection of optimal HRTF sets via holistic comparative evaluation in the Audio Engineering Society Conference on Audio for Virtual and Augmented Reality (AVAR) 2018 (Audio Engineering Society, Redmond, WA, USA, 2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">86.</div><div class="CitationContent" id="CR86">Silzle, A.: Selection and tuning of HRTFs in Audio Eng Soc Conv 112 (2002), 1–14.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">87.</div><div class="CitationContent" id="CR87">Simon, L., Zacharov, N., Katz, B. F. G.: Perceptual attributes for the comparison of Head-Related Transfer Functions. J Acous Soc America <strong class="EmphasisTypeBold ">140</strong>, 3623–3632 (Nov. 2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">88.</div><div class="CitationContent" id="CR88">Søndergaard, P., Majdak, P. in The Technology of Binaural Listening (ed Blauert, J.) 33–56 (Springer, Berlin, Heidelberg, 2013).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">89.</div><div class="CitationContent" id="CR89">Spagnol, S., Geronazzo, M., Avanzini, F.: On the relation between pinna reflection patterns and head-related transfer function features. IEEE transactions on audio, speech, and language processing <strong class="EmphasisTypeBold ">21</strong>, 508–519 (2012).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">90.</div><div class="CitationContent" id="CR90">Steadman, M. A., Kim, C., Lestang, J.-H., Goodman, D. F., Picinali, L.: Short-term effects of sound localization training in virtual reality. Scientific Reports <strong class="EmphasisTypeBold ">9</strong>, 1–17 (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">91.</div><div class="CitationContent" id="CR91">Stitt, P., Katz, B. F.: Sensitivity analysis of pinna morphology on head-related transfer functions simulated via a parametric pinna model. J Acoust Soc Am <strong class="EmphasisTypeBold ">149</strong>, 2559–2572 (2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">92.</div><div class="CitationContent" id="CR92">Stitt, P., Picinali, L., Katz, B. F. G.: Auditory Accommodation to poorly MatchedNon-Individual spectral Localization Cues throughActive Learning. Scientific Reports <strong class="EmphasisTypeBold ">9</strong>, 1063:1–14 (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">93.</div><div class="CitationContent" id="CR93">Teranishi, R., Shaw, E. A.: External-Ear Acoustic Models with Simple Geometry. J Acoust Soc Am <strong class="EmphasisTypeBold ">44</strong>, 257–263 (1968).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">94.</div><div class="CitationContent" id="CR94">Trapeau, R., Aubrais, V., Schönwiesner, M.: Fast and persistent adaptation to new spectral cues for sound localization suggests a many-to-one mapping mechanism. J Acous Soc America <strong class="EmphasisTypeBold ">140</strong>, 879–890 (2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">95.</div><div class="CitationContent" id="CR95">Van Wanrooij, M. M., Van Opstal, A. J.: Relearning sound localization with a new ear. J of Neuroscience <strong class="EmphasisTypeBold ">25</strong>, 5413–5424 (2005).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">96.</div><div class="CitationContent" id="CR96">Voong, T. M., Oehler, M.: Tournament Formats as Method for Determining Best-fitting HRTF Profiles for Individuals wearing Bone Conduction Headphones in Proceedings of the 23rd International Congress on Acoustics : integrating 4th EAA Euroregio 2019 : 9–13 September 2019 in Aachen, Germany (eds Ochmann, M., Vorländer, M., Fels, J.) (Berlin, Germany, Sept. 9, 2019), 4841–4847.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">97.</div><div class="CitationContent" id="CR97">Wan, Y., Zare, A., McMullen, K.: Evaluating the consistency of subjectively selected head-related transfer functions (HRTFs) over time in Audio Engineering Society Conference: 55th International Conference: Spatial Audio (2014).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">98.</div><div class="CitationContent" id="CR98">Warusfel, O.: IRCAM Listen HRTF database <span class="ExternalRef"><a href="http://recherche.ircam.fr/equipes/salles/listen"><span class="RefSource">http://​recherche.​ircam.​fr/​equipes/​salles/​listen</span></a></span>. 2003.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">99.</div><div class="CitationContent" id="CR99">Wenzel, E. M., Arruda, M., Kistler, D. J.,Wightman, F. L.: Localization using nonindividualized head-related transfer functions. J Acous Soc America <strong class="EmphasisTypeBold ">94</strong>, 111–123 (1993).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">100.</div><div class="CitationContent" id="CR100">Whitton, J. P., Hancock, K. E., Shannon, J. M., Polley, D. B.: Audiomotor perceptual training enhances speech intelligibility in background noise. Current Biology <strong class="EmphasisTypeBold ">27</strong>, 3237–3247 (2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">101.</div><div class="CitationContent" id="CR101">Wightman, F. L., Kistler, D. J.: The dominant role of low-frequency interaural time differences in sound localization. J Acoust SocAm <strong class="EmphasisTypeBold ">91</strong>, 1648–1661 (Mar. 1992).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">102.</div><div class="CitationContent" id="CR102">Wightman, F. L., Kistler, D. J.: Resolution of front-back ambiguity in spatial hearing by listener and source movement. J Acoust Soc Am <strong class="EmphasisTypeBold ">105</strong>, 2841–2853 (1999).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">103.</div><div class="CitationContent" id="CR103">Winn, M. B., Wendt, D., Koelewijn, T., Kuchinsky, S. E.: Best practices and advice for using pupillometry to measure listening effort: An introduction for those who want to get started. Trends in hearing <strong class="EmphasisTypeBold ">22</strong>, 1–32 (2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">104.</div><div class="CitationContent" id="CR104">Wright, B. A., Zhang, Y.: A review of learning with normal and altered sound-localization cues in human adults. Intl J of Audiology <strong class="EmphasisTypeBold ">45</strong>, 92–98 (2006).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">105.</div><div class="CitationContent" id="CR105">Xie, B.: Head-Related Transfer Functions and Virtual Auditory Display 2nd ed. (J. Ross Publishing, Plantation, FL, USA, 2013).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">106.</div><div class="CitationContent" id="CR106">Yairi, S., Iwaya,Y.,Yôiti, S.: Individualization feature of head-related transfer functions based on subjective evaluation in 14th Intl Conf onAuditory Display (Paris, 2008).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">107.</div><div class="CitationContent" id="CR107">Zacharov, N., Lorho, G.: What are the requirements of a listening panel for evaluating spatial audio quality? in Proc. Int.Workshop on Spatial Audio and Sensory Evaluation Techniques (2006).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">108.</div><div class="CitationContent" id="CR108">Zagala, F., Noisternig, M., Katz, B. F.: Comparison of direct and indirect perceptual head-related transfer function selection methods. J Acoust Soc Am <strong class="EmphasisTypeBold ">147</strong>, 3376–3389 (2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">109.</div><div class="CitationContent" id="CR109">Zahorik, P., Bangayan, P., Sundareswaran, V.,Wang, K., Tam, C.: Perceptual recalibration in human sound localization: Learning to remediate front-back reversals. J Acous Soc America <strong class="EmphasisTypeBold ">120</strong>, 343–359 (2006).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">110.</div><div class="CitationContent" id="CR110">Ziegelwanger, H., Kreuzer, W., Majdak, P.: Mesh2HRTF: An open-source software package for the numerical calculation of head-related transfer functions in 22nd International Congress on Sound and Vibration (2015).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">111.</div><div class="CitationContent" id="CR111">Ziegelwanger, H., Majdak, P., Kreuzer,W.: Numerical calculation of listenerspecific head-related transfer functions and sound localization: Microphone model and mesh discretization. J Acoust Soc Am <strong class="EmphasisTypeBold ">138</strong>, 208–222 (2015).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">112.</div><div class="CitationContent" id="CR112">Zotkin, D., Hwang, J., Duraiswaini, R., Davis, L. S.: HRTF personalization using anthropometric measurements in 2003 IEEEWorkshop on Applications of Signal Processing to Audio and Acoustics (IEEE Cat. No. 03TH8684) (2003), 157–160.</div></li></ol></div></aside><aside aria-label="Footnotes" class="FootnoteSection" epub:type="footnotes"><div class="Heading">Footnotes</div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn1_source">1</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn1" role="doc-footnote"><p class="Para" id="Par3">We use the term <em class="EmphasisTypeItalic ">HRTF</em> to indicate the set of filters, each representing a pair of transfer functions from a point source in space at a given distance around a given head to the left and right ear, normalised by the transfer function with the body absent. The plural, <em class="EmphasisTypeItalic ">HRTFs</em>, therefore, represents a collection of more than one HRTF, typically for different heads or test conditions. The head-related impulse response or <em class="EmphasisTypeItalic ">HRIR</em> is the time domain transform of the HRTF.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn2_source">2</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn2" role="doc-footnote"><p class="Para" id="Par27">The diffuse field component is the spatial average of the HRTF. When removed from the HRTF, the result is a diffuse field equalised <em class="EmphasisTypeItalic "><span id="ITerm26">directional transfer function</span>
</em> (DTF) [<span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>].</p></div><div class="ClearBoth"> </div></div></aside></div></div></body></html>