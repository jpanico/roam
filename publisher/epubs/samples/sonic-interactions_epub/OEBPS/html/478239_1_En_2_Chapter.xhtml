<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops"><head><title>Procedural Modeling of Interactive Sound Sources in Virtual Reality</title><meta content="text/html; charset=utf-8" http-equiv="content-type"/><link href="../css/springer_epub.css" rel="styleSheet" type="text/css"/></head><body><div epub:type="chapter" role="doc-chapter"><div class="ChapterContextInformation"><div class="ContextInformation" id="Chap2"><div class="ChapterCopyright">© The Author(s) 2023</div><span class="ContextInformationAuthorEditorNames">M. Geronazzo, S. Serafin<span class="CollaboratorDesignation"> (eds.)</span></span><span class="ContextInformationBookTitles"><span class="BookTitle">Sonic Interactions in Virtual Environments</span></span><span class="ContextInformationSeries"><span class="SeriesTitle" lang="en">Human–Computer Interaction Series</span></span><span class="ChapterDOI"><a href="https://doi.org/10.1007/978-3-031-04021-4_2">https://doi.org/10.1007/978-3-031-04021-4_2</a></span></div></div><!--Begin Abstract--><div class="MainTitleSection"><h1 class="ChapterTitle" lang="en">2. Procedural Modeling of Interactive Sound Sources in Virtual Reality</h1></div><div class="AuthorGroup"><div class="AuthorNames"><span class="Author"><span class="AuthorName">Federico Avanzini</span><sup><a href="#Aff34">1</a> <a aria-label="Contact information for this author" href="#ContactOfAuthor1"><span class="ContactIcon"> </span></a></sup></span></div><div class="Affiliations"><div class="Affiliation" id="Aff34"><span class="AffiliationNumber">(1)</span><div class="AffiliationText">Laboratory of Music Informatics, Department of Computer Science, University of Milano, Via G. Celoria 18, IT-20135 Milano, Italy</div></div><div class="ClearBoth"> </div></div><div class="Contacts"><div class="Contact" id="ContactOfAuthor1"><div class="ContactIcon"> </div><div class="ContactAuthorLine"><span class="AuthorName">Federico Avanzini</span></div><div class="ContactAdditionalLine"><span class="ContactType">Email: </span><a href="mailto:federico.avanzini@di.unimi.it">federico.avanzini@di.unimi.it</a></div></div></div></div><section class="Abstract" id="Abs1" lang="en" role="doc-abstract"><h2 class="Heading">Abstract</h2><p class="Para" id="Par1">This chapter addresses the first building block of sonic interactions in virtual environments, i.e., the modeling and synthesis of sound sources. Our main focus is on procedural approaches, which strive to gain recognition in commercial applications and in the overall sound design workflow, firmly grounded in the use of samples and event-based logics. Special emphasis is placed on physics-based sound synthesis methods and their potential for improved interactivity. The chapter starts with a discussion of the categories, functions, and affordances of sounds that we listen to and interact with in real and virtual environments. We then address perceptual and cognitive aspects, with the aim of emphasizing the relevance of sound source modeling with respect to the senses of presence and embodiment of a user in a virtual environment. Next, procedural approaches are presented and compared to sample-based approaches, in terms of models, methods, and computational costs. Finally, we analyze the state of the art in current uses of these approaches for Virtual Reality applications.</p></section><!--End Abstract--><div class="Fulltext"><section class="Section1 RenderAsSection1" id="Sec1"><h2 class="Heading"><span class="HeadingNumber">2.1 </span>Introduction</h2><div class="Para" id="Par2">Takala and Hahn [<span class="CitationRef"><a epub:type="biblioref" href="#CR86" role="doc-biblioref">86</a></span>] were possibly the first scholars who proposed a sound rendering pipeline, in analogy with the image rendering pipeline, aimed at producing an overall “soundtrack” starting from a description of the objects in an audio-visual scene. Their pipeline included sound modeling and sound rendering stages, running in parallel with the image rendering pipeline. Figure <span class="InternalRef"><a href="#Fig1">2.1</a></span> proposes an updated picture, which considers several aspects investigated by researchers throughout the last three decades and may represent a general pipeline for sound simulation in Virtual Reality (hereinafter, VR).<figure class="Figure" id="Fig1"><div class="MediaObject" id="MO1"><img alt="" src="../images/478239_1_En_2_Chapter/478239_1_En_2_Fig1_HTML.png" style="width:33.95em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.1</span><p class="SimplePara">A general pipeline for sound simulation in Virtual Reality</p><div class="Credit"><p class="SimplePara">(figure based on [<span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>])</p></div></div></figcaption></figure>
</div><p class="Para" id="Par3">Much of recent and current research is concerned with aspects related to the “Propagation” and “Rendering” blocks represented in this figure, as well as the geometrical and material properties of acoustic enclosures in the “Modeling” block. This chapter focuses instead on the remaining balloon of the “Modeling” block, the modeling of <em class="EmphasisTypeItalic ">sound sources</em>.</p><p class="Para" id="Par4">One obvious motivation for looking into sound source <span id="ITerm1">modeling</span> is that all sounds occurring in a virtual (and in a real) environment originate from some sources, before propagating into the environment and finally reaching the listener. Secondly, many of the sonic interactions occurring in a virtual environments are interactions between the subject’s avatar and sound sources. Here, our definition of <em class="EmphasisTypeItalic ">interactive</em> is analogous to the one given by Collins [<span class="CitationRef"><a epub:type="biblioref" href="#CR20" role="doc-biblioref">20</a></span>] for video-game audio: whereas adaptive audio generically refers to audio that reacts appropriately to events and changes occurring in the simulation, interactive audio refers to sound events occurring directly in reaction to avatar’s gestures (ranging from pressing a button to walking or hitting objects in the virtual scene).</p><p class="Para" id="Par5">The current dominant paradigm in VR audio, largely based on sound samples<sup><a epub:type="noteref" href="#Fn1" id="Fn1_source" role="doc-noteref">1</a></sup> triggered by specific events generated by the avatar or the simulation, is minimally adaptive and interactive. This is the main motivation for looking into <em class="EmphasisTypeItalic ">procedural</em> approaches to sound generation.</p></section><section class="Section1 RenderAsSection1" id="Sec2"><h2 class="Heading"><span class="HeadingNumber">2.2 </span>What to Model</h2><p class="Para" id="Par7">The first question that should be asked is as follows: what are the sound sources that need to be modeled in a virtual environment, and how can these be organized into a coherent and comprehensive taxonomy? Such a taxonomy would provide a useful tool to analyze in a systematic way the state of the art of the research in this field and possibly to spot research directions that are still under-explored.</p><section class="Section2 RenderAsSection2" id="Sec3"><h3 class="Heading"><span class="HeadingNumber">2.2.1 </span>Diegetic Sounds</h3><p class="Para" id="Par8">One first possible and often used distinction can be mutated from narrative theory. The term <em class="EmphasisTypeItalic ">diegesis</em> has been used in film theory to refer to the fictional world of the film story, and correspondingly the adjective <em class="EmphasisTypeItalic ">diegetic</em> refers to elements that are part of the depicted fictional world. By contrast, non-diegetic elements are those which should be considered non-existent in the fictional world.</p><p class="Para" id="Par9">As far as sound in particular is concerned, three main categories are traditionally used in films: speech and dialogue, sound effects, and music [<span class="CitationRef"><a epub:type="biblioref" href="#CR80" role="doc-biblioref">80</a></span>]. The first two categories comprise diegetic <span id="ITerm2">sounds</span>, while music is a non-diegetic element having mostly an affective and emotional role, a distinction that may be related to the motto “Sound effects make it real, music makes you feel” [<span class="CitationRef"><a epub:type="biblioref" href="#CR49" role="doc-biblioref">49</a></span>].</p><p class="Para" id="Par10">Several taxonomies for sounds in video-<span id="ITerm3">games</span> have been proposed and are typically based on similar categories [<span class="CitationRef"><a epub:type="biblioref" href="#CR42" role="doc-biblioref">42</a></span>]. These may be employed in the context of VR as well, with the additional caveat that VR applications only partly overlap with video-games. In particular, VR, and immersive VR specifically, may be defined as “a medium in which people respond with their whole bodies, treating what they perceive as real” [<span class="CitationRef"><a epub:type="biblioref" href="#CR77" role="doc-biblioref">77</a></span>]. In light of this definition, in this chapter, we focus on diegetic sounds, those that “make it real”: in other words, those that contribute most to the overall sense of the presence of a user within a virtual environment, which we will discuss in Sect. <span class="InternalRef"><a href="#Sec5">2.3</a></span>.</p><p class="Para" id="Par11">An interesting example of a taxonomy for sound in games is provided by Stockburger [<span class="CitationRef"><a epub:type="biblioref" href="#CR84" role="doc-biblioref">84</a></span>], who considers five different types of sound objects. Non-diegetic elements include (i) music, but also (ii) interface sounds, which may sometimes be included into the diegetic part of the game environment; proper diegetic elements instead comprise the three categories of (iii) speech and dialogue, (iv) ambience (or “zone” sounds in Stockburger’s definition), and (v) effects.</p><p class="Para" id="Par12">Speech and dialogue are very relevant components of a virtual environment; however, our focus in this chapter is on non-verbal sound. The distinction between ambience and effect sounds is mainly a perspectival one: the former are background sounds, connected to locations or zones (understood both as different spatial locations in an environment and different levels in a game) and having distinct auditory qualities; the latter are instead foreground sounds other than speech, that are cognitively linked to objects or events, and are therefore perceived as being produced by such objects and events. Sound-producing objects may be moving or static elements, may be directly interactable by the avatar or just synchronized to the visual simulation, or may be even outside the visual field of view.</p><div class="Para" id="Par13">Stockburger [<span class="CitationRef"><a epub:type="biblioref" href="#CR84" role="doc-biblioref">84</a></span>] proceeds in distinguishing effect subcategories, depending on the elements of the environment they are linked to. His classification is heavily tailored to games, but serves as an inspiration to further inspect and subdivide effect sounds. For the purpose of the present discussion, we only make a distinction between two subcategories: (i) effects linked to the avatar, and (ii) all remaining effects in the environment. Effects linked to the avatar are related to sounds produced by the avatar’s movement or object manipulation: footsteps, swishing of an object cutting through the air, knocking on a wall, clothes, etc. They can also include sounds produced by the avatar’s own body, such as breathing or scratching. The remaining effects in the environment may include non-verbal human sounds, sounds produced by human activities, machine sounds, and so on. A visual summary is provided in Fig. <span class="InternalRef"><a href="#Fig2">2.2</a></span>. The categories and subcategories identified here can be usefully mapped into interactive and adaptive sound sources.<figure class="Figure" id="Fig2"><div class="MediaObject" id="MO2"><img alt="" src="../images/478239_1_En_2_Chapter/478239_1_En_2_Fig2_HTML.png" style="width:22.68em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.2</span><p class="SimplePara">Categories and interactivity of diegetic sounds in a virtual environment</p></div></figcaption></figure>
</div></section><section class="Section2 RenderAsSection2" id="Sec4"><h3 class="Heading"><span class="HeadingNumber">2.2.2 </span>Everyday Sounds</h3><p class="Para" id="Par14">An orthogonal approach with respect to the previous one amounts to characterizing sound sources in terms of the physical mechanisms and events that are associated to those sources.</p><div class="Para" id="Par15">Typical lists of audio assets for games or VR include, at the second level of classification (after the branch between ambience and sound effects), such categories as footsteps, doors, wind and weather, and cars and engines, with varying degrees of detail. These categories in fact refer to objects and events that are physically responsible for the corresponding sounds; however, such classifications follow common practices rather than a standardized taxonomy. A more systematic categorization can be found in the classic works by Gaver [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>], who proposed an “ecological” categorization of everyday sounds (the ecological approach to auditory perception will be discussed in more detail in Sect. <span class="InternalRef"><a href="#Sec7">2.3.2</a></span>). Gaver derived a tentative map of everyday <span id="ITerm4">sounds</span>, which is shown in Fig. <span class="InternalRef"><a href="#Fig3">2.3</a></span> and discussed in the remainder of this section.<figure class="Figure" id="Fig3"><div class="MediaObject" id="MO3"><img alt="" src="../images/478239_1_En_2_Chapter/478239_1_En_2_Fig3_HTML.png" style="width:32em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.3</span><p class="SimplePara">A taxonomy of everyday sounds that may be present in a virtual environment. Within each class (solids, liquids, and gases), rectangles, rounded rectangles, and ellipses represent basic, patterned, and compound sounds, respectively. Intersections between classes represent hybrid sounds.</p><div class="Credit"><p class="SimplePara">Figure based on the taxonomy of everyday sounds by Gaver [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>, Fig. 7]</p></div></div></figcaption></figure>
</div><p class="Para" id="Par16">At the highest level, Gaver’s taxonomy considers three broad classes of sounds: those involving vibrating solids, liquids, and aerodynamics in sound generation, respectively. Sounds generated by solid objects have patterns of vibrations structured by a number of physical attributes: those of the <em class="EmphasisTypeItalic ">interaction</em> that has produced the vibration, those of the <em class="EmphasisTypeItalic ">material</em> of the vibrating objects, and those of the <em class="EmphasisTypeItalic ">geometry</em> and configuration of the objects. Sounds involving liquids (e.g., dripping and splashing) also depend on an initial deformation that is counter-acted by restoring forces in the material, but no audible sound is produced by the vibrations of the liquid and instead the resulting sounds are created by the resonant cavities (bubbles) that form and oscillate in the liquid. Aerodynamic sounds are caused by the direct modification of atmospheric pressure differences from some source, such as those created by an exploding balloon or by the noise of a fan, or even events in which such changes in pressure transmit energy to objects and set them into vibration (e.g., when wind passes through a wire).</p><p class="Para" id="Par17">At the next level, sounds are classified along layers of complexity, defined as follows. “Basic” sound-producing events are identified for solids, liquids, and gases: sounds made by vibrating solids may be caused by impacts, scraping, or other interactions; liquid sounds may be caused by discrete drips, or by more continuous splashing, rippling, or pouring events; and aerodynamic sounds may be made by discrete, sudden changes of pressure (explosions), or by more continuous introductions of pressure variations (gusts and wind). “Patterned” sounds are situated at a higher level of complexity, as they are produced through temporal patterning of basic events. As an example, walking, breaking, bouncing, and so on are all complex events involving patterns of simpler impacts. Similarly, crumpling or crushing are examples of patterned deformation sounds. “Compound” sounds occupy the third level of complexity and involve more than one type of basic and patterned events. An example may be provided by the sound of a door slam, which involves the squeak of scraping hinges and the impact of the door on its frame, or a complex activity such as writing, which involves irregular temporal patterns of both impacts and scrapes. Compound sounds involve mutual constraints on their building components: as an example, concatenating the creak of a heavy door closing slowly with the slap of a light door slammed shut would arguably not sound natural.</p><p class="Para" id="Par18">Finally, Gaver’s taxonomy also considers “hybrid” events, in which two or three types of material are involved. An example of a hybrid sound involving solids and liquids is the one produced by raindrops hitting a window glass, which involves attributes of both liquid and vibrating solid sounds.</p><p class="Para" id="Par19">A taxonomy such as the one discussed here has at least two very attractive features. First, it provides a comprehensive framework for classifying any everyday sound potentially encountered in our world (and thus in a virtual world as well), with a fine level of detail. Secondly, its hierarchical structure provides a theoretical framework that can aid not only the sound design process but also the development of sound design tools. An example of an ecologically inspired software library for procedural sound design will be discussed in Sect. <span class="InternalRef"><a href="#Sec17">2.5.3</a></span>.</p></section></section><section class="Section1 RenderAsSection1" id="Sec5"><h2 class="Heading"><span class="HeadingNumber">2.3 </span>Perceptual and Cognitive Aspects</h2><p class="Para" id="Par20">In this section, we critically review and discuss some relevant aspects related to the perception and cognition of sonic interactions and provide links between these aspects and central concepts of VR, such as the plausibility illusion, the place illusion, the sense of embodiment, and the sense of agency. Nordahl and Nillson [<span class="CitationRef"><a epub:type="biblioref" href="#CR57" role="doc-biblioref">57</a></span>] also consider how sound production and perception relate to plausibility illusion, place illusion, and the sense of body ownership, although from a somewhat different angle.</p><p class="Para" id="Par21">Our main claim is that interactive sound sources in a virtual environment contribute in particular to the plausibility <span id="ITerm5">illusion</span>, the sense of <span id="ITerm6">agency</span>, and the sense of body <span id="ITerm7">ownership</span>. In addition, our analysis of perceptual and cognitive aspects provides requirements and guidelines for the development and the implementation of sound models.</p><section class="Section2 RenderAsSection2" id="Sec6"><h3 class="Heading"><span class="HeadingNumber">2.3.1 </span>Latency, Causality, and Multisensory Integration</h3><p class="Para" id="Par22">In any interactive system, <span id="ITerm8">latency</span> and its associated jitter have a major perceptual impact. High latency or jitter may impair the user’s performance or, at least, provide a frustrating and tiring <span id="ITerm9">experience</span>. Perceptually acceptable limits for latency and <span id="ITerm10">jitter</span> in an interactive system should therefore be determined. However, such limits depend on several factors which are not easily disentangled.</p><p class="Para" id="Par23">Characterizing latency and jitter in the sound rendering pipeline can be restated as a problem of perceived synchronization between pairs of events [<span class="CitationRef"><a epub:type="biblioref" href="#CR46" role="doc-biblioref">46</a></span>], which in turn may be divided into three categories: (i) an external and an internal temporal pattern (such as those occurring in a collaborative activity, e.g., music playing, between two persons in a virtual environment); (ii) pairs of external events (which may or may not pertain to the same sensory modality, such as pairs of sounds or a visual flash and a sound); (iii) actions of the user and their effects (e.g., the pressing of a button and the corresponding feedback sound).</p><p class="Para" id="Par24">The latter case in particular is tightly connected to the definition of interactive sound adopted in this chapter. It is inherently a problem of multimodal synchronization, as it involves a form of extrinsic (auditory) feedback and a form of intrinsic (tactile, proprioceptive, and kinesthetic) feedback generated by the user’s action [<span class="CitationRef"><a epub:type="biblioref" href="#CR53" role="doc-biblioref">53</a></span>]. The complex interaction occurring between these modalities influences their perceived synchronization (and thus the acceptable latency). High latencies can deteriorate the quality of the interaction, impair the performance on a given task, and even disrupt the perceived link of causality between the user’s action and the resulting sonic outcome.</p><p class="Para" id="Par25">The task at hand also influences the acceptable latency. As an example, it has been traditionally accepted that music performance is a task requiring extremely low (<span class="InlineEquation" id="IEq1"><img alt="$$\le 10$$" src="../images/478239_1_En_2_Chapter/478239_1_En_2_Chapter_TeX_IEq1.png" style="width:2.31em"/></span> ms) latencies between the player’s actions and the response of a digital musical instrument [<span class="CitationRef"><a epub:type="biblioref" href="#CR99" role="doc-biblioref">99</a></span>]. Similarly, it has been shown that even small amounts of jitter can be detrimental to the perceived quality of the interaction [<span class="CitationRef"><a epub:type="biblioref" href="#CR41" role="doc-biblioref">41</a></span>]. In this respect, music provides a good “worst case” and a lower bound for latency in other, non-musical tasks, where various studies suggest that higher latencies may be acceptable or even unperceivable [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR93" role="doc-biblioref">93</a></span>].</p><p class="Para" id="Par26">The type of interaction must be considered as well. Impulsive interactions (either musical, such as playing a drum, or non-musical, such as knocking on a door) are likely to require lower latencies than continuous ones (bowing a violin string, or accompanying a closing door). As an example, it has been shown that the continuous interaction involved in playing a theremin allows for relatively high (<span class="InlineEquation" id="IEq2"><img alt="$$&amp;gt;30$$" src="../images/478239_1_En_2_Chapter/478239_1_En_2_Chapter_TeX_IEq2.png" style="width:2.19em"/></span> ms) latencies, despite this being a musical task [<span class="CitationRef"><a epub:type="biblioref" href="#CR54" role="doc-biblioref">54</a></span>]. Finally, cognitive aspects also play a role: humans create expectations for the latency between their actions and the resulting feedback, detect disturbances to such expectations, and compensate for them. A study on the latency in live musical sound monitoring [<span class="CitationRef"><a epub:type="biblioref" href="#CR48" role="doc-biblioref">48</a></span>] showed significant discrepancies between different instruments, suggesting that certain players (e.g., pianists) are more tolerant to latency as they are accustomed to the inherent mechanical latency of their instrument, while others (e.g., drummers) are less so.</p><p class="Para" id="Par27">We conclude this section with a hint at the second type of synchronization mentioned at the beginning, i.e., that between pairs of external (possibly multimodal) events. Humans achieve robust perception through both the combination and the integration of information from multiple sensory modalities: the former strategy refers to interactions between non-redundant and complementary sensory signals aimed at disambiguating the sensory estimate, while the latter describes interactions between redundant signals aimed at reducing the variance in the sensory estimate and increasing its reliability [<span class="CitationRef"><a epub:type="biblioref" href="#CR28" role="doc-biblioref">28</a></span>]. The temporal relationships between inputs from different senses play an important role in multisensory combination and integration, which can be realized only within a window of synchrony between different modalities (e.g., auditory and visual, or auditory and haptic feedbacks) where a single percept is produced. Many studies [<span class="CitationRef"><a epub:type="biblioref" href="#CR19" role="doc-biblioref">19</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR83" role="doc-biblioref">83</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR96" role="doc-biblioref">96</a></span>] report quantitative results about “integration windows” between modalities, which can be used as constraints for the synchronization of the sound simulation pipeline with the visual (and possibly the haptic) modality. For more details regarding these issues, please refer to Part IV in this book, and in particular to Ch. <span class="ExternalRef"><a href="478239_1_En_10_Chapter.xhtml"><span class="RefSource">10</span></a></span>.</p></section><section class="Section2 RenderAsSection2" id="Sec7"><h3 class="Heading"><span class="HeadingNumber">2.3.2 </span>Everyday Listening and the Plausibility Illusion</h3><p class="Para" id="Par28">Human listeners are extremely good at interpreting sounds in terms of the events that produced them. The patterns of mechanical or aeroacoustic vibrations generated by sound-producing events depend on (and thus carry information about) contact forces, duration of contact, time-variations of the interaction, sizes, shapes, materials, and textures of the involved objects. We are immersed in a landscape of everyday sounds since the day we are born, and we have learned to extract meaning from this continuous and omnidirectional flow of information.</p><p class="Para" id="Par29">Gaver [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>] introduced the concept of <em class="EmphasisTypeItalic ">everyday <span id="ITerm11">listening</span></em>, as opposed to <em class="EmphasisTypeItalic ">musical <span id="ITerm12">listening</span></em>. When a listener hears a sound, she might concentrate on attributes like pitch, loudness, and timbre, or she might notice its masking effect on other sounds. These are examples of musical listening, meaning that the considered perceptual dimensions and attributes have to do with the sound itself, and are those used in the creation of music. On the other hand, the listener might concentrate on the characteristics of the sound source and possibly the surrounding environment. When hearing an approaching car, she might notice that the engine is powerful, that the car is approaching quickly from behind, or even that the road is a narrow alley with echoing walls on each side. This is an example of everyday listening.</p><p class="Para" id="Par30">The two perceptual processes associated to musical and everyday listening cannot be completely disentangled and may occur simultaneously. Still, the idea that in our everyday listening experience the physical characteristics of sound-producing objects can be linked to the corresponding acoustic features is a powerful one. The literature of ecological <span id="ITerm13">acoustics</span> provides several quantitative results on such links. The underlying assumption is that the flow of acoustic energy reaching our ears, the <em class="EmphasisTypeItalic ">acoustic array</em>, contains specific patterns, or <em class="EmphasisTypeItalic ">invariants</em>, which the listener exploits to infer information about the environment and guide her action. These concepts and terminology originate in the framework of ecological perception, rooted in Gibson’s works on visual perception in the 1950s [<span class="CitationRef"><a epub:type="biblioref" href="#CR35" role="doc-biblioref">35</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR55" role="doc-biblioref">55</a></span>].<sup><a epub:type="noteref" href="#Fn2" id="Fn2_source" role="doc-noteref">2</a></sup>
</p><p class="Para" id="Par32"><span id="ITerm14">Acoustic invariants</span> associated to sound events may include several attributes of a vibrating solid, such as its size, shape, and density, as these attributes contribute differently to characteristics of the resulting sound such as pitch, spectrum, amplitude envelope, and so on. In patterned sounds (see Sect. <span class="InternalRef"><a href="#Sec4">2.2.2</a></span>), the relevant information is also carried by the timing of successive events: footstep sounds must occur within a range of rates and regularities in order to be perceived as walking; the regularity in the temporal pattern of a bouncing sound provides information about the shape of the object (e.g., a sphere versus a cube).</p><p class="Para" id="Par33">The mapping between physical parameters and acoustic features is in general many-to-many. A single physical parameter can influence simultaneously many characteristics of the sound, and different physical parameters influence the same characteristics in different ways. As an example, changing the size of an object will scale the sound spectrum, i.e., will change the frequencies of the sound but not their pattern. On the other hand, changing the object’s shape results in a change in both the frequencies and their relationships. Acoustic invariants are thus the result of these complex patterns of change. Surveys of classic studies in ecological acoustics and acoustic invariants have been provided in previous works [<span class="CitationRef"><a epub:type="biblioref" href="#CR5" role="doc-biblioref">5</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR36" role="doc-biblioref">36</a></span>].</p><p class="Para" id="Par34">The above discussion provides a solid theoretical framework to reason on the importance of ecologically valid acoustic information in eliciting the qualia of <em class="EmphasisTypeItalic "><span id="ITerm15">presence</span>
</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR72" role="doc-biblioref">72</a></span>] in an immersive VR system. Among the many definitions proposed in the literature, we follow Skarbez et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR76" role="doc-biblioref">76</a></span>] in defining presence broadly as “the perceived realness of a mediated or virtual experience”. Slater et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR77" role="doc-biblioref">77</a></span>] introduced the concepts of plausibility illusion and place illusion, to refer to two distinct subjective internal feelings, both of which contribute to eliciting the sense of presence in a subject experiencing an immersive VR scenario. This conceptual model of presence is depicted in Fig. <span class="InternalRef"><a href="#Fig4">2.4</a></span>.<sup><a epub:type="noteref" href="#Fn3" id="Fn3_source" role="doc-noteref">3</a></sup>
</p><div class="Para" id="Par36">In this section we are particularly interested in the plausibility <span id="ITerm16">illusion</span>, i.e., the illusion that the scenario being depicted is actually occurring (we will discuss the place illusion in Sect. <span class="InternalRef"><a href="#Sec8">2.3.3</a></span> next). This is determined by the overall credibility of a virtual environment in comparison with subjective expectations. Slater argued that an important component of the plausibility illusion is “for the virtual reality to provide correlations between external events not directly caused by the participant and his/her own sensations” [<span class="CitationRef"><a epub:type="biblioref" href="#CR77" role="doc-biblioref">77</a></span>]. Skarbez et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR76" role="doc-biblioref">76</a></span>] proposed the construct of coherence, an objective characteristic of a virtual scenario that gives rise to the plausibility illusion (see Fig. <span class="InternalRef"><a href="#Fig4">2.4</a></span>, right) and depends on the internal logical and behavioral consistency of the virtual experience, with respect to prior knowledge. Building on these definitions, we argue that sound will contribute to the plausibility illusion of a virtual scenario as long as coherence is ensured for the auditory modality, i.e., as long as sound carries relevant ecological information expected by the user’s everyday listening experience.<figure class="Figure" id="Fig4"><div class="MediaObject" id="MO4"><img alt="" src="../images/478239_1_En_2_Chapter/478239_1_En_2_Fig4_HTML.png" style="width:31.72em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.4</span><p class="SimplePara">A conceptual model of presence: cloud boxes represent subjective internal feelings (qualia), circles represent functions affected by individual differences, and rounded rectangles represent objective characteristics of the virtual experience.</p><div class="Credit"><p class="SimplePara">Figure based on Skarbez [<span class="CitationRef"><a epub:type="biblioref" href="#CR76" role="doc-biblioref">76</a></span>, Fig. 2]</p></div></div></figcaption></figure>
</div><p class="Para" id="Par37">It shall be noted that <span id="ITerm17">coherence</span> makes no assumptions about the high fidelity of a virtual environment to the real world. Consequently, the plausibility illusion “does not require physical realism” [<span class="CitationRef"><a epub:type="biblioref" href="#CR77" role="doc-biblioref">77</a></span>]: several studies show that virtual characters or objects displayed with low visual fidelity in the virtual environment do not disrupt the illusion. With regard to the auditory domain, this observation may be related to the concept of cartoon sounds [<span class="CitationRef"><a epub:type="biblioref" href="#CR69" role="doc-biblioref">69</a></span>], i.e., simplified descriptions of sounding phenomena with exaggerated features. We argue that cartoon sounds do not disrupt the plausibility illusion as long as they still carry relevant ecological information. This is fundamentally the same principle exploited in the empirical science of <span id="ITerm18">Foley Art</span> for creating ecologically plausible sound effects [<span class="CitationRef"><a epub:type="biblioref" href="#CR2" role="doc-biblioref">2</a></span>].</p></section><section class="Section2 RenderAsSection2" id="Sec8"><h3 class="Heading"><span class="HeadingNumber">2.3.3 </span>Active Perception, Place Illusion, Embodiment</h3><p class="Para" id="Par38">The “enactive”<span id="ITerm19"/> approach to experience posits that it is not possible to disassociate perception and action schematically and that every kind of perception is intrinsically active and thoughtful. One of the most influential contributions in this direction is due to Varela et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR94" role="doc-biblioref">94</a></span>]. In the authors’ conception, experience does not occur inside the perceiver, but rather it is enacted by the perceiver while exploring the environment. In this view, the subject of mental states is the embodied, environmentally situated perceiver. The term “embodied” highlights two main points: (i) perception depends upon the kinds of experience that are generated from specific motor capabilities, and (ii) these capabilities are themselves embedded in a biological, psychological, and cultural context. Sensory and motor processes are fundamentally inseparable, and perception consists in exercising an exploratory skill. As an example [<span class="CitationRef"><a epub:type="biblioref" href="#CR58" role="doc-biblioref">58</a></span>], the sensation of softness experienced when holding a sponge consists in being aware that one can exercise certain skills: one can press the sponge, and it will yield under the pressure. The experience of the softness of the sponge is characterized by a variety of such possible patterns of interaction. Sensorimotor dependencies, or contingencies, are the laws that describe these interactions. When a perceiver knows that he is exercising the sensorimotor contingencies associated with softness, then he is experiencing the sensation of softness.</p><p class="Para" id="Par39">Embodied theories of perception provide the ground for discussing further central concepts for VR, such as immersion, place illusion, sense of <span id="ITerm20">embodiment</span>, and their relation to interactive sound. As depicted in Fig. <span class="InternalRef"><a href="#Fig4">2.4</a></span> (left), immersion is an objective property of a VR system. Research has concentrated largely on characteristics such as latency, rendering frame rate, and tracking [<span class="CitationRef"><a epub:type="biblioref" href="#CR22" role="doc-biblioref">22</a></span>]. However, immersive systems can be also characterized in relation to the supported sensorimotor <span id="ITerm21">contingencies</span>, which in turn define a set of valid actions that are perceptually meaningful (for instance, with a head-mounted display and head-tracking, it is possible to turn your head or bend forward producing changes in the rendered visual images). When a system supports sensorimotor contingencies that approximate those of physical reality, it can give rise to the place <span id="ITerm22">illusion</span>, a specific subjective internal feeling which is the illusion of being located inside the rendered virtual environment, of “being there”  [<span class="CitationRef"><a epub:type="biblioref" href="#CR77" role="doc-biblioref">77</a></span>]. Whereas the plausibility illusion is based on what a subject perceives in the virtual environment, the place illusion is based on how she is able to perceive it.</p><p class="Para" id="Par40">The great majority of studies addressing explicitly the effect of sound on the place illusion are concerned with spatial attributes: this is not entirely surprising, since many of these attributes are perceived by exercising specific motor actions (e.g., rotating the head to perceive the distance or the direction of a sound source or a reflecting surface). In this respect, directivity is possibly the only sound source attribute contributing to the place illusion, while other ecological attributes are more likely to contribute to the plausibility illusion only, as discussed in Sect. <span class="InternalRef"><a href="#Sec7">2.3.2</a></span>. In accordance with this picture, over the years, various authors [<span class="CitationRef"><a epub:type="biblioref" href="#CR11" role="doc-biblioref">11</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR38" role="doc-biblioref">38</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR60" role="doc-biblioref">60</a></span>] found that spatialized sound positively influences presence as being there when compared to no-sound or non-spatialized sound conditions, but does not affect the perceived realism of the environment. A comprehensive survey up to 2010 is provided by Larsson [<span class="CitationRef"><a epub:type="biblioref" href="#CR47" role="doc-biblioref">47</a></span>].</p><p class="Para" id="Par41">The sense of embodiment refers to yet another subjective internal feeling. Specifically, the sense of embodiment in an immersive virtual environment is concerned with the relationship between one’s self and one’s body, whereas the sense of presence refers to the relationship between one’s self and the environment (and may occur even without the sensation of having a body). Kilteni et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR45" role="doc-biblioref">45</a></span>] provide a working definition of a sense of embodiment toward an artificial body, as the sense that emerges when that artificial body’s properties are processed as if they were the properties of one’s own biological body. Further, the authors associate it to three main components: (i) the sense of self-<span id="ITerm23">location</span>, (ii) of body <span id="ITerm24">ownership</span>, and (iii) of <span id="ITerm25">agency</span>, the latter being investigated as an independent construct by other researchers [<span class="CitationRef"><a epub:type="biblioref" href="#CR17" role="doc-biblioref">17</a></span>].</p><p class="Para" id="Par42">The sense of self-location refers to one’s spatial experience of being inside a body, rather than being inside a world (with or without a body), and is highly determined by the visuospatial perspective, <span id="ITerm26">proprioception</span>, and vestibular signals, as well as tactile sensations at the border between our body and the <span id="ITerm27">environment</span>. The sense of body ownership refers to one’s self-attribution of an artificial body perceived as the source of the experienced sensations and emerges as a complex combination of afferent multisensory information and cognitive processes that may modulate the processing of sensory stimuli, as demonstrated by the well-known rubber hand <span id="ITerm28">illusion</span> [<span class="CitationRef"><a epub:type="biblioref" href="#CR13" role="doc-biblioref">13</a></span>]. The sense of agency refers to the sense of having global motor control in relation to one’s own body and has been proposed to result from a comparison between the predicted and the actual sensory consequences of one’s actions [<span class="CitationRef"><a epub:type="biblioref" href="#CR24" role="doc-biblioref">24</a></span>]: when the two match by, for example, the presence of synchronous visuomotor correlations under active movement, one feels oneself to be the agent of those actions.</p><p class="Para" id="Par43">The above discussion suggests that interactive sounds occurring directly in reaction to the avatar’s gestures in a virtual scenario, and coherently with the available sensorimotor contingencies, can positively affect the sense of agency in particular. One relevant example is provided by footsteps: several studies have addressed the issue of generating footstep <span id="ITerm29">sounds</span> [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR85" role="doc-biblioref">85</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR95" role="doc-biblioref">95</a></span>] however without assessing their specific relevance to the sense of agency. Other studies have shown that interactively generated sound can support haptic sensations, as in the case of impact sounds reinforcing or modulating the perceived hardness of an impulsive contact [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>], or friction sounds affecting the perceived effort in dragging an object [<span class="CitationRef"><a epub:type="biblioref" href="#CR4" role="doc-biblioref">4</a></span>] (refer to Chap. <span class="ExternalRef"><a href="478239_1_En_12_Chapter.xhtml"><span class="RefSource">12</span></a></span> for other audio-haptic case studies). Yet, no attempt was made in these studies to specifically address the issue of agency.</p><p class="Para" id="Par44">Even less research seems to have been conducted on the effects of interactive sound on the sense of body ownership. Footsteps provide a relevant example also in this case, as the sound of steps can be related to the perceived weight of one’s own body [<span class="CitationRef"><a epub:type="biblioref" href="#CR85" role="doc-biblioref">85</a></span>] or that of an avatar [<span class="CitationRef"><a epub:type="biblioref" href="#CR74" role="doc-biblioref">74</a></span>]. Sikström et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>] evaluated the role of self-produced sounds in participants’ sensation of ownership of virtual wings in an immersive scenario. A related issue is that of the sound of one’s own voice in a virtual environment [<span class="CitationRef"><a epub:type="biblioref" href="#CR61" role="doc-biblioref">61</a></span>].</p></section></section><section class="Section1 RenderAsSection1" id="Sec9"><h2 class="Heading"><span class="HeadingNumber">2.4 </span>Events Versus Processes</h2><p class="Para" id="Par45">Having discussed the perceptual and cognitive aspects involved in interactive sound generation, we now jump back to the pipeline of Fig. <span class="InternalRef"><a href="#Fig1">2.1</a></span> and look specifically at the “source modeling”<span id="ITerm30"/> box.</p><p class="Para" id="Par46">When creating sound sources in a virtual environment, approaches based on sample playback are still the most common ones [<span class="CitationRef"><a epub:type="biblioref" href="#CR12" role="doc-biblioref">12</a></span>], taking advantage of sound design techniques that have been refined through a long history, and being able to yield perfect realism, “at least for single sounds triggered only once” [<span class="CitationRef"><a epub:type="biblioref" href="#CR21" role="doc-biblioref">21</a></span>]. From a completely different perspective, procedural approaches defer the generation of sound signals until runtime, when information on sound-producing event is available and can be used to yield more interactive sonic results. This section discusses these two dichotomical approaches.</p><section class="Section2 RenderAsSection2" id="Sec10"><h3 class="Heading"><span class="HeadingNumber">2.4.1 </span>Event-Driven Approaches</h3><p class="Para" id="Par47">Approaches based on <span id="ITerm31">sample</span> playback follow an event-driven logics, in which a specific sound exists as a waveform stored in a file or a table in memory and is bound to some event occurring in the virtual world. Borrowing an example from Farnell [<span class="CitationRef"><a epub:type="biblioref" href="#CR31" role="doc-biblioref">31</a></span>]: <span class="EmphasisFontCategoryNonProportional ">if (moves(gate)) play(scrape.wav)</span>.</p><div class="Para" id="Par48">One immediate consequence of this is that the playback and the post-processing of samples are dissociated from the underlying physics engine and the graphical rendering engine. In the case of a sound played back once, the length of the sound is predetermined and thus any timing relationship between auditory and visual elements must also be predefined. In the case of a looped sound, the endpoint must be explicitly given, e.g., as a response to a subsequent event. More in general, the playback of sound is controlled by a finite and small set of states (such as in the case of an elevator that can be starting, moving, stopping, or stopped). Correspondingly, any event is bound to a sound “asset”, or to some post-processing of that asset.<figure class="Figure" id="Fig5"><div class="MediaObject" id="MO5"><img alt="" src="../images/478239_1_En_2_Chapter/478239_1_En_2_Fig5_HTML.png" style="width:32.48em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.5</span><p class="SimplePara">Event-driven logics for VR sound design using samples and audio middleware software</p></div></figcaption></figure>
</div><p class="Para" id="Par49">Current practices of sound design for VR are deeply and firmly rooted in such event-driven logics, depicted in Fig. <span class="InternalRef"><a href="#Fig5">2.5</a></span>. One clear example of this is provided by “audio middleware” software [<span class="CitationRef"><a epub:type="biblioref" href="#CR12" role="doc-biblioref">12</a></span>], which are tools that facilitate the work of the sound designer by reducing programming time and testing the sound design in real time along with the game engine. The most commonly adopted middleware solutions, such as FMOD Studio (Firelight Technologies)<sup><a epub:type="noteref" href="#Fn4" id="Fn4_source" role="doc-noteref">4</a></sup> and Wwise (Audiokinetic),<sup><a epub:type="noteref" href="#Fn5" id="Fn5_source" role="doc-noteref">5</a></sup> largely follow the traditional paradigm of DAWs (Digital Audio Workstations) and include GUIs for adding, controlling, and processing samples; linking them to objects, areas, and events of the virtual environment; and imposing rules for triggering and playback.</p><p class="Para" id="Par52">One of the main acknowledged limitations of samples is that they are static, and they are just single, atomic instances of events. The repetitiveness involved in multiple playbacks of the same sounds has the potential to disrupt many of the perceptual and cognitive effects discussed in Sect. <span class="InternalRef"><a href="#Sec5">2.3</a></span>, and even to lead to fatigue. Partial remedies to this problem include the use of multiple samples for the same event, as well as the use of various post-processing operations, the most common being modifications to pitch, time, dynamics, as well as sound layering and looping [<span class="CitationRef"><a epub:type="biblioref" href="#CR75" role="doc-biblioref">75</a></span>].</p><p class="Para" id="Par53">Well-established time-stretching and pitch-shifting algorithms exist; however, the quality of the processing is in general guaranteed only for relatively small shifting and stretching factors. Concerning dynamics, typical approaches are based on blending, cross-fading, and mixing of different samples, similarly to a musical sampler (and with similar limitations as well). Layering and looping are especially useful for the construction of ambiences: multiple sounds can be individually looped and played concurrently to create complex and layered ambiences. Repetitiveness can be reduced by assigning different lengths to different loops, and immersion can be enhanced by rendering individual layers at different virtual spatial locations. All this requires manual operations by the sound designer, such as splitting, cross-fading, and so on.</p><p class="Para" id="Par54">Further countermeasures to repetition and listener fatigue include the use of techniques based on randomization. These can be applied to many aspects of sound, including, but not limited to (i) pitch and amplitude variations, (ii) sample selection, (iii) sample concatenation, (iv) looping, and (v) location of sound sources. As an example, randomized sample selection amounts to performing randomizations of alternative samples associated to the same event, e.g., a collision: a different sample is played back at each occurrence of the event, mimicking the differences occurring due to slightly different contact points and velocities. In randomized concatenation, different samples are concatenated to build a composite sound in response to a repetitive sequence of events, such as in the case of footsteps, weapon sounds, and so on. Triggering different points with different probabilities can also be used to reduce the repetitiveness of looped layers in ambience sounds. The audio middleware solutions mentioned above typically implement several of these techniques.</p><p class="Para" id="Par55">Randomization techniques hint at another issue with samples, which is the need for very large amounts of data. Putting together a large sample library is a slow and labor-intensive process. Moreover, data need to be stored in memory, possibly in secondary storage, from which they then have to be prefetched before playback.</p></section><section class="Section2 RenderAsSection2" id="Sec11"><h3 class="Heading"><span class="HeadingNumber">2.4.2 </span>Procedural Approaches</h3><p class="Para" id="Par56">Techniques based on the randomization of several sample-processing parameters, such as those discussed above, are sometimes loosely referred to as <em class="EmphasisTypeItalic ">procedural</em> in the sound design practice [<span class="CitationRef"><a epub:type="biblioref" href="#CR75" role="doc-biblioref">75</a></span>, Chap. 2]. Here, we favor a stricter definition. In Farnell’s words [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>], procedural <span id="ITerm32">audio</span> is <em class="EmphasisTypeItalic ">“sound as a process, rather than sound as data”</em>. This definition shifts the focus onto the creation of audio assets, as opposed to the manipulation of existing ones.</p><div class="Para" id="Par57">Procedural audio is thus synthetic sound, is real time, and most importantly is created according to a set of programmatic rules and live input. This implies that procedurally generated sound is synthesized at runtime, when all the needed input and contextual information are available, whereas in a sample-based approach, most of the work is performed offline prior to execution, implying that “many decisions are made in advance and cast in stone” [<span class="CitationRef"><a epub:type="biblioref" href="#CR31" role="doc-biblioref">31</a></span>].<figure class="Figure" id="Fig6"><div class="MediaObject" id="MO6"><img alt="" src="../images/478239_1_En_2_Chapter/478239_1_En_2_Fig6_HTML.png" style="width:32.48em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.6</span><p class="SimplePara">Procedural sound design: <strong class="EmphasisTypeBold ">a</strong> model building, and <strong class="EmphasisTypeBold ">b</strong> method analysis stages</p><div class="Credit"><p class="SimplePara">(figures loosely based on Farnell [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>, Figs. 16.4–5])</p></div></div></figcaption></figure>
</div><p class="Para" id="Par58">The stages involved in the process of procedural sound design may be loosely based on those of software life-cycle, including (i) requirements analysis, (ii) research and acquisition, (iii) model building, (iv) method analysis, (v) implementation, (vi) integration, (vii) test and iteration, and (viii) maintenance. Figure <span class="InternalRef"><a href="#Fig6">2.6</a></span> provides a graphical summary of the two central stages, i.e., model building and method analysis.</p><p class="Para" id="Par59">Building a model (Fig. <span class="InternalRef"><a href="#Fig6">2.6</a></span>a) provides a simplification of the properties and behaviors of a real object, which starts from the analysis of sound data (including time- and/or frequency-domain analysis, extraction of relevant audio features, etc.), as well as a physical analysis of the involved sound-generating mechanisms, and results into a set of parametric controls and behaviors. The hierarchy of everyday sounds depicted in Fig. <span class="InternalRef"><a href="#Fig3">2.3</a></span> provides a useful reference framework: the model at hand can be positioned inside this hierarchy. Moreover, following the discussion on everyday listening of Sect. <span class="InternalRef"><a href="#Sec7">2.3.2</a></span>, the choice of the model parametrization can be informed by the knowledge of relevant acoustic invariants carrying information about sound-generating objects and events.</p><p class="Para" id="Par60">The method analysis stage is where the most appropriate sound synthesis and processing techniques are chosen, starting from a palette of available ones, and based on the model at hand. Figure <span class="InternalRef"><a href="#Fig6">2.6</a></span>b shows a set of commonly employed sound synthesis techniques (in Sect. <span class="InternalRef"><a href="#Sec12">2.4.3</a></span>, we will explore physics-based techniques in particular). As a result of this stage, an implementation plan is produced that includes a set of techniques and corresponding low-level synthesis parameters, as well as the involved audio streams.</p><p class="Para" id="Par61">Based on this discussion, we can identify two main qualities of procedural approaches with respect to sample playback. The first one is their intrinsic adaptability and interactivity (according to the definitions given in Sect. <span class="InternalRef"><a href="#Sec1">2.1</a></span>), which derive from the deferring of sound generation at runtime based on programmatic rules and user input, and result in ever-changing sonic results in response to real-time control. The second one is flexibility, where a single procedural model can be parametrized to produce a variety of sound events within a given class of sounds: this contrasts with sample-based, event-driven approaches, where ever-increasing amounts of data and assets are needed in order to cope with the needs of complex virtual worlds.</p></section><section class="Section2 RenderAsSection2" id="Sec12"><h3 class="Heading"><span class="HeadingNumber">2.4.3 </span>Physics-Based Methods</h3><p class="Para" id="Par62">Looking back at Fig. <span class="InternalRef"><a href="#Fig6">2.6</a></span>b, one of the available paints in the palette of sound synthesis techniques is that of physics-based methods.</p><p class="Para" id="Par63">The boundaries between what can be considered physical (or physics-based, or physics-informed) sound synthesis are somewhat blurry in the scientific literature. Here, we adopt the definition given by Smith [<span class="CitationRef"><a epub:type="biblioref" href="#CR78" role="doc-biblioref">78</a></span>] and refer to synthesis techniques where “ [...] there is an explicit representation of the relevant physical state of the sound source. For example, a string physical model must offer the possibility of exciting the string at any point along its length. [...] All we need is Newton.” The last claim refers to the idea that physical <span id="ITerm33">modeling</span> always starts with a quantitative description of the sound sources based on Newtonian mechanics. Such description may be approximate and simplified to various extents, but the above definition provides an unambiguous—albeit broad—characterization in terms of physical state access. Resorting to a simple (yet historically relevant [<span class="CitationRef"><a epub:type="biblioref" href="#CR68" role="doc-biblioref">68</a></span>]) example, we can say that additive <span id="ITerm34">synthesis</span> of bell sounds is not physics-based, as additive sinusoidal partials only describe the time-frequency characteristics of the sound signal without any reference to the physical state of the bell. On the other hand, modal synthesis [<span class="CitationRef"><a epub:type="biblioref" href="#CR1" role="doc-biblioref">1</a></span>] of the same bell, with modal oscillators tuned to the sound partials, is only apparently a similar approach: a linear combination of the modes can provide the displacement and the velocity at any point of the bell, and each modal shape defines to what extent an external force applied at a given point affects the corresponding mode.</p><p class="Para" id="Par64">The history of physics-based <span id="ITerm35">synthesis</span> is rooted in studies on the acoustics of the vocal apparatus [<span class="CitationRef"><a epub:type="biblioref" href="#CR44" role="doc-biblioref">44</a></span>] and of musical instruments [<span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR40" role="doc-biblioref">40</a></span>], where numerical models were initially used for simulation rather than synthesis purposes. Current techniques are based on several alternative formulations and methods, including ordinary or partial differential equations, equivalent circuit representations, modal representations, finite-difference and finite-element schemes, and so on [<span class="CitationRef"><a epub:type="biblioref" href="#CR78" role="doc-biblioref">78</a></span>]. Comprehensive surveys of physical modeling approaches have been published [<span class="CitationRef"><a epub:type="biblioref" href="#CR79" role="doc-biblioref">79</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR89" role="doc-biblioref">89</a></span>]. Although these deal with musical sound synthesis mostly, much of what has been learned in that domain can be applied to the physical modeling of any sounding object.</p><p class="Para" id="Par65">Although physics-based synthesis is sometimes made synonymous with procedural audio, Fig. <span class="InternalRef"><a href="#Fig6">2.6</a></span>b provides a clear picture of the relation between the two. In this perspective, “procedural audio is more than physical modeling,” [<span class="CitationRef"><a epub:type="biblioref" href="#CR31" role="doc-biblioref">31</a></span>] and the latter can be seen as one of the tools at the disposal of the sound designer to reduce a sound to its behavioral realization. Combining physics-based approaches with knowledge of auditory perception and cognition often results in procedural models in which the physical description has been drastically simplified while retaining the ecological validity of sounds and the realism of the interactions, thus preserving the plausibility illusion of the resulting sonic world and the sense of agency of the subject (see related discussions in Sects. <span class="InternalRef"><a href="#Sec7">2.3.2</a></span> and <span class="InternalRef"><a href="#Sec8">2.3.3</a></span>).</p></section><section class="Section2 RenderAsSection2" id="Sec13"><h3 class="Heading"><span class="HeadingNumber">2.4.4 </span>Computational Costs</h3><p class="Para" id="Par66">Event-driven and procedural approaches must be analyzed also in terms of the involved computational <span id="ITerm36">requirements</span>. In case of insufficient resources, excessive computational costs may introduce artifacts in the rendered sound or in alternative may require to increase the overall latency of the rendering up to a point where the perception of causality and multisensory integration are disrupted (see Sect. <span class="InternalRef"><a href="#Sec6">2.3.1</a></span>).</p><p class="Para" id="Par67">With reference to Fig. <span class="InternalRef"><a href="#Fig1">2.1</a></span>, it can be stated that one main computational bottleneck in the sound simulation rendering pipeline [<span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>] is the “per sound source” cost. This relates in particular to the sound propagation stage (see Chap. <span class="ExternalRef"><a href="478239_1_En_3_Chapter.xhtml"><span class="RefSource">3</span></a></span>), as reflections, scattering, occlusions, Doppler effects, and so on must be computed for each sound source involved in the simulation. But it also includes the source modeling stage, with particular reference to the generation of the sound source signals.</p><p class="Para" id="Par68">Sample playback has a fixed cost, irrespective of the sound being played. Moreover, the cost of playback is very small. However, samples must be loaded in memory before being played. As a consequence, when a sound is triggered, the playback may involve a prefetch phase where a soundbank is loaded from the secondary memory. Moreover, some management of polyphony must be set in place in order to prioritize the playback in case of several simultaneously active sounds. This can use policies similar to those employed in music synthesizers: typically, sounds falling below a certain amplitude threshold are dropped, leaving place for other sounds. The underlying assumption is that louder sounds mask softer ones, so that dropping the latter has no or minimal perceptual consequences. Although modern architectures allow for the simultaneous playback of hundreds of audio assets, generating complex soundscapes may exceed the amount of available channels.</p><p class="Para" id="Par69">On the other hand, procedural sound has variable costs, which depend on the complexity of the corresponding model and on the employed methods. This is particularly evident in the case of physics-based techniques: for large-scale, brute-force approaches, like higher dimensional finite-element or finite-difference methods, real time is still hard to achieve. On the other hand, techniques like modal <span id="ITerm37">synthesis</span> can be implemented very efficiently, albeit at the cost of reduced flexibility of the models (e.g., interaction with sounding objects limited to single input-output), which in turn can have a detrimental effect on the plausibility illusion. Some non-physical methods are very cheap in terms of computational requirements, as in the case of subtractive synthesis for generating wind or fire sounds. Section  <span class="InternalRef"><a href="#Sec15">2.5.1</a></span> provides several examples of procedural methods for various classes of everyday sounds.</p><p class="Para" id="Par70">Although it is generally true that sample-based methods outperform procedural audio for small amounts of sounds, it has been noted [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>] that this is not necessarily true in the limit of larger numbers: whereas the fixed cost of sample playback results in a computational complexity that is linear in the number of rendered sources, the availability of very cheap procedural models can produce the result that for high numbers of sources the situation reverses and procedural sound starts to outperform sample-based methods.</p></section></section><section class="Section1 RenderAsSection1" id="Sec14"><h2 class="Heading"><span class="HeadingNumber">2.5 </span>Procedural and Physics-Based Approaches in VR Audio</h2><p class="Para" id="Par71">Given these premises, what is the current development of procedural and physics-based approaches in audio for VR? In this section, we show that, despite a substantial amount of research, these approaches are still struggling to gain popularity in real-world products and practices.</p><section class="Section2 RenderAsSection2" id="Sec15"><h3 class="Heading"><span class="HeadingNumber">2.5.1 </span>Methods</h3><p class="Para" id="Par72">Far from providing a comprehensive survey of previous literature in the field, which would go way beyond the scope of this chapter, this section aims at assessing to what extent the taxonomy of everyday sounds provided in Fig. <span class="InternalRef"><a href="#Fig3">2.3</a></span> has been covered by existing procedural approaches. This exercise also serves as a testbed to verify the generality of that taxonomy. For a recent and broad survey, see Liu and Manocha [<span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>].</p><p class="Para" id="Par73"><span id="ITerm38">Solid sounds</span> are by far the most investigated category. For basic models, modal synthesis [<span class="CitationRef"><a epub:type="biblioref" href="#CR1" role="doc-biblioref">1</a></span>] is the dominant approach. There are several works investigating the use of modal methods for the procedural generation of contact sounds between solid objects, including the optimization of modal synthesis through specialized numerical schemes and/or perceptual criteria, as in the work by Raghuvanshi et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR63" role="doc-biblioref">63</a></span>]. Procedural models of surface textures have been proposed by several scholars [<span class="CitationRef"><a epub:type="biblioref" href="#CR66" role="doc-biblioref">66</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR91" role="doc-biblioref">91</a></span>] and applied to <span id="ITerm39">scraping</span> and rolling <span id="ITerm40">sounds</span> [<span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>]. Basic interaction forces (impact and sliding friction) can be modeled with a variety of approaches that range from qualitative approximations of temporal profiles of impulsive force magnitudes [<span class="CitationRef"><a epub:type="biblioref" href="#CR92" role="doc-biblioref">92</a></span>] to the physical simulation of stick-slip phenomena in friction forces [<span class="CitationRef"><a epub:type="biblioref" href="#CR7" role="doc-biblioref">7</a></span>].</p><p class="Para" id="Par74">At the next level of complexity, models of patterned solid sounds have also been widely studied. Stochastic models of crumpling <span id="ITerm41">phenomena</span> have been proposed, with applications to cloth sound synthesis [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>], crumpling paper sounds, or sounds produced by deformations of aggregate materials, such as sand, snow, or gravel [<span class="CitationRef"><a epub:type="biblioref" href="#CR15" role="doc-biblioref">15</a></span>]. The latter have also been used in the context of <span id="ITerm42">walking</span> interactions [<span class="CitationRef"><a epub:type="biblioref" href="#CR81" role="doc-biblioref">81</a></span>] (see also Sect. <span class="InternalRef"><a href="#Sec8">2.3.3</a></span>) to simulate the sound of a footstep onto aggregate grounds. Breaking <span id="ITerm43">sounds</span> have been modeled especially with the purpose of synchronizing animations of brittle fractures produced by a physics engine [<span class="CitationRef"><a epub:type="biblioref" href="#CR59" role="doc-biblioref">59</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR100" role="doc-biblioref">100</a></span>].</p><p class="Para" id="Par75">The category of aerodynamic <span id="ITerm44">sounds</span> is less studied. Within the basic level of complexity, the sound produced by wind includes those resulting from interaction with large static obstructions, aeolian tones, and cavity tones: these have been procedurally modeled with techniques ranging from computationally intensive fluid-dynamics simulations [<span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>] to simple (yet efficient and effective) subtractive schemes using noisy sources and filters [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>]. These can be straightforwardly employed to construct patterned and compound sonic events, including windy scenes, swinging objects, and so on [<span class="CitationRef"><a epub:type="biblioref" href="#CR71" role="doc-biblioref">71</a></span>]. Other basic aeroacoustic events include turbulences, most notably explosions, which are a key component of more complex sounds such as gunshots [<span class="CitationRef"><a epub:type="biblioref" href="#CR37" role="doc-biblioref">37</a></span>] and fire [<span class="CitationRef"><a epub:type="biblioref" href="#CR18" role="doc-biblioref">18</a></span>]. Yet another relevant patterned sonic event is that produced by combustion engines [<span class="CitationRef"><a epub:type="biblioref" href="#CR10" role="doc-biblioref">10</a></span>].</p><p class="Para" id="Par76"><span id="ITerm45">Liquid sounds</span> appear to be the least addressed category. Basic procedural models include sounds produced by drops in a liquid [<span class="CitationRef"><a epub:type="biblioref" href="#CR90" role="doc-biblioref">90</a></span>] or by pouring a liquid [<span class="CitationRef"><a epub:type="biblioref" href="#CR65" role="doc-biblioref">65</a></span>], whereas patterned and compound sonic events have been more often simulated using concatenative approaches relying on the output of the graphical procedural simulation [<span class="CitationRef"><a epub:type="biblioref" href="#CR98" role="doc-biblioref">98</a></span>]. A relevant example of hybrid solid-liquid sounds is that of rain [<span class="CitationRef"><a epub:type="biblioref" href="#CR50" role="doc-biblioref">50</a></span>].</p></section><section class="Section2 RenderAsSection2" id="Sec16"><h3 class="Heading"><span class="HeadingNumber">2.5.2 </span>Optimizations</h3><p class="Para" id="Par77">We have provided in Sect. <span class="InternalRef"><a href="#Sec13">2.4.4</a></span> a general discussion on computational costs associated to procedural approaches, in comparison to sample-based methods. Since the former typically results in higher “per sound source” costs than the latter, various studies have proposed strategies for reducing the load of complex procedural audio scenes in virtual environments.</p><p class="Para" id="Par78">One attractive feature of procedural sound in terms of computational complexity is the possibility of dynamically adapting the level of detail (LOD) of the synthesized audio. The concept of LOD is a long-established one in computer graphics and encompasses various optimization techniques for decreasing the complexity of 3D object rendering [<span class="CitationRef"><a epub:type="biblioref" href="#CR52" role="doc-biblioref">52</a></span>]. The general goal of LOD techniques is to increase the rendering speed by reducing details while minimizing the perceived degradation of <span id="ITerm46">quality</span>. Most commonly, the LOD is varied as a function of the distance from the camera, but other metrics can be used, including size, speed of motion, priority, and so on. Reducing the LOD may be achieved by simplifying the 3D object mesh, or by using impostors (i.e., replacing mesh-based with image-based rendering), and other approaches can be used to dynamically control the LOD of landscape rendering, crowd simulation, and so on.</p><p class="Para" id="Par79">Similar ideas may be applied to procedural sound, achieving further reductions of computational costs for complex sound scenes with respect to sample playback. However, very few studies explored the concept of LOD in the auditory domain, and there is not even a commonly accepted definition in the related literature: some scholars have coined the term Sound Level Of Detail (SLOD) [<span class="CitationRef"><a epub:type="biblioref" href="#CR70" role="doc-biblioref">70</a></span>], while others use Level Of Audio <span id="ITerm47">Detail</span> (LOAD) [<span class="CitationRef"><a epub:type="biblioref" href="#CR27" role="doc-biblioref">27</a></span>], both generically referring to varying sound resolution according to the required perceived precision. Here, we stick to the latter definition (LOAD), since this seems to be more frequently adopted in recent literature.</p><div class="Para" id="Par80">Strategies for dynamic LOAD can be partly derived from graphics. Simple approaches amount to fade out and turn off distant sounds based on radial distance or zoning. Depending on their distance, sound sources may be also clustered or activated according to some predefined behavior. Techniques based on impostors can be used as well: as an example, when rendering the sound of a crowd, individual sounds emitted by several characters can be replaced by a global sample-based ambience sound. However, one should be aware of the differences between visual and auditory perception and exploit the peculiarities of the latter to develop more advanced strategies for dynamic LOAD. Figure <span class="InternalRef"><a href="#Fig7">2.7</a></span> depicts an example of a dynamic LOAD strategy based on radial distance, in which levels of details are associated to three overlapping <span id="ITerm48">proximity</span> profiles around the listener (foreground, middle ground, and background): sounds in the foreground are rendered individually through procedural approaches; those that fall into the middle ground can be rendered through some simplifying approaches (clustering, grouping, and statistical behaviors); and finally, sounds in the background may be substituted by audio impostors such as audio files.<figure class="Figure" id="Fig7"><div class="MediaObject" id="MO7"><img alt="" src="../images/478239_1_En_2_Chapter/478239_1_En_2_Fig7_HTML.png" style="width:29.12em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 2.7</span><p class="SimplePara">Example of dynamic LOAD based on the radial distance from the listener, where levels of details are associated to three overlapping proximity profiles.</p><div class="Credit"><p class="SimplePara">Figure partly based on Schwarz et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR70" role="doc-biblioref">70</a></span>, Fig. 3]</p></div></div></figcaption></figure>
</div><p class="Para" id="Par81">Pioneering work in this direction was carried out by Fouad et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR32" role="doc-biblioref">32</a></span>], although the authors did not explicitly refer to the concept of LOD. This work proposes a set of “perceptually based scheduling algorithms”, that allows a scheduler to assign execution time to each sound in the scene minimizing some perceptually motivated error metric. In particular, sounds are prioritized depending on the listener’s gaze, the loudness, and the age of the sound. Tsingos and coworkers [<span class="CitationRef"><a epub:type="biblioref" href="#CR56" role="doc-biblioref">56</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR88" role="doc-biblioref">88</a></span>] proposed an approach to reduce the number of (sample-based) sound sources in a complex scenario, by combining perceptual culling and perceptual clustering. The culling stage removes perceptually inaudible sources based on a global masking model, while the clustering stage groups the remaining sound sources into a predefined number of clusters: as a result, a representative point source is constructed for each cluster and a set of equivalent source signals is generated. Schwarz et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR70" role="doc-biblioref">70</a></span>] proposed a design with three LOADs based on proximity and smooth transitions between proximity levels, very much like those depicted in Fig. <span class="InternalRef"><a href="#Fig7">2.7</a></span>: (i) foreground, i.e., individually driven sound events (e.g., individual raindrops on tree leaves); (ii) middle ground, i.e., group-driven sound events, at the point where individual events cannot be isolated and can be replaced by stochastical behaviors; (iii) background, i.e., sound sources that are further away and can be rendered by audio impostors such as audio files or dynamic mixing of groups of procedural impostors. More recently, Dall’Avanzi et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR23" role="doc-biblioref">23</a></span>] analyzed the effect on player’s immersion in response to soundscapes with two applied LOADs. Two groups of participants played two different versions of the same game, and the player’s immersion was measured through two questionnaires. However, results in this case showed no considerable difference between the two groups.</p><p class="Para" id="Par82">Other researchers proposed or evaluated LOAD techniques specifically tailored to certain synthesis methods. Raghuvanshi et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR63" role="doc-biblioref">63</a></span>] addressed modal synthesis and investigated various perceptually motivated techniques for improving the efficiency of the synthesis. These include a “quality scaling” technique that effectively controls the dynamic LOAD: briefly, in a scene involving many sounding objects, the number of modes assigned to individual objects scales with objects location from foreground to background, without significant losses in perceived quality. Durr et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR27" role="doc-biblioref">27</a></span>] evaluated through subjective tests various procedural models of sound sources with three applied LOADs. Specifically, three procedural models proposed by Farnell [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>] (see also Sect. <span class="InternalRef"><a href="#Sec15">2.5.1</a></span>) were chosen for investigation: (i) fire sounds employ subtractive synthesis to generate and combine hissing, crackling, and lapping features; (ii) bubbles sounds use a form of additive synthesis with frequency- and amplitude-controlled sinusoidal components representing single bubbles; (iii) wind sounds are again produced using subtractive synthesis (amplitude-modulated noise and various filtering elements to represent different wind effects). A different approach to applying LOAD was implemented for each model. Correspondingly, listening tests provided different results for each model in terms of perceived quality at different LOADs.</p><p class="Para" id="Par83">The reader interested in further discussion about audio quality should also refer to Chap. <span class="ExternalRef"><a href="478239_1_En_5_Chapter.xhtml"><span class="RefSource">5</span></a></span>.</p></section><section class="Section2 RenderAsSection2" id="Sec17"><h3 class="Heading"><span class="HeadingNumber">2.5.3 </span>Tools</h3><p class="Para" id="Par84">In spite of all the valuable research results produced so far, there is still a lack of software tools that assist the sound designer in using procedural approaches.</p><p class="Para" id="Par85">Designers working with procedural audio use a variety of audio programming <span id="ITerm49">environments</span>. Popular choices include (but are not limited to) Pure Data,<sup><a epub:type="noteref" href="#Fn6" id="Fn6_source" role="doc-noteref">6</a></sup> Max/MSP,<sup><a epub:type="noteref" href="#Fn7" id="Fn7_source" role="doc-noteref">7</a></sup> or CSound.<sup><a epub:type="noteref" href="#Fn8" id="Fn8_source" role="doc-noteref">8</a></sup> The first two in particular implement a common, dataflow-oriented paradigm [<span class="CitationRef"><a epub:type="biblioref" href="#CR62" role="doc-biblioref">62</a></span>] and use a visual patch language where “the diagram is the program”: Farnell [<span class="CitationRef"><a epub:type="biblioref" href="#CR31" role="doc-biblioref">31</a></span>] argues that this paradigm is particularly suited for procedural audio as it has a natural congruence with the abstract model resulting from the design process. On the other hand, integrating these environments into the most widespread gaming/VR engines is not straightforward: at the time of writing, some active open-source projects include libpd [<span class="CitationRef"><a epub:type="biblioref" href="#CR16" role="doc-biblioref">16</a></span>], a C library that turns Pure Data into an embeddable audio synthesis library and provides wrappers for a range of languages, and Cabbage [<span class="CitationRef"><a epub:type="biblioref" href="#CR97" role="doc-biblioref">97</a></span>], a framework for developing audio plugins in Csound, including plugins for the FMOD middleware. Commercial gaming/VR engines typically provide limited functionalities to support procedural sound design, although some recent developments may hint at an ongoing change of perspective: as an example, the Blueprint visual scripting system within the Unreal Engine has been used for dataflow-oriented procedural audio programming, also using some native synthesis (subtractive, etc.) capabilities.</p><p class="Para" id="Par89">All of the tools mentioned above still require to work at a low level of abstraction, implying that the sound designer must have the technical skills needed to deal with low-level synthesis methods and parameters, and at the same time limiting productivity. There is a clear need for tools that allow the designer to work at higher levels of abstraction. One instructive example is provided by the Sound Design Toolkit (SDT), an open-source software package developed over several years [<span class="CitationRef"><a epub:type="biblioref" href="#CR9" role="doc-biblioref">9</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>] which provides a set of sound models for the interactive generation of several acoustic phenomena. In its current embodiment, SDT is composed of a core C library exposing an API, plus a set of wrappers for Max and Pure Data, and a related collection of patches and help files. Interestingly, the collection is based on a hierarchical taxonomy of everyday sound events which follows very closely the one depicted in Fig. <span class="InternalRef"><a href="#Fig3">2.3</a></span> and implements a rich subset of its items. The designer has access to both low-level parameters (e.g., the modal frequencies of a basic solid resonator) and to high-level ones (e.g., the initial height of a bouncing object).</p><p class="Para" id="Par90">Commercial products facilitating the designer’s workflow are also far from abundant: Lesound<sup><a epub:type="noteref" href="#Fn9" id="Fn9_source" role="doc-noteref">9</a></sup> (formerly Audiogaming) sells a set of plugins for FMOD and Wwise that include procedural simulations of wind, rain, motor, and weather sounds, while for its part AudioKinetic (developer of Wwise) develops the soundseed plugin series, which include procedural generation of wind and whooshing sounds as well as impact sounds. Nemisindo<sup><a epub:type="noteref" href="#Fn10" id="Fn10_source" role="doc-noteref">10</a></sup> provides a web-based platform for real-time synthesis and manipulation of procedural audio, which stems from the FXive academic project [<span class="CitationRef"><a epub:type="biblioref" href="#CR8" role="doc-biblioref">8</a></span>], but no plugin-based integration with VR engines or audio middleware software is available at the time of writing.</p><p class="Para" id="Par93">A much-needed facilitating tool for the sound designer is one that automates part of the design process, allowing in particular for automatic tuning of the parameters of a procedural model starting from a target (e.g., recorded) sound. This would provide a means to recreate procedurally a desired sound and more in general to ease the design by providing a starting set of parameter values that can be further edited.</p><p class="Para" id="Par94">In the context of modal synthesis, various authors have proposed automatic analysis approaches for determining modal parameters from a target signal (e.g., an impact sound). In this case, the parametrization of the model is relatively simple: every mode at a given position is fully characterized by a triplet of scalars representing its frequency, decay coefficient, and gain. This generalizes to an array of gains if multiple points on the object are considered, or to continuous modal shapes as functions of spatial coordinates on the object. Ren et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR67" role="doc-biblioref">67</a></span>] proposed a method that extracts perceptually salient features from audio examples and a parameter estimation algorithm searches for the best material parameters for modal <span id="ITerm50">synthesis</span>. Based on this work, Sterling et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR82" role="doc-biblioref">82</a></span>] added a probabilistic model for the damping parameters in order to reduce the effect of external factors (object support, background noise, etc.) and non-linearities on the estimate of damping. Tiraboschi et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR87" role="doc-biblioref">87</a></span>] also presented an approach to the automatic estimation of modal parameters based on a target sound, which employs a spectral modeling algorithm to track energy envelopes of detected sinusoidal components and then performs linear regression to estimate the corresponding modal parameters.</p><p class="Para" id="Par95">While the case of solid objects and modal synthesis is a relatively simple one, the issue of automatic parameter estimation has been largely disregarded for other classes of sounds and models.</p></section></section><section class="Section1 RenderAsSection1" id="Sec18"><h2 class="Heading"><span class="HeadingNumber">2.6 </span>Conclusions</h2><p class="Para" id="Par96">Our discussion in this chapter has hopefully shown that procedural approaches offer extensive possibilities for designing sonic interactions in virtual environments. And yet as of today the number of real-world applications and tools utilizing these approaches is very limited. In fact, not much has changed since ten or fifteen years ago, when other researchers observed a similar lack of interest from the industry [<span class="CitationRef"><a epub:type="biblioref" href="#CR12" role="doc-biblioref">12</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR29" role="doc-biblioref">29</a></span>], with the same technical and cultural obstacles to adoption still in place. In a way recent technological developments have further favored the use of sample-based approaches: in particular, decreasing costs of RAM and secondary storage, as well as optimized strategies to manage caching and prefetching of sound assets, have made it possible to store ever larger amounts of data. This state of affairs mimics closely what happened in the music industry during the last three decades: physics-based techniques in particular have been around for a long time, but the higher sound quality and accuracy of samples are still preferred over the flexibility of physical models for the emulation of musical instruments.</p><p class="Para" id="Par97">Perhaps then the question is not whether procedural approaches can overcome sample-based audio, but when, i.e., under what specific circumstances. In this chapter, we have provided some elements, particularly links to a number of relevant perceptual and cognitive aspects, such as the plausibility and place illusions, the sense of embodiment, and the sense of agency. We argue that procedural audio can compete with samples in cases where either (i) very large amounts of data are needed to minimize repetition and support the plausibility illusion, or (ii) interactivity is needed beyond an event-driven logics, in order to provide tight synchronization and plausible variations with user actions, and to support her sense of agency and body ownership.</p><p class="Para" id="Par98">One example of the first circumstance is provided by wind sounds: good recordings of real wind effects are technically difficult to come by and long recordings are required to create convincing ambiences of windy scenes using looping, while on the other hand procedurally generated wind sounds achieve high levels of realism. It is therefore no surprise that the few commercially available tools for procedural sound all include wind (see Sect. <span class="InternalRef"><a href="#Sec17">2.5.3</a></span>) and have been successfully employed also in large productions.<sup><a epub:type="noteref" href="#Fn11" id="Fn11_source" role="doc-noteref">11</a></sup> While wind falls in the category of adaptive, rather than interactive sounds, two relevant examples for the second circumstance may be provided by footsteps and sliding friction (bike breaking, hinges squeaking, rubbing, etc.): beside requiring large amounts of data and randomization to avoid repetition, these sounds arise in response to complex and continuous motor actions by the user, which cannot be fully captured by an event-driven logics.</p><p class="Para" id="Par100">Future research and development should therefore focus on cases where procedural models can compete with samples, looking more deeply into the effects on the plausibility illusion, sense of agency, and sense of body ownership. From a more technical perspective, promising directions for future research include the development of dynamic LOAD techniques, as well as high-level authoring tools and automation.</p></section><div class="Acknowledgments" epub:type="acknowledgments" role="doc-acknowledgments"><div class="Heading">Acknowledgements</div><p class="SimplePara">This chapter is partly based on the ideas and materials which I developed for my course “Sound in Interaction”, held at the University of Milano for the MSc degree in Computer Science.</p></div><div class="License LicenseSubType-cc-by"><a href="https://creativecommons.org/licenses/by/4.0"><img alt="Creative Commons" src="../css/cc-by.png"/></a><p class="SimplePara"><strong class="EmphasisTypeBold ">Open Access</strong> This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (<span class="ExternalRef"><a href="http://creativecommons.org/licenses/by/4.0/"><span class="RefSource">http://​creativecommons.​org/​licenses/​by/​4.​0/​</span></a></span>), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p><p class="SimplePara">The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p></div><aside aria-labelledby="Bib1Heading" class="Bibliography" id="Bib1"><div epub:type="bibliography" role="doc-bibliography"><div class="Heading" id="Bib1Heading">References</div><ol class="BibliographyWrapper"><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">1.</div><div class="CitationContent" id="CR1">Adrien, J.-M. in Representations of Musical Signals (eds De Poli, G., Piccialli, A., Roads, C.) 269-297 (MIT Press, Cambridge, MA, 1991).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">2.</div><div class="CitationContent" id="CR2">Ament, V. T.: The Foley grail: The art of performing sound for film, games, and animation Second edition (CRC Press, New York, 2014).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.4324/9780203766880"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">3.</div><div class="CitationContent" id="CR3">An, S. S., James, D. L., Marschner, S.: Motion-driven Concatenative Synthesis of Cloth Sounds. ACM Trans. Graphics <strong class="EmphasisTypeBold ">31</strong> (July 2012).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">4.</div><div class="CitationContent" id="CR4">Avanzini, F., Rocchesso, D., Serafin, S.: Friction sounds for sensory substitution, in Proc. Int. Conf. Auditory Display (ICAD04) (Sidney, July 2004).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">5.</div><div class="CitationContent" id="CR5">Avanzini, F. in Sound to Sense, Sense to Sound. A State of the Art in Sound and Music Computing (eds Rocchesso, D., Polotti, P.) 345–396 (Logos Verlag, Berlin, 2008).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">6.</div><div class="CitationContent" id="CR6">Avanzini, F., Crosato, P. in Haptic and audio interaction design (eds Mc-Gookin, D., Brewster, S.) 24–35 (Lecture Notes in Computer Science 4129/2006, Springer Verlag, Berlin/Heidelberg, 2006).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">7.</div><div class="CitationContent" id="CR7">Avanzini, F., Serafin, S., Rocchesso, D.: Interactive simulation of rigid body interaction with friction-induced sound generation. IEEE Trans. Speech Audio Process. <strong class="EmphasisTypeBold ">13</strong>, 1073–1081 (2005).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">8.</div><div class="CitationContent" id="CR8">Bahadoran, P., Benito, A., Vassallo, T., Reiss, J. D.: FXive: A web platform for procedural sound synthesis, in Proc. 144 Audio Engin. Soc. Conv. (Milano, 2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">9.</div><div class="CitationContent" id="CR9">Baldan, S., Delle Monache, S., Rocchesso, D.: The sound design toolkit. SoftwareX <strong class="EmphasisTypeBold ">6</strong>, 255–260 (2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">10.</div><div class="CitationContent" id="CR10">Baldan, S., Lachambre, H., Delle Monache, S., Boussard, P.: Physically informed car engine sound synthesis for virtual and augmented environments, in Proc. IEEE Int. Workshop on Sonic Interactions for Virtual Environments (SIVE2015) (Arles, 2015), 21–26.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">11.</div><div class="CitationContent" id="CR11">Bormann, K.: Presence and the utility of audio spatialization. Presence: Teleoperators and Virtual Environment <strong class="EmphasisTypeBold ">14</strong>, 278–297 (2005).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">12.</div><div class="CitationContent" id="CR12">Böttcher, N.: Current problems and future possibilities of procedural audio in computer games. Journal of Gaming &amp; Virtual Worlds <strong class="EmphasisTypeBold ">5</strong>, 215–234 (2013).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">13.</div><div class="CitationContent" id="CR13">Botvinick, M., Cohen, J.: Rubber hands ’feel’ touch that eyes see. Nature <strong class="EmphasisTypeBold ">391</strong>, 756–756 (1998).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">14.</div><div class="CitationContent" id="CR14">Bresin, R., Papetti, S., Civolani, M., Fontana, F.: Expressive sonification of footstep sounds, in Proc. Interactive Sonification Workshop (Stockholm, 2010), 51–54.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">15.</div><div class="CitationContent" id="CR15">Bresin, R. et al.: Auditory feedback through continuous control of crumpling sound synthesis, in Proc. Workshop Sonic Interaction Design (CHI2008) (Firenze, 2008), 23–28.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">16.</div><div class="CitationContent" id="CR16">Brinkmann, P., Wilcox, D., Kirshboim, T., Eakin, R., Alexander, R.: Libpd: Past, Present, and Future of Embedding Pure Data, in Proc. Pure Data Convention (New York, 2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">17.</div><div class="CitationContent" id="CR17">Caspar, E. A., Cleeremans, A., Haggard, P.: The relationship between human agency and embodiment. Consciousness and cognition <strong class="EmphasisTypeBold ">33</strong>, 226–236 (2015).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">18.</div><div class="CitationContent" id="CR18">Chadwick, J. N., James, D. L.: Animating Fire with Sound. ACM Trans. Graphics <strong class="EmphasisTypeBold ">30</strong> (2011).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">19.</div><div class="CitationContent" id="CR19">Chen, L., Vroomen, J.: Intersensory binding across space and time: a tutorial review. Attention, Perception, &amp; Psychophysics <strong class="EmphasisTypeBold ">75</strong>, 790–811 (2013).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">20.</div><div class="CitationContent" id="CR20">Collins, K. in Essays on Sound and Vision (eds Richardson, J., Hawkins, S.) 263–298 (Helsinki University Press, Helsinki, 2007).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">21.</div><div class="CitationContent" id="CR21">Cook, P. R.: Real sound synthesis for interactive applications (CRC Press, 2002).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">22.</div><div class="CitationContent" id="CR22">Cummings, J. J., Bailenson, J.N.: How immersive is enough? Ameta-analysis of the effect of immersive technology on user presence. Media Psychology <strong class="EmphasisTypeBold ">19</strong>, 272–309 (2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">23.</div><div class="CitationContent" id="CR23">Dall’Avanzi, I., Yee-King, M.: Measuring the impact of level of detail for environmental soundscapes in digital games, in Proc. 146 Audio Engin. Soc. Conv. (London, 2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">24.</div><div class="CitationContent" id="CR24">David, N., Newen, A., Vogeley, K.: The “sense of agency” and its underlying cognitive and neural mechanisms. Consciousness and cognition <strong class="EmphasisTypeBold ">17</strong>, 523–534 (2008).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">25.</div><div class="CitationContent" id="CR25">Delle Monache, S., Polotti, P., Rocchesso, D.: A toolkit for explorations in sonic interaction design, in Proc. Int. Conf. Audio Mostly (AM2010) (Piteå, 2010), 1–7.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">26.</div><div class="CitationContent" id="CR26">Dobashi, Y.,Yamamoto, T., Nishita, T.: Real-time Rendering of Aerodynamic Sound using Sound Textures based on Computational Fluid Dynamics, in Proc. ACM SIGGRAPH 2003 (San Diego, 2003), 732–740.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">27.</div><div class="CitationContent" id="CR27">Durr, G., Peixoto, L., Souza, M., Tanoue, R., Reiss, J. D.: Implementation and evaluation of dynamic level of audio detail, in Proc. 56th AES Int. Conf. Audio for Games (London, 2015).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">28.</div><div class="CitationContent" id="CR28">Ernst, M. O., Bülthoff, H. H.: Merging the senses into a robust percept. TRENDS in Cognitive Sciences <strong class="EmphasisTypeBold ">8</strong>, 162–169 (2004).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">29.</div><div class="CitationContent" id="CR29">Farnell, A.: An introduction to procedural audio and its application in computer games (2007). URL <span class="ExternalRef"><a href="http://obiwannabe.co.uk/html/papers/proc-audio/proc-audio.pdf"><span class="RefSource">http://​obiwannabe.​co.​uk/​html/​papers/​proc-audio/​proc-audio.​pdf</span></a></span>. Accessed March 29,
2021.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">30.</div><div class="CitationContent" id="CR30">Farnell, A.: Designing sound (MIT Press, 2010).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">31.</div><div class="CitationContent" id="CR31">Farnell, A. in Game sound technology and player interaction: Concepts and developments (ed Grimshaw, M.) 313–339 (Information Science Reference, 2011).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">32.</div><div class="CitationContent" id="CR32">Fouad, H., Hahn, J. K., Ballas, J. A.: Perceptually Based Scheduling Algorithms for Real-time Synthesis of Complex Sonic Environments, in Proc. Int. Conf. Auditory Display (ICAD97) (Palo Alto, 1997).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">33.</div><div class="CitationContent" id="CR33">Gaver, W. W.: How do we hear in the world? Explorations of ecological acoustics. Ecological Psychology <strong class="EmphasisTypeBold ">5</strong>, 285–313 (1993).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">34.</div><div class="CitationContent" id="CR34">Gaver, W. W.: What in the world do we hear? An ecological approach to auditory event perception. Ecological Psychology <strong class="EmphasisTypeBold ">5</strong>, 1–29 (1993).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">35.</div><div class="CitationContent" id="CR35">Gibson, J. J.: The ecological approach to visual perception (Lawrence Erlbaum Associates, Mahwah, NJ, 1986).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">36.</div><div class="CitationContent" id="CR36">Giordano, B., Avanzini, F. in Multisensory Softness (ed Luca, M. D.) 49–84 (Springer Verlag, London, 2014).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">37.</div><div class="CitationContent" id="CR37">Hacıhabiboğlu, H. in Game Dynamics: Best Practices in Procedural and Dynamic Game Content Generation (eds Korn, O., Lee, N.) 47–69 (Springer International Publishing, Cham, 2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">38.</div><div class="CitationContent" id="CR38">Hendrix, C., Barfield, W.: The Sense of Presence within Auditory Virtual Environments. Presence: Teleoperators and Virtual Environment <strong class="EmphasisTypeBold ">5</strong>, 290–301 (1996).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">39.</div><div class="CitationContent" id="CR39">Hiller, L., Ruiz, P.: Synthesizing Musical Sounds by Solving the Wave Equation for Vibrating Objects: Part I. J. Audio Eng. Soc. <strong class="EmphasisTypeBold ">19</strong>, 462–470 (1971).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">40.</div><div class="CitationContent" id="CR40">Hiller, L., Ruiz, P.: Synthesizing Musical Sounds by Solving the Wave Equation for Vibrating Objects: Part II. J. Audio Eng. Soc. <strong class="EmphasisTypeBold ">19</strong>, 542–551 (1971).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">41.</div><div class="CitationContent" id="CR41">Jack, R. H., Stockman, T., McPherson, A.: Effect of latency on performer interaction and subjective quality assessment of a digital musical instrument, in Proc. Int. Conf. Audio Mostly (AM’16) (Norrköping, 2016), 116–123.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">42.</div><div class="CitationContent" id="CR42">Jørgensen, K. in Game sound technology and player interaction: Concepts and developments (ed Grimshaw, M.) 78–97 (Information Science Reference, 2011).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">43.</div><div class="CitationContent" id="CR43">Kaaresoja, T., Brewster, S., Lantz, V.: Towards the temporally perfect virtual button: touch-feedback simultaneity and perceived quality in mobile touchscreen press interactions. ACM Trans. Applied Perception <strong class="EmphasisTypeBold ">11</strong>, 1–25 (2014).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">44.</div><div class="CitationContent" id="CR44">Kelly, J. L., Lochbaum, C. C.: Speech synthesis, in Proc. 4th Int. Congr. Acoustics (Copenhagen, 1962), 1–4.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">45.</div><div class="CitationContent" id="CR45">Kilteni, K., Groten, R., Slater, M.: The sense of embodiment in virtual reality. Presence: Teleoperators and Virtual Environments <strong class="EmphasisTypeBold ">21</strong>, 373–387 (2012).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">46.</div><div class="CitationContent" id="CR46">Lago, N. P., Kon, F.: The quest for low latency, in Proc. Int. Computer Music Conf. (ICMC2004) (Miami, 2004).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">47.</div><div class="CitationContent" id="CR47">Larsson, P., Väljamäe, A., Västfjäll, D., Tajadura-Jiménez, A., Kleiner, M. in The engineering of mixed reality systems (eds Dubois, E., Gray, P., Nigay, L.) 143–163 (Springer, 2010).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">48.</div><div class="CitationContent" id="CR48">Lester, M., Boley, J.: The effects of latency on live sound monitoring, in Proc. 123 Audio Engin. Soc. Convention (New York, 2007).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">49.</div><div class="CitationContent" id="CR49">Liljedahl, M. in Game sound technology and player interaction: Concepts and developments (ed Grimshaw, M.) 22–43 (Information Science Reference, 2011).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">50.</div><div class="CitationContent" id="CR50">Liu, S., Cheng, H., Tong, Y.: Physically-Based Statistical Simulation of Rain Sound. ACM Trans. Graphics <strong class="EmphasisTypeBold ">38</strong> (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">51.</div><div class="CitationContent" id="CR51">Liu, S., Manocha, D.: Sound Synthesis, Propagation, and Rendering: A Survey. arXiv preprint. 2020.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">52.</div><div class="CitationContent" id="CR52">Luebke, D. et al.: Level of detail for 3D graphics (Morgan Kaufmann, 2003).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">53.</div><div class="CitationContent" id="CR53">Magill, R. A., Anderson, D. I.: Motor learning and control: Concepts and applications. Eleventh edition (McGraw-Hill New York, 2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">54.</div><div class="CitationContent" id="CR54">Mäki-Patola, T., Hämäläinen, P.: Latency tolerance for gesture controlled continuous sound instrument without tactile feedback, in Proc. Int. Computer Music Conf. (ICMC2004) (Miami, 2004).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">55.</div><div class="CitationContent" id="CR55">Michaels, C. F., Carello, C.: Direct Perception (Prentice-Hall, Englewood Cliffs, NJ, 1981).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">56.</div><div class="CitationContent" id="CR56">Moeck, T. et al.: Progressive perceptual audio rendering of complex scenes, in Proc. Symp. on Interactive 3D Graphics and Games (I3D’07) (Seattle, 2007), 189–196.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">57.</div><div class="CitationContent" id="CR57">Nordahl, R., Nilsson, N. C. in The Oxford handbook of interactive audio (eds Collins, K., Kapralos, B., Tessler, H.) (Oxford University Press, 2014).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">58.</div><div class="CitationContent" id="CR58">O’Regan, J. K., Noë, A.: A sensorimotor account of vision and visual consciousness. Behavioral and Brain Sciences <strong class="EmphasisTypeBold ">24</strong>, 883–917 (2001).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">59.</div><div class="CitationContent" id="CR59">Picard, C., Tsingos, N., Faure, F.: Retargetting Example Sounds to Interactive Physics-Driven Animations, in Proc. AES Conf. Audio in Games (London, 2009).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">60.</div><div class="CitationContent" id="CR60">Poeschl, S., Wall, K., Doering, N.: Integration of spatial sound in immersive virtual environments an experimental study on effects of spatial sound on presence, in Proc. IEEE Conf. Virtual Reality (Orlando, 2013), 129–130.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">61.</div><div class="CitationContent" id="CR61">Pörschmann, C.: One’s own voice in auditory virtual environments. Acta Acustica un. w. Acustica <strong class="EmphasisTypeBold ">87</strong>, 378–388 (2001).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">62.</div><div class="CitationContent" id="CR62">Puckette, M.: Max at seventeen. Computer Music J. <strong class="EmphasisTypeBold ">26</strong>, 31–43 (2002).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">63.</div><div class="CitationContent" id="CR63">Raghuvanshi, N., Lin, M. C.: Physically Based Sound Synthesis for Large-Scale Virtual Environments. IEEE Computer Graphics and Applications <strong class="EmphasisTypeBold ">27</strong>, 14–18 (2007).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">64.</div><div class="CitationContent" id="CR64">Rath, M., Rocchesso, D.: Continuous sonic feedback from a rolling ball. IEEE MultiMedia <strong class="EmphasisTypeBold ">12</strong>, 60–69 (2005).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">65.</div><div class="CitationContent" id="CR65">Rath, M., Fontana, F. in The Sounding Object (eds Rocchesso, D., Fontana, F.) 173–204 (Mondo Estremo, Firenze, 2003).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">66.</div><div class="CitationContent" id="CR66">Ren, Z., Yeh, H., Lin, M. C.: Synthesizing contact sounds between textured models, in Proc. IEEE Conf. Virtual Reality (Waltham, 2010), 139–146.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">67.</div><div class="CitationContent" id="CR67">Ren, Z., Yeh, H., Lin, M. C.: Example-guided physically based modal sound synthesis. ACM Trans. on Graphics <strong class="EmphasisTypeBold ">32</strong>, 1 (2013).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">68.</div><div class="CitationContent" id="CR68">Risset, J.-C., Wessel, D. L. in The psychology of music (ed Deutsch, D.) Second edition, 113–169 (Elsevier, 1999).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">69.</div><div class="CitationContent" id="CR69">Rocchesso, D., Bresin, R., Fernstrom, M.: Sounding objects. IEEE MultiMedia <strong class="EmphasisTypeBold ">10</strong>, 42–52 (2003).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">70.</div><div class="CitationContent" id="CR70">Schwarz, D., Cahen, R., Brument, F., Ding, H., Jacquemin, C.: Sound level of detail in interactive audiographic 3D scenes, in Proc. Int. Computer Music Conf. (ICMC2011) (Huddersfield, 2011), 312–315.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">71.</div><div class="CitationContent" id="CR71">Selfridge, R., Moffat, D., Reiss, J. D.: Sound synthesis of objects swinging through air using physical models. Applied Sciences <strong class="EmphasisTypeBold ">7</strong>, 1177 (2017).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.3390/app7111177"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">72.</div><div class="CitationContent" id="CR72">(eds Sheridan, T. B., Furness, T. A.) Premier Issue <strong class="EmphasisTypeBold ">1</strong> (1992).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">73.</div><div class="CitationContent" id="CR73">Sikström, E., De Götzen, A., Serafin, S.: The role of sound in the sensation of ownership of a pair of virtual wings in immersive VR, in Proc. Int. Conf. Audio Mostly (AM’14) (Aalborg, 2014), 1–6.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">74.</div><div class="CitationContent" id="CR74">Sikström, E., De Götzen, A., Serafin, S.: Self-characterstics and sound in immersive virtual reality - Estimating avatar weight from footstep sounds, in Proc. IEEE Conf. Virtual Reality (Arles, 2015), 283–284.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">75.</div><div class="CitationContent" id="CR75">Sinclair, J.-L.: Principles of Game Audio and Sound Design: Sound Design and Audio Implementation for Interactive and Immersive Media (CRC Press, 2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">76.</div><div class="CitationContent" id="CR76">Skarbez, R., Brooks Jr, F. P., Whitton, M. C.: A survey of presence and related concepts. ACM Computing Surveys <strong class="EmphasisTypeBold ">50</strong>, 1–39 (2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">77.</div><div class="CitationContent" id="CR77">Slater, M.: Place illusion and plausibility can lead to realistic behaviour in immersive virtual environments. Phil. Trans. R. Soc. B <strong class="EmphasisTypeBold ">364</strong>, 3549–3557 (2009).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">78.</div><div class="CitationContent" id="CR78">Smith, J. O.: Physical Audio Signal Processing. Online book. 2010. URL <span class="ExternalRef"><a href="http://ccrma.stanford.edu/Eoejos/pasp/"><span class="RefSource">http://​ccrma.​stanford.​edu/​Ëoejos/​pasp/​</span></a></span>. Accessed March 11, 2021.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">79.</div><div class="CitationContent" id="CR79">Smith, J. O.: Virtual acoustic musical instruments: Review and update. J. New Music Res. <strong class="EmphasisTypeBold ">33</strong>, 283–304 (2004).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">80.</div><div class="CitationContent" id="CR80">Sonnenschein, D.: Sound design: The expressive power of music, voice, and sound effects in cinema (Michael Wiese Productions, 2001).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">81.</div><div class="CitationContent" id="CR81">Human Walking in Virtual Environments: Perception, Technology, and Applications (eds Steinicke, F., Visell, Y., Campos, J., Lecuyer, A.) (Springer Verlag, New York, 2013).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">82.</div><div class="CitationContent" id="CR82">Sterling, A., Rewkowski, N., Klatzky, R. L., Lin, M. C.: Audio-Material Reconstruction for Virtualized Reality Using a Probabilistic Damping Model. IEEE Trans. on Visualization and Comp. Graphics <strong class="EmphasisTypeBold ">25</strong>, 1855–1864 (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">83.</div><div class="CitationContent" id="CR83">Stevenson, R. A. et al.: Identifying and quantifying multisensory integration: a tutorial review. Brain Topography <strong class="EmphasisTypeBold ">27</strong>, 707–730 (2014).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">84.</div><div class="CitationContent" id="CR84">Stockburger, A.: The game environment from an auditory perspective, in Proc. Level Up: Digital Games Research Conference (eds Copier, M., Raessens, J.) (Utrecht, 2003).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">85.</div><div class="CitationContent" id="CR85">Tajadura-Jiménez, A. et al.: As light as your footsteps: altering walking sounds to change perceived body weight, emotional state and gait, in Proc. ACM Conf. on Human Factors in Computing Systems (Seoul, 2015), 2943–2952.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">86.</div><div class="CitationContent" id="CR86">Takala, T., Hahn, J.: Sound Rendering. Computer Graphics <strong class="EmphasisTypeBold ">26</strong>, 211–220 (1992).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">87.</div><div class="CitationContent" id="CR87">Tiraboschi, M., Avanzini, F., Ntalampiras, S.: Spectral Analysis for Modal Parameters Linear Estimate, in Proc. Int. Conf. Sound and Music Computing (SMC2020) (Torino, 2020), 276–283.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">88.</div><div class="CitationContent" id="CR88">Tsingos, N., Gallo, E., Drettakis, G.: Perceptual audio rendering of complex virtual environments. ACM Trans. on Graphics (TOG) <strong class="EmphasisTypeBold ">23</strong>, 249–258 (2004).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">89.</div><div class="CitationContent" id="CR89">Välimäki, V., Pakarinen, J., Erkut, C., Karjalainen, M.: Discrete-time modelling of musical instruments. Rep. Prog. Phys. <strong class="EmphasisTypeBold ">69</strong>, 1–78 (2006).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">90.</div><div class="CitationContent" id="CR90">Van den Doel, K.: Physically based models for liquid sounds. ACM Trans. Applied Perception <strong class="EmphasisTypeBold ">2</strong>, 534–546 (2005).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">91.</div><div class="CitationContent" id="CR91">Van den Doel, K., Kry, P. G., Pai, D. K.: FoleyAutomatic: Physically-based Sound Effects for Interactive Simulation and Animation, in Proc. ACM SIGGRAPH 2001 (Los Angeles, 2001), 537–544.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">92.</div><div class="CitationContent" id="CR92">Van den Doel, K., Pai, D. K. in Audio Anecdotes (ed Greenebaum, K.) (AK Peters, Natick, MA, 2004).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">93.</div><div class="CitationContent" id="CR93">Van Vugt, F. T., Tillmann, B.: Thresholds of auditory-motor coupling measured with a simple task in musicians and non-musicians: was the sound simultaneous to the key press? PLoS One <strong class="EmphasisTypeBold ">9</strong>, e87176 (2014).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1371/journal.pone.0087176"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">94.</div><div class="CitationContent" id="CR94">Varela, F., Thompson, E., Rosch, E.: The Embodied Mind (MIT Press, Cambridge, MA, 1991).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.7551/mitpress/6730.001.0001"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">95.</div><div class="CitationContent" id="CR95">Visell, Y. et al.: Sound design and perception in walking interactions. Int. J. Human-Computer Studies <strong class="EmphasisTypeBold ">67</strong>, 947–959 (2009).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">96.</div><div class="CitationContent" id="CR96">Vroomen, J., Keetels, M.: Perception of intersensory synchrony: a tutorial review. Attention, Perception, &amp; Psychophysics <strong class="EmphasisTypeBold ">72</strong>, 871–884 (2010).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">97.</div><div class="CitationContent" id="CR97">Walsh, R.: Audio plugin development with cabbage, in Proc. Linux Audio Conf. (Maynooth, 2011), 47–53.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">98.</div><div class="CitationContent" id="CR98">Wang, K., Liu, S.: Example-based synthesis for sound of ocean waves caused by bubble dynamics. Comput. Anim. and Virtual Worlds <strong class="EmphasisTypeBold ">29</strong>, e1835 (2018).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1002/cav.1835"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">99.</div><div class="CitationContent" id="CR99">Wessel, D.,Wright, M.: Problems and prospects for intimate musical control of computers. Computer Music J. <strong class="EmphasisTypeBold ">26</strong>, 11–22 (2002).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">100.</div><div class="CitationContent" id="CR100">Zheng, C., James, D. L.: Rigid-body fracture sound with precomputed soundbanks. ACM Trans. Graphics <strong class="EmphasisTypeBold ">29</strong> (2010).</div></li></ol></div></aside><aside aria-label="Footnotes" class="FootnoteSection" epub:type="footnotes"><div class="Heading">Footnotes</div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn1_source">1</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn1" role="doc-footnote"><p class="Para" id="Par6">For the sake of clarity, in this chapter, we use the term “sample” in its commonly accepted meaning of pre-recorded/pre-processed sound excerpt, rather than that of a single value of a digital signal.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn2_source">2</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn2" role="doc-footnote"><p class="Para" id="Par31">In this context, the label “ecological” is associated to two main concepts: first, perception is an achievement of animal-environment systems, not simply animals, or their brains; second, the main purpose of perception is to guide action, so a theory of perception cannot ignore what animals do.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn3_source">3</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn3" role="doc-footnote"><p class="Para" id="Par35">Skarbez et al. [<span class="CitationRef"><a epub:type="biblioref" href="#CR76" role="doc-biblioref">76</a></span>] consider a third component, the social presence illusion, which we do not address here.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn4_source">4</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn4" role="doc-footnote"><p class="Para" id="Par50"><span class="ExternalRef"><a href="https://www.fmod.com/"><span class="RefSource">https://​www.​fmod.​com/​</span></a></span>.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn5_source">5</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn5" role="doc-footnote"><p class="Para" id="Par51"><span class="ExternalRef"><a href="https://www.audiokinetic.com/products/wwise/"><span class="RefSource">https://​www.​audiokinetic.​com/​products/​wwise/​</span></a></span>.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn6_source">6</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn6" role="doc-footnote"><p class="Para" id="Par86"><span class="ExternalRef"><a href="https://puredata.info/"><span class="RefSource">https://​puredata.​info/​</span></a></span>.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn7_source">7</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn7" role="doc-footnote"><p class="Para" id="Par87"><span class="ExternalRef"><a href="https://cycling74.com/"><span class="RefSource">https://​cycling74.​com/​</span></a></span>.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn8_source">8</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn8" role="doc-footnote"><p class="Para" id="Par88"><span class="ExternalRef"><a href="https://csound.com/"><span class="RefSource">https://​csound.​com/​</span></a></span>.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn9_source">9</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn9" role="doc-footnote"><p class="Para" id="Par91"><span class="ExternalRef"><a href="https://lesound.io/"><span class="RefSource">https://​lesound.​io/​</span></a></span>.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn10_source">10</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn10" role="doc-footnote"><p class="Para" id="Par92"><span class="ExternalRef"><a href="https://nemisindo.com"><span class="RefSource">https://​nemisindo.​com</span></a></span>.</p></div><div class="ClearBoth"> </div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn11_source">11</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn11" role="doc-footnote"><p class="Para" id="Par99">As an example, the procedural wind simulator by Lesound has been reportedly used for generating ambiences in Quentin Tarantino’s Django Unchained, see <span class="ExternalRef"><a href="http://lesound.io/product/audiowind-pro/"><span class="RefSource">http://​lesound.​io/​product/​audiowind-pro/​</span></a></span>.</p></div><div class="ClearBoth"> </div></div></aside></div></div></body></html>