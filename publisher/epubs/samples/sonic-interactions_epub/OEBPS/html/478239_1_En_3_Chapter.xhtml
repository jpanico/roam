<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops"><head><title>Interactive and Immersive Auralization</title><meta content="text/html; charset=utf-8" http-equiv="content-type"/><link href="../css/springer_epub.css" rel="styleSheet" type="text/css"/></head><body><div epub:type="chapter" role="doc-chapter"><div class="ChapterContextInformation"><div class="ContextInformation" id="Chap3"><div class="ChapterCopyright">© The Author(s) 2023</div><span class="ContextInformationAuthorEditorNames">M. Geronazzo, S. Serafin<span class="CollaboratorDesignation"> (eds.)</span></span><span class="ContextInformationBookTitles"><span class="BookTitle">Sonic Interactions in Virtual Environments</span></span><span class="ContextInformationSeries"><span class="SeriesTitle" lang="en">Human–Computer Interaction Series</span></span><span class="ChapterDOI"><a href="https://doi.org/10.1007/978-3-031-04021-4_3">https://doi.org/10.1007/978-3-031-04021-4_3</a></span></div></div><!--Begin Abstract--><div class="MainTitleSection"><h1 class="ChapterTitle" lang="en">3. Interactive and Immersive Auralization</h1></div><div class="AuthorGroup"><div class="AuthorNames"><span class="Author"><span class="AuthorName">Nikunj Raghuvanshi</span><sup><a href="#Aff34">1</a> <a aria-label="Contact information for this author" href="#ContactOfAuthor1"><span class="ContactIcon"> </span></a></sup> and </span><span class="Author"><span class="AuthorName">Hannes Gamper</span><sup><a href="#Aff34">1</a> <a aria-label="Contact information for this author" href="#ContactOfAuthor2"><span class="ContactIcon"> </span></a></sup></span></div><div class="Affiliations"><div class="Affiliation" id="Aff34"><span class="AffiliationNumber">(1)</span><div class="AffiliationText">Microsoft Research, Redmond, USA</div></div><div class="ClearBoth"> </div></div><div class="Contacts"><div class="Contact" id="ContactOfAuthor1"><div class="ContactIcon"> </div><div class="ContactAuthorLine"><span class="AuthorName">Nikunj Raghuvanshi</span> (Corresponding author)</div><div class="ContactAdditionalLine"><span class="ContactType">Email: </span><a href="mailto:nikunjr@microsoft.com">nikunjr@microsoft.com</a></div></div><div class="Contact" id="ContactOfAuthor2"><div class="ContactIcon"> </div><div class="ContactAuthorLine"><span class="AuthorName">Hannes Gamper</span></div><div class="ContactAdditionalLine"><span class="ContactType">Email: </span><a href="mailto:hannes.gamper@microsoft.com">hannes.gamper@microsoft.com</a></div></div></div></div><section class="Abstract" id="Abs1" lang="en" role="doc-abstract"><h2 class="Heading">Abstract</h2><p class="Para" id="Par1">Real-time auralization is essential in virtual reality (VR), gaming, and architecture to enable an immersive audio-visual experience. The audio rendering must be congruent with visual feedback and respond with minimal delay to interactive events and user motion. The wave nature of sound poses critical challenges for plausible and immersive rendering and leads to enormous computational costs. These costs have only increased as virtual scenes have progressed away from enclosures toward complex, city-scale scenes that mix indoor and outdoor areas. However, hard real-time constraints must be obeyed while supporting numerous dynamic sound sources, frequently within a tightly limited computational budget. In this chapter, we provide a general overview of VR auralization systems and approaches that allow them to meet such stringent requirements. We focus on the mathematical foundation, perceptual considerations, and application-specific design requirements of practical systems today, and the future challenges that remain.</p></section><!--End Abstract--><div class="Fulltext"><section class="Section1 RenderAsSection1" id="Sec1"><h2 class="Heading"><span class="HeadingNumber">3.1 </span>Introduction</h2><p class="Para" id="Par2">Audition and vision are unique among our senses: they perceive propagating waves. As a result, they bring us detailed information not only of our immediate surroundings but of the world much beyond as well. Imagine talking to a friend in a cafe, the door is open, and outside is a bustling city intersection. While touch and smell give a detailed sense of our immediate surroundings, sight and sound tell us we are conversing with a friend, surrounded by other people in the cafe, immersed in a city, its sounds streaming in through the door. Virtual reality ultimately aims to re-create this sense of presence and immersion in a virtual environment, enabling a vast array of applications for society, ranging from entertainment to architecture and social interaction without the constraints of distance.</p><p class="Para" id="Par3"><strong class="EmphasisTypeBold ">Rendering.</strong> To reproduce the audio-visual <span id="ITerm1">experience</span> given in the example above, one requires a dynamic, digital 3D simulation of the world describing how <em class="EmphasisTypeItalic ">both</em> light and sound would be radiated, propagated, and perceived by an observer immersed in the computed virtual fields of light and sound. The world model usually takes the form of a 3D geometric description composed of triangulated meshes and surface materials. Sources of light and sound are specified with their 3D positions and radiative properties, including their directivity and the energy emitted within the perceivable frequency range. Given this information as input, special algorithms produce dynamic audio-visual signals that are displayed to the user via screens and speaker arrays or stereoscopic head-mounted displays and near-to-ear speakers or headphones. This is the overall process of <em class="EmphasisTypeItalic ">rendering</em><span id="ITerm2"/>, whose two components are <em class="EmphasisTypeItalic ">visualization</em> and <em class="EmphasisTypeItalic "><span id="ITerm3">auralization</span>
</em> (or visual- and audio-rendering).</p><p class="Para" id="Par4">Rendering has been a central problem in both the graphics and audio communities for decades. While the initial thrust for graphics came from computer-aided design applications, within audio, room acoustic auralization of planned auditoria and concert halls was a central driving force. The technical challenge with rendering is that modeling propagation in complex worlds is immensely compute-intensive. A naïve implementation of classical physical laws governing optics and acoustics is found to be many orders of magnitude slower than required (elaborated in Sect. <span class="InternalRef"><a href="#Sec3">3.2.1</a></span>). Furthermore, the exponential increase in compute power governed by Moore’s law has begun to stall in the last decade due to fundamental physical limits [<span class="CitationRef"><a epub:type="biblioref" href="#CR97" role="doc-biblioref">97</a></span>]. These two facts together mean that modeling propagation quickly enough for practical use requires research into specialized system architectures and simulation algorithms.</p><p class="Para" id="Par5"><strong class="EmphasisTypeBold ">Perception and Interactivity.</strong> A common theme in rendering research is that quantitative accuracy as required in engineering applications is not the primary goal. Rather, perception plays the central role: one must find ways to compute those aspects of physical phenomena that inform our sensory system. Consequently, initial graphics research in the 1970s focused on visible-surface determination [<span class="CitationRef"><a epub:type="biblioref" href="#CR54" role="doc-biblioref">54</a></span>] to convey spatial relations and object silhouettes, while initial room <span id="ITerm4">acoustics</span> research focused on reverberation time [<span class="CitationRef"><a epub:type="biblioref" href="#CR60" role="doc-biblioref">60</a></span>] to convey presence in a room and indicate its size. With that foundation, subsequent research has been devoted toward increasing the amount of detail to reach “perceptually authentic” audio-visual rendering: one that is indistinguishable from an audio-visual capture of a real scene. Research has focused on the coupled problems of increasing our knowledge of psycho-physics, and designing fast techniques that leverage this knowledge to reduce computation while providing the means to test new psycho-physical hypotheses.</p><p class="Para" id="Par6">The interactivity of virtual reality and games adds an additional dimension of difficulty. In linear media such as movies, the sequence of events is fixed, and computation times of hours or days for pre-rendered digital content can be acceptable, with human assistance provided as necessary. However, interactive applications cannot be pre-rendered in this way, as the user actions are not known in advance. Instead, the computer must perform <em class="EmphasisTypeItalic ">real-time <span id="ITerm5">rendering</span>
</em>: as events unfold based on user input, the system must model how the scene would look and sound from moment to moment as the user moves and interacts with the virtual world. It must do so with minimal latency of about 10–100 ms, depending on the application. Audio introduces the additional challenge of a <em class="EmphasisTypeItalic ">hard</em> real-time deadline. While a visual frame rendered slightly late is not ideal but perhaps acceptable, audio lags may result in silent gaps in the output. Such signal discontinuities annoy the user and break immersion and <span id="ITerm6">presence</span>. Therefore, auralization systems in VR tend to prioritize computational efficiency and perceptual <span id="ITerm7">plausibility</span> while building toward perceptual <span id="ITerm8">authenticity</span> from that starting point.</p><p class="Para" id="Par7"><strong class="EmphasisTypeBold ">Goal.</strong> The purpose of this chapter is to present the fundamental concepts and design principles of modern real-time auralization systems, with an emphasis on recent developments in virtual reality and gaming applications. We do not aim for an exhaustive treatment of the theory and methods in the field. For such a treatment, we refer the reader to Vorländer’s treatise on the subject [<span class="CitationRef"><a epub:type="biblioref" href="#CR102" role="doc-biblioref">102</a></span>].</p><p class="Para" id="Par8"><strong class="EmphasisTypeBold ">Organization.</strong> We begin by outlining the computational challenges and the resulting architectural design choices of real-time auralization systems in Sect. <span class="InternalRef"><a href="#Sec2">3.2</a></span>. This architecture is then formalized via the Bidirectional Impulse Response (BIR), Head-Related Transfer Functions (HRTFs), and rendering equation in Sect. <span class="InternalRef"><a href="#Sec7">3.3</a></span>. In Sect. <span class="InternalRef"><a href="#Sec13">3.4</a></span>, we summarize relevant psycho-acoustic phenomena in complex VR scenes and elaborate on how one must balance a believable rendering with real-time constraints among other system design factors in Sect. <span class="InternalRef"><a href="#Sec18">3.5</a></span>. We then discuss in Sect. <span class="InternalRef"><a href="#Sec21">3.6</a></span> how the formalism, perception, and design constraints come together into the deterministic-statistical decomposition of the BIR, a powerful idea employed by most auralization systems. Section <span class="InternalRef"><a href="#Sec24">3.7</a></span> provides a brief overview of the two common approaches to acoustical simulation: geometric and wave-based methods. In Sect. <span class="InternalRef"><a href="#Sec27">3.8</a></span>, we discuss some example systems in use today in more depth, to illustrate how they balance the various constraints informing their design decisions, followed by the conclusion in Sect. <span class="InternalRef"><a href="#Sec32">3.9</a></span>.</p></section><section class="Section1 RenderAsSection1" id="Sec2"><h2 class="Heading"><span class="HeadingNumber">3.2 </span>Architecture of Real-time Auralization Systems</h2><p class="Para" id="Par9">In this section, we discuss the specific physical aspects of sound that make it computationally difficult to model, which motivates a modular, efficient system architecture.</p><section class="Section2 RenderAsSection2" id="Sec3"><h3 class="Heading"><span class="HeadingNumber">3.2.1 </span>Computational Cost</h3><div class="Para" id="Par10"><span id="ITerm9">To</span> understand the specific modeling concerns of auralization, it helps to juxtapose with light simulation in games and VR applications. In particular <div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par11"><strong class="EmphasisTypeBold ">Speed:</strong> The propagation speed of sound is low enough that we perceive its various transient aspects such as initial reflections and reverberation, which carry distinct perceptible information, while light propagation can be treated as instantaneous;</p></li><li><p class="Para" id="Par12"><strong class="EmphasisTypeBold ">Phase:</strong> Everyday sounds are often coherent or harmonic signals whose phase must be treated carefully throughout the auralization pipeline to avoid audible distortions such as signal discontinuities, whereas natural light sources tend to be incoherent;</p></li><li><p class="Para" id="Par13"><strong class="EmphasisTypeBold ">Wavelength:</strong> Audible sound wavelengths are comparable to the size of architectural and human features (cm to m) which makes wave diffraction ubiquitous. Unlike visuals, audible sound is not limited by line of sight.</p></li></ul></div>
</div><p class="Para" id="Par14">Given the unique characteristics of sound <span id="ITerm10">propagation</span> outlined above, auralization must begin with a fundamental treatment of sound as a transient, coherent wave phenomenon, while lighting can assume a much simpler geometric formulation of ray propagation for computing a stochastic, steady-state solution [<span class="CitationRef"><a epub:type="biblioref" href="#CR57" role="doc-biblioref">57</a></span>]. Auralization must carefully approximate the relevant physical mechanisms underlying the vibration of objects, propagation in air, and scattering by the listener’s body. All these mechanisms require modeling highly oscillatory wave fields that must be sufficiently sampled in space and time, giving rise to the tremendous computational expense of brute-force simulation.</p><p class="Para" id="Par15">Assume some physical domain of interest with diameter <span class="InlineEquation" id="IEq1"><img alt="$$\mathcal {D}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq1.png" style="width:1.12em"/></span>, the highest frequency of interest <span class="InlineEquation" id="IEq2"><img alt="$$\nu _{\textrm{max}}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq2.png" style="width:2em"/></span> and speed of propagation <em class="EmphasisTypeItalic ">c</em>. The smallest propagating wavelength of interest is <span class="InlineEquation" id="IEq3"><img alt="$$c/\nu _{\textrm{max}}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq3.png" style="width:3em"/></span>. Thus, the total degrees of freedom in the space-time volume of interest are <span class="InlineEquation" id="IEq4"><img alt="$$N_{dof}=(2\mathcal {D}\nu _{\textrm{max}}/c)^4$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq4.png" style="width:8.88em"/></span>. The factor of two is due to the Nyquist limit which enforces two degrees of freedom per oscillation. As an example, for full audible bandwidth simulation of sound propagation up to <span class="InlineEquation" id="IEq5"><img alt="$$\nu _{\textrm{max}}=20,\!000$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq5.png" style="width:6.12em"/></span> Hz in a scene that is <span class="InlineEquation" id="IEq6"><img alt="$$\mathcal {D}=100$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq6.png" style="width:3.81em"/></span> m across, with <span class="InlineEquation" id="IEq7"><img alt="$$c=340$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq7.png" style="width:3.44em"/></span> m/s in air: <span class="InlineEquation" id="IEq8"><img alt="$$N_{dof}=1.9\times 10^{16}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq8.png" style="width:7.81em"/></span>. For an update interval of 60 ms to meet latency requirements for interactive listener head orientation updates [<span class="CitationRef"><a epub:type="biblioref" href="#CR22" role="doc-biblioref">22</a></span>], one would thus need a computational rate of over 100 PetaFLOPS. By comparison, a typical game or VR application will allocate a single CPU core for audio with a computational rate in the range of tens of GigaFLOPS, which is too slow by a factor of at least one million. This gap motivates research in the area.</p></section><section class="Section2 RenderAsSection2" id="Sec4"><h3 class="Heading"><span class="HeadingNumber">3.2.2 </span>Modular Design</h3><div class="Para" id="Par16">Since pioneering work in the 1990s such as DIVA [<span class="CitationRef"><a epub:type="biblioref" href="#CR86" role="doc-biblioref">86</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR96" role="doc-biblioref">96</a></span>], most real-time auralization systems follow a modular architecture shown in Fig. <span class="InternalRef"><a href="#Fig1">3.1</a></span>. This architecture results in a flexible implementation and significant reduction of computational complexity, without substantially impacting simulation accuracy in cases of practical interest.<figure class="Figure" id="Fig1"><div class="MediaObject" id="MO1"><img alt="" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Fig1_HTML.png" style="width:33.95em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.1</span><p class="SimplePara">Modular architecture of real-time auralization systems. The propagation of sound emitted from each source is simulated within the 3D environment to compute a directional sound field immersing the listener. This field is given to the spatializer component that computes appropriate transducer signals for headphone or speaker playback</p></div></figcaption></figure>
</div><div class="Para" id="Par17">Rather than simulating the global scene as a single system which might be prohibitively expensive (see Sect. <span class="InternalRef"><a href="#Sec3">3.2.1</a></span>), the problem is divided into three components in a causal chain without feedback:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par18"><strong class="EmphasisTypeBold ">Production:</strong> Sound is first produced at the source due to vibration, which, combined with local self-scattering, results in a direction-dependent radiated source signal;<span id="ITerm11"/>
</p></li><li><p class="Para" id="Par19"><strong class="EmphasisTypeBold ">Propagation:</strong> The radiated sound diffracts, scatters, and reflects in the scene to result in a direction-dependent sound field at the listener location;<span id="ITerm12"/>
</p></li><li><p class="Para" id="Par20"><strong class="EmphasisTypeBold ">Spatialization:</strong> The sound field is heard by the listener. The spatialization component computes transducer signals for playback, taking the listener’s head orientation into account. In the case of using headphones, this implies accounting for scattering due to the listener’s head and shoulders, as described by <span id="ITerm13">the</span> head-related transfer function (HRTF)<span id="ITerm14"/>.</p></li></ul></div>
</div><p class="Para" id="Par21">Our focus in this chapter will be on the latter two components; sound production techniques such as physical-modeling synthesis are covered in Chap. <span class="ExternalRef"><a href="478239_1_En_2_Chapter.xhtml"><span class="RefSource">2</span></a></span>. Here, we assume a source modeled as a (monophonic) radiated signal combined with a direction-dependent radiation pattern.</p><p class="Para" id="Par22">This separation of the auralization problem into different components is key for efficient computation. Firstly, the perceptual characteristics of all three components may be studied separately and then approximated with tailored numerical methods. Secondly, since the final rendering is composed of these separate models, they can be flexibly modified at runtime. For instance, a source’s sound and directivity pattern may be updated, or the listener orientation may change, without expensive re-computation of global sound propagation. Section <span class="InternalRef"><a href="#Sec7">3.3</a></span> will formalize this idea.</p><p class="Para" id="Par23"><strong class="EmphasisTypeBold ">Limitations.</strong> This architecture is not a good fit for cases with <span id="ITerm15">strong</span> near-field interaction. For instance, if the listener’s head is close to a wall, there can be non-negligible multiple scattering, so the feedback between propagation and spatialization cannot be ignored. This can be an important scenario in VR [<span class="CitationRef"><a epub:type="biblioref" href="#CR69" role="doc-biblioref">69</a></span>]. Similarly, if one plays a trumpet with its bell very close to a surface, the resonant modes and radiated sound will be modified, much like placing a mute, which is a case where there is feedback between all three components outlined above. Thus, numerical simulations for musical acoustics tend to be quite challenging. The interested reader can consult Bilbao’s text on the subject [<span class="CitationRef"><a epub:type="biblioref" href="#CR12" role="doc-biblioref">12</a></span>] and more recent overview [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>]. In the computer graphics community, the work in [<span class="CitationRef"><a epub:type="biblioref" href="#CR104" role="doc-biblioref">104</a></span>] also shows sound production and propagation modeled directly without the separability assumption, with special emphasis on handling dynamic geometry, for application in computer animation. Such simulations tend to be off-line, but modern graphics cards have become fast enough for approximate modeling of interactive 2D wind instruments in real-time [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>].</p></section><section class="Section2 RenderAsSection2" id="Sec5"><h3 class="Heading"><span class="HeadingNumber">3.2.3 </span>Propagation</h3><p class="Para" id="Par24">The <span id="ITerm16">propagation</span> component takes the locations of a source and listener in the scene to predict the scene’s acoustic response, modeling salient effects such as diffracted occlusion, initial reflections, and reverberation. Combined with source sounds and radiation patterns, it outputs a directional sound field to the listener. Propagation is usually the most compute-intensive portion of an auralization pipeline, motivating many techniques and systems, which we will discuss in Sects. <span class="InternalRef"><a href="#Sec24">3.7</a></span> and <span class="InternalRef"><a href="#Sec27">3.8</a></span>. The methods have two assumptions in common.</p><p class="Para" id="Par25"><strong class="EmphasisTypeBold ">Linearity.</strong> For most auralization applications, it is safe to assume that sound amplitudes remain low enough to obey ideal linear propagation, modeled by the scalar wave equation. As a result, the sound field at the listener location is a linear summation of contributions from all sound sources. There are some cases in games and VR when the assumption of linearity may be violated, for instance with explosions or brass instruments. In most such cases, the non-linear behavior is restricted to the vicinity of the event and may be treated via a first-order perturbative approximation which amounts to linear propagation with a locally varying sound speed [<span class="CitationRef"><a epub:type="biblioref" href="#CR4" role="doc-biblioref">4</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR27" role="doc-biblioref">27</a></span>].</p><p class="Para" id="Par26"><strong class="EmphasisTypeBold ">Quasi-static scene configuration.</strong> Interactive scenes are dynamic, but most propagation methods assume that the problem may be treated as quasi-static. At some fixed update rate, such as a visual frame, they take a static snapshot of the scene shape as well as the locations of the source and listener within it. Then propagation is modeled assuming a linear, time-invariant system for the duration of the visual frame. The computed response for each sound source is smoothly interpolated over frames to ensure a dynamic rendering free of artifacts to the listener.</p><p class="Para" id="Par27">Fast-moving sources need to be treated with additional care as direct interpolation of acoustic responses can become error-prone [<span class="CitationRef"><a epub:type="biblioref" href="#CR80" role="doc-biblioref">80</a></span>]. An important related aspect is the <span id="ITerm17">Doppler Shift</span> on first arrival, a salient, audible effect. It may be approximated in the source model by modifying the radiated signal based on source and listener velocities, or by interpolating the propagation delay of the initial sound. Another case violating the quasi-static assumption are aero-acoustic sounds radiated from fast object motion through the air. These can be approximated within the source model with Lighthill’s acoustic analogy [<span class="CitationRef"><a epub:type="biblioref" href="#CR53" role="doc-biblioref">53</a></span>], with subsequent linear <span id="ITerm18">propagation</span> for real-time rendering [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR31" role="doc-biblioref">31</a></span>].</p></section><section class="Section2 RenderAsSection2" id="Sec6"><h3 class="Heading"><span class="HeadingNumber">3.2.4 </span>Spatialization</h3><p class="Para" id="Par28">In a virtual reality scenario, the target of the audio rendering engine is typically a listener located within the virtual scene experiencing the virtual acoustic environment with both ears. For this experience to feel plausible or natural, sound should be rendered to the user’s ears as if they were actually present in the virtual scene. The architecture in Fig. <span class="InternalRef"><a href="#Fig1">3.1</a></span> neglects the effect of the listener on global sound propagation. The <em class="EmphasisTypeItalic ">spatialization</em><span id="ITerm19"/> system (shown to the right in the figure) inserts the listener virtually into the scene and requires additional processing. A properly spatialized virtual sound source should be perceived by the listener as emanating from a given location. In the simplest case of free-field propagation, a sound source can be positioned virtually by convolving the source signal with a pair of filters (also known as <em class="EmphasisTypeItalic ">head-related transfer functions</em> (HRTFs))<span id="ITerm20"/>. This results in two ear input signals that can be presented directly to the listener over headphones. For a more complex virtual scene containing multiple sound sources as well as their acoustic interactions with the virtual environment, spatialization entails encoding appropriate localization cues to the sound field at the listener’s ear entrances. Common approaches include spherical-harmonics based rendering (“Ambisonics”) [<span class="CitationRef"><a epub:type="biblioref" href="#CR42" role="doc-biblioref">42</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR67" role="doc-biblioref">67</a></span>] as well as object-based rendering [<span class="CitationRef"><a epub:type="biblioref" href="#CR17" role="doc-biblioref">17</a></span>].</p><p class="Para" id="Par29"><strong class="EmphasisTypeBold ">HRTFs.</strong> If the sound is played back to the listener via headphones, this implies simulating the filtering that sound undergoes in a real sound field as it enters the ear entrances, due to reflections and scattering from the listener’s torso, head, and pinnae. A convenient way to describe this filtering behavior is via the HRTFs. The HRTFs are a function of the direction of arrival and contain the localization cues that the human auditory system decodes to determine the direction of an incoming wavefront. HRTFs for a particular listener are usually constructed via measurements in an anechoic chamber [<span class="CitationRef"><a epub:type="biblioref" href="#CR40" role="doc-biblioref">40</a></span>], though recent efforts exist to derive HRTFs for a listener on the fly without an anechoic chamber [<span class="CitationRef"><a epub:type="biblioref" href="#CR50" role="doc-biblioref">50</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR61" role="doc-biblioref">61</a></span>], by adapting or <em class="EmphasisTypeItalic ">personalizing</em> existing HRTF databases using anthropometric features [<span class="CitationRef"><a epub:type="biblioref" href="#CR15" role="doc-biblioref">15</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR38" role="doc-biblioref">38</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR41" role="doc-biblioref">41</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR89" role="doc-biblioref">89</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR106" role="doc-biblioref">106</a></span>], or by capturing image or depth data to model the HRTFs numerically [<span class="CitationRef"><a epub:type="biblioref" href="#CR20" role="doc-biblioref">20</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR58" role="doc-biblioref">58</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR65" role="doc-biblioref">65</a></span>]. For a review of <span id="ITerm21">HRTF personalization</span> techniques, refer to Chap. <span class="ExternalRef"><a href="478239_1_En_4_Chapter.xhtml"><span class="RefSource">4</span></a></span> and see [<span class="CitationRef"><a epub:type="biblioref" href="#CR48" role="doc-biblioref">48</a></span>]. The HRTFs can be tabulated as two spherical functions <span class="InlineEquation" id="IEq9"><img alt="$$H^{\{l,r\}}(s,t)$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq9.png" style="width:4.76em"/></span> that encapsulate the angle-dependent acoustic transfer in the free field to the left and right ears. The set of incident angles <em class="EmphasisTypeItalic ">s</em> contained in the HRTF dataset is typically dictated by the HRTF measurement setup [<span class="CitationRef"><a epub:type="biblioref" href="#CR5" role="doc-biblioref">5</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>]. The process of applying HRTFs to a virtual source signal <span id="ITerm22">to</span> encode localization cues is referred to as <em class="EmphasisTypeItalic ">binaural spatialization</em>.</p><p class="Para" id="Par30">Spatialization for loudspeaker arrays is also possible, commonly performed using channel-based methods such as Vector Base Amplitude Panning [<span class="CitationRef"><a epub:type="biblioref" href="#CR72" role="doc-biblioref">72</a></span>] or Ambisonics [<span class="CitationRef"><a epub:type="biblioref" href="#CR42" role="doc-biblioref">42</a></span>]. It is also possible to physically reproduce the virtual directional sound field using Wave Field Synthesis [<span class="CitationRef"><a epub:type="biblioref" href="#CR2" role="doc-biblioref">2</a></span>] with large loudspeaker <span id="ITerm23">arrays</span>. For the rest of this chapter, we will focus on binaural spatialization, although most of the discussion can be easily adapted to loudspeaker reproduction as discussed in Chap. <span class="ExternalRef"><a href="478239_1_En_5_Chapter.xhtml"><span class="RefSource">5</span></a></span>.</p><p class="Para" id="Par31"><strong class="EmphasisTypeBold ">Spherical-harmonics based rendering.</strong> Various methods exist to spatialize acoustic scenes. A convenient description of directional fields is via spherical <span id="ITerm24">harmonics</span> (SHs) or <span id="ITerm25">Ambisonics</span> [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>]. Given a SH representation of a scene, binaural ear input signals can be obtained directly via filtering with a SH representation of the listener’s HRTFs [<span class="CitationRef"><a epub:type="biblioref" href="#CR29" role="doc-biblioref">29</a></span>]. However, encoding complex acoustic scenes to SHs of sufficiently high order while minimizing audible artifacts can be challenging [<span class="CitationRef"><a epub:type="biblioref" href="#CR10" role="doc-biblioref">10</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR11" role="doc-biblioref">11</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR19" role="doc-biblioref">19</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>]. The openly available Resonance Audio [<span class="CitationRef"><a epub:type="biblioref" href="#CR47" role="doc-biblioref">47</a></span>] system follows this approach.</p><p class="Para" id="Par32"><strong class="EmphasisTypeBold ">Object-based rendering.</strong> <span id="ITerm26">In</span> this chapter, we will follow the direct parameterization over time and angle of arrival which is also common in practice, such as the illustrative auralization system we discuss in Sect. <span class="InternalRef"><a href="#Sec31">3.8.4</a></span>. The system directly outputs signals and directions, suitable for spatialization by applying appropriate HRTF pairs. The description of the acoustic propagation problem from a source to the listener in terms of a directional sound field as presented in Sect. <span class="InternalRef"><a href="#Sec11">3.3.4</a></span> results in a convenient interface between the propagation model and the spatialization engine.</p><p class="Para" id="Par33">This provides three major advantages. Firstly, it enables a modular system design that treats propagation modeling and (real-time) spatialization as separate problems that are solved by independent sub-systems. This separation in turn allows improving and optimizing the sub-systems individually and can lead to significant computational cost savings. Secondly, a description of a sound field enveloping the listener in terms of time and angle of arrival is equivalent to an object-based representation, which is a well-established input format for existing spatialization software, thus allowing the system designer to build easily on existing spatialization systems. Finally, psycho-acoustic research on perceptual limits of human spatial hearing, such as just-noticeable-differences, are expressed as a function of time and angle of arrival (Sect. <span class="InternalRef"><a href="#Sec13">3.4</a></span>). Knowledge of these perceptual limits can be exploited for further computational savings.</p></section></section><section class="Section1 RenderAsSection1" id="Sec7"><h2 class="Heading"><span class="HeadingNumber">3.3 </span>Mathematical Model</h2><p class="Para" id="Par34">Auralization may be formalized as a linear, time-invariant process as follows. Assume a quasi-static state of the world at the current visual frame. To auralize a sound source, consider its current <em class="EmphasisTypeItalic ">pose</em> (position and orientation) to determine its directional sound radiation and then model propagation and spatialization as a feed-forward chain of linear filters. Those filters in turn depend on the current world shape and listener pose, respectively.</p><p class="Para" id="Par35"><strong class="EmphasisTypeBold ">Notation.</strong> For the remainder of this chapter, for any quantity <span class="InlineEquation" id="IEq10"><img alt="$$(\star )$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq10.png" style="width:1.38em"/></span> referring to the listener, we use prime <span class="InlineEquation" id="IEq11"><img alt="$$(\star ')$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq11.png" style="width:1.62em"/></span> to denote a corresponding quantity referring to the source. In particular, <em class="EmphasisTypeItalic ">x</em> is listener location and <span class="InlineEquation" id="IEq12"><img alt="$$x'$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq12.png" style="width:1em"/></span> source location. Temporal convolution is denoted by <span class="InlineEquation" id="IEq13"><img alt="$$*$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq13.png" style="width:0.75em"/></span>.</p><section class="Section2 RenderAsSection2" id="Sec8"><h3 class="Heading"><span class="HeadingNumber">3.3.1 </span>The Green’s Function</h3><div class="Para" id="Par36">With the linearity and time-invariance assumptions, along with the absence of mean flow or wind, the Navier-Stokes equations simplify to the scalar wave <span id="ITerm27">equation</span> that models propagating longitudinal pressure deviations from quiescent atmospheric pressure  [<span class="CitationRef"><a epub:type="biblioref" href="#CR70" role="doc-biblioref">70</a></span>]:<div class="Equation NumberedEquation" id="Equ1"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\begin{aligned} \left[ ({1}\big /{c^2})\,\partial _t^2-\nabla _x^2\right] p\left( t,x,x'\right) =\delta \left( t\right) \delta \left( x-x'\right) , \end{aligned}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_Equ1.png" style="width:20.07em"/></div></div> <div class="EquationNumber">(3.1)</div></div></div>where <span class="InlineEquation" id="IEq14"><img alt="$$c=340$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq14.png" style="width:3.44em"/></span> m/s is the speed of sound, <span class="InlineEquation" id="IEq15"><img alt="$$\nabla _x^2$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq15.png" style="width:1.5em"/></span> the 3D Laplacian operator ranging over <em class="EmphasisTypeItalic ">x</em>. The solution is performed on some 3D domain provided by the scene’s shape, with appropriate boundary conditions to model the frequency-dependent absorptivity of physical materials.</div><div class="Para" id="Par37"><span id="ITerm28">Sound propagation</span> is induced by a pulsed excitation at time <span class="InlineEquation" id="IEq16"><img alt="$$t=0$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq16.png" style="width:2.37em"/></span> and source location <span class="InlineEquation" id="IEq17"><img alt="$$x'$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq17.png" style="width:1em"/></span> with <span class="InlineEquation" id="IEq18"><img alt="$$\delta (\cdot )$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq18.png" style="width:1.69em"/></span> denoting the Dirac delta function. The solution <span class="InlineEquation" id="IEq19"><img alt="$$p(t,x,x')$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq19.png" style="width:4.24em"/></span> is <span id="ITerm29">Green’s function</span> that fully describes the scene’s global wave transport, including diffraction and scattering. The principle of acoustic reciprocity ensures that source and listener positions are interchangeable [<span class="CitationRef"><a epub:type="biblioref" href="#CR70" role="doc-biblioref">70</a></span>]:<div class="Equation NumberedEquation" id="Equ2"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\begin{aligned} p(t,x,x') = p(t,x',x) . \end{aligned}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_Equ2.png" style="width:9.94em"/></div></div> <div class="EquationNumber">(3.2)</div></div></div>For treating general scenes, a numerical solver must be employed to discretely sample Green’s function in space and time. This includes accurate wave-based methods that directly solve for the time-evolving field on a grid, or fast geometric methods that employ the high-frequency Eikonal approximation. We will discuss solution methods in Sect. <span class="InternalRef"><a href="#Sec24">3.7</a></span>.</div><p class="Para" id="Par38">In principle, Green’s function has complete information [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>], including directionality, which can be extracted via spatio-temporal convolution of <span class="InlineEquation" id="IEq20"><img alt="$$p(t,x,x')$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq20.png" style="width:4.24em"/></span> with volumetric source and listener distributions that can model arbitrary radiation patterns [<span class="CitationRef"><a epub:type="biblioref" href="#CR13" role="doc-biblioref">13</a></span>] and listener directivity [<span class="CitationRef"><a epub:type="biblioref" href="#CR91" role="doc-biblioref">91</a></span>]. But such an approach is too expensive for real-time evaluation on large scenes, requiring temporal convolution and spatial quadrature over sub-wavelength grids that need to be repeated when either the source or listener moves. Geometric techniques cannot follow such an approach at all, as they do not model wave phase.</p><p class="Para" id="Par39">This is where modularity (Sect. <span class="InternalRef"><a href="#Sec4">3.2.2</a></span>) becomes indispensable: the source and listener are not directly included within the propagation simulation, but are instead incorporated via tabulated directivity functions that result from their local radiation and scattering characteristics. Below, we formulate the propagation component of this modular approach, beginning with the simplest case of an isotropic source and listener, building up to a fully bidirectional representation that can be combined with arbitrary source and listener directivity during rendering.</p></section><section class="Section2 RenderAsSection2" id="Sec9"><h3 class="Heading"><span class="HeadingNumber">3.3.2 </span>Impulse Response</h3><div class="Para" id="Par40">Consider an isotropic (omni-directional) sound source located at <span class="InlineEquation" id="IEq21"><img alt="$$x'$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq21.png" style="width:1em"/></span> that is emitting a coherent pressure signal <span class="InlineEquation" id="IEq22"><img alt="$$q'(t)$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq22.png" style="width:2.07em"/></span>. The resulting pressure signal at listener location <em class="EmphasisTypeItalic ">x</em> can be computed using a temporal convolution:<div class="Equation NumberedEquation" id="Equ3"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\begin{aligned} q(t; x, x^\prime ) = q^\prime (t)~*~p(t; x, x^\prime ). \end{aligned}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_Equ3.png" style="width:13.44em"/></div></div> <div class="EquationNumber">(3.3)</div></div></div>Here, <span class="InlineEquation" id="IEq23"><img alt="$$p(t; x, x^\prime )$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq23.png" style="width:4.19em"/></span> is obtained by evaluating Green’s function between the listener and source locations <span class="InlineEquation" id="IEq24"><img alt="$$(x, x^\prime )$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq24.png" style="width:2.74em"/></span>. We denote this evaluation by putting them after semi-colon <span class="InlineEquation" id="IEq25"><img alt="$$p(t; x, x^\prime )$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq25.png" style="width:4.19em"/></span> to signify they are held constant, yielding a function of time alone. This function is the (monoaural) impulse response capturing the various acoustic path delays and amplitudes from the source to the listener via the scene. The vibrational aspects of how the source event generated the sound <span class="InlineEquation" id="IEq26"><img alt="$$q'(t)$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq26.png" style="width:2.07em"/></span> are abstracted away—it may be synthesized at runtime, or read out from a pre-recorded file and freely substituted.</div></section><section class="Section2 RenderAsSection2" id="Sec10"><h3 class="Heading"><span class="HeadingNumber">3.3.3 </span>Directional Impulse Response</h3><p class="Para" id="Par41">The <em class="EmphasisTypeItalic ">directional</em> impulse response <span class="InlineEquation" id="IEq27"><img alt="$$d(t,s;x,x')$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq27.png" style="width:5.13em"/></span> [<span class="CitationRef"><a epub:type="biblioref" href="#CR32" role="doc-biblioref">32</a></span>]<span id="ITerm30"/> generalizes the impulse response <span class="InlineEquation" id="IEq28"><img alt="$$p(t;x,x')$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq28.png" style="width:4.19em"/></span> to include direction of <span id="ITerm31">arrival, <em class="EmphasisTypeItalic ">s</em></span>. Intuitively, it is the signal obtained by the listener if they were to point an ideal directional microphone in direction <em class="EmphasisTypeItalic ">s</em> when the source at <span class="InlineEquation" id="IEq29"><img alt="$$x'$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq29.png" style="width:1em"/></span> emits an isotropic impulse.</p><div class="Para" id="Par42">Given a directional impulse response, spatialization for the listener can be performed to reproduce the directional listening experience via<div class="Equation NumberedEquation" id="Equ4"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\begin{aligned} q^{\{l,r\}}(t; x,x') = q'(t)*\int _{\mathcal {S}^2} d\left( t,s;x,x'\right) \,*\,H^{\{l,r\}}\left( \mathcal { \mathcal {R} }^{-1}(s),t\right) \, ds~, \end{aligned}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_Equ4.png" style="width:28.01em"/></div></div> <div class="EquationNumber">(3.4)</div></div></div>where <span class="InlineEquation" id="IEq30"><img alt="$$H^{\{l,r\}}(s,t)$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq30.png" style="width:4.76em"/></span> are the left and right HRTFs of the <span id="ITerm32">listener</span> as discussed in Sect. <span class="InternalRef"><a href="#Sec6">3.2.4</a></span>, <span class="InlineEquation" id="IEq31"><img alt="$$\mathcal { \mathcal {R} }$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq31.png" style="width:1.18em"/></span> is a rotation matrix mapping from head to world coordinate system, and <span class="InlineEquation" id="IEq32"><img alt="$$s \in \mathcal {S}^2$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq32.png" style="width:3em"/></span> represents the space of incident spherical directions forming the integration domain. Note the advantage of separating propagation (directional impulse response) from spatialization (HRTF application). The expensive simulation necessary for solving (<span class="InternalRef"><a href="#Equ1">3.1</a></span>) can ignore the listener’s body entirely, which is inserted later taking its dynamic rotation <span class="InlineEquation" id="IEq33"><img alt="$$\mathcal { \mathcal {R} }$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq33.png" style="width:1.18em"/></span> into account, via separately tabulated HRTFs as in (<span class="InternalRef"><a href="#Equ4">3.4</a></span>).</div></section><section class="Section2 RenderAsSection2" id="Sec11"><h3 class="Heading"><span class="HeadingNumber">3.3.4 </span>Bidirectional Impulse Response (BIR) and Rendering Equation</h3><div class="Para" id="Par43">The above still leaves out direction-dependent radiation at the source. A complete description of auralization for localized sound sources can be achieved by the natural extension to the bidirectional impulse <span id="ITerm33">response</span> (BIR) [<span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>]; an 11-dimensional function of the wave field, <span class="InlineEquation" id="IEq34"><img alt="$$D(t,s,s';x,x')$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq34.png" style="width:6.69em"/></span>, illustrated in Fig. <span class="InternalRef"><a href="#Fig2">3.2</a></span>. Analogous to the HRTF, the source’s radiation pattern is tabulated in a source <span id="ITerm34">directivity</span> function (SDF), <span class="InlineEquation" id="IEq35"><img alt="$$S(s',t)$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq35.png" style="width:3.2em"/></span>, such that its radiated signal in any direction <span class="InlineEquation" id="IEq36"><img alt="$$s'$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq36.png" style="width:0.94em"/></span> is given by <span class="InlineEquation" id="IEq37"><img alt="$$q'(t)*S(t;s')$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq37.png" style="width:6.12em"/></span>.<figure class="Figure" id="Fig2"><div class="MediaObject" id="MO6"><img alt="" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Fig2_HTML.png" style="width:21.02em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.2</span><p class="SimplePara">Bidirectional impulse response (BIR). An impulse radiates from source position <span class="InlineEquation" id="IEq38"><img alt="$$x'$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq38.png" style="width:1em"/></span>, propagates through a scene, and arrives via two paths in this simple case at listener position <em class="EmphasisTypeItalic ">x</em>. The paths radiate in directions <span class="InlineEquation" id="IEq39"><img alt="$$s_1'$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq39.png" style="width:1em"/></span> and <span class="InlineEquation" id="IEq40"><img alt="$$s_2'$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq40.png" style="width:1em"/></span> and arrive from directions <span class="InlineEquation" id="IEq41"><img alt="$$s_1$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq41.png" style="width:1em"/></span> and <span class="InlineEquation" id="IEq42"><img alt="$$s_2$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq42.png" style="width:1em"/></span>, respectively, with delays based on the respective path lengths. The bidirectional impulse response (BIR) denoted by <span class="InlineEquation" id="IEq43"><img alt="$$D(t,s,s';x,x')$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq43.png" style="width:6.69em"/></span> contains this time-dependent directional information. Evaluating for specific radiant and incoming directions isolates arrivals, as shown on the right.</p><div class="Credit"><p class="SimplePara">(figure adapted from [<span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>])</p></div></div></figcaption></figure>
</div><div class="Para" id="Par44">We can now write the (binaural) rendering equation:<div class="Equation NumberedEquation" id="Equ5"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\begin{aligned} q^{\{l,r\}}(t;x,x')&amp;amp;= q'(t)~*\nonumber \\&amp;amp;\iint \! D\left( t,s,s';x,x^{\prime }\right) *S\left( \mathcal { \mathcal {R} }'^{-1}(s'),t\right) *H^{\{l,r\}}\left( \mathcal { \mathcal {R} }^{-1}(s),t\right) \,ds\,ds'\!, \end{aligned}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_Equ5.png" style="width:33.06em"/></div></div> <div class="EquationNumber">(3.5)</div></div></div>where <span class="InlineEquation" id="IEq44"><img alt="$$\mathcal { \mathcal {R} }$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq44.png" style="width:1.18em"/></span> is a rotation matrix mapping from the listener’s head to the world coordinate system, <span class="InlineEquation" id="IEq45"><img alt="$$\mathcal { \mathcal {R} }'$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq45.png" style="width:1.32em"/></span> maps rotation from the source to the world coordinate system, and the double integral varies over the space of both incident and emitted directions <span class="InlineEquation" id="IEq46"><img alt="$$s, s' \in \mathcal {S}^2$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq46.png" style="width:4.24em"/></span>. A similar formulation can be obtained for speaker-based rendering by using, for instance, VBAP speaker panning weights [<span class="CitationRef"><a epub:type="biblioref" href="#CR72" role="doc-biblioref">72</a></span>] instead of HRTFs.</div><p class="Para" id="Par45">The BIR is convolved with the source’s and listener’s free-field directional responses <span class="InlineEquation" id="IEq47"><img alt="$$S$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq47.png" style="width:0.94em"/></span> and <span class="InlineEquation" id="IEq48"><img alt="$$H^{\{l,r\}}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq48.png" style="width:2.74em"/></span>, respectively, while accounting for their rotation since <span class="InlineEquation" id="IEq49"><img alt="$$(s,s')$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq49.png" style="width:2.56em"/></span> are in world coordinates, to capture modification due to directional radiation and reception. The integral repeats this for all combinations of <span class="InlineEquation" id="IEq50"><img alt="$$(s,s')$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq50.png" style="width:2.56em"/></span>, yielding the net binaural response. This is finally convolved with the emitted signal <span class="InlineEquation" id="IEq51"><img alt="$$q'(t)$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq51.png" style="width:2.07em"/></span> to obtain a binaural output that should be delivered to the entrances of the listener’s ear canals. Finally, if multiple sound sources are present, this process is repeated for each source and the results are summed.</p><div class="Para" id="Par46"><strong class="EmphasisTypeBold ">Bidirectional decomposition and reciprocity.</strong> The bidirectional impulse response generalizes the more restrictive notions of impulse response in (<span class="InternalRef"><a href="#Equ4">3.4</a></span>) and (<span class="InternalRef"><a href="#Equ3">3.3</a></span>), illustrated in Fig. <span class="InternalRef"><a href="#Fig2">3.2</a></span>. The directional impulse response can be obtained by integrating over all radiating directions <span class="InlineEquation" id="IEq52"><img alt="$$s'$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq52.png" style="width:0.94em"/></span> and yields directional effects to the listener for an omnidirectional source:<div class="Equation NumberedEquation" id="Equ6"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\begin{aligned} d(t,s;x,x') \equiv \int _{\mathcal {S}^2} D(t,s,s';x,x')\, ds' . \end{aligned}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_Equ6.png" style="width:16.56em"/></div></div> <div class="EquationNumber">(3.6)</div></div></div>Similarly, a subsequent integration over directions to the listener, <em class="EmphasisTypeItalic ">s</em>, yields back the monoaural impulse response, <span class="InlineEquation" id="IEq53"><img alt="$$p(t;x,x')$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq53.png" style="width:4.19em"/></span>.</div><div class="Para" id="Par47">The BIR admits direct geometric interpretation. With source and listener located at <span class="InlineEquation" id="IEq54"><img alt="$$(x',x)$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq54.png" style="width:2.74em"/></span>, respectively, consider any pair of radiated and arrival directions <span class="InlineEquation" id="IEq55"><img alt="$$(s',s)$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq55.png" style="width:2.56em"/></span>. In general, multiple paths connect these pairs, <span class="InlineEquation" id="IEq56"><img alt="$$(x',s')\rightsquigarrow (x,s)$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq56.png" style="width:6.94em"/></span>, with corresponding delays and amplitudes, all of which are captured by <span class="InlineEquation" id="IEq57"><img alt="$$D(t,s,s';x,x')$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq57.png" style="width:6.69em"/></span>. Figure <span class="InternalRef"><a href="#Fig2">3.2</a></span> illustrates a simple case. The BIR is thus a fully reciprocal description of sound propagation within an arbitrary scene. Interchanging source and listener, all propagation paths reverse:<div class="Equation NumberedEquation" id="Equ7"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\begin{aligned} D(t,s,s';x,x') = D(t,s',s;x',x) . \end{aligned}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_Equ7.png" style="width:14.94em"/></div></div> <div class="EquationNumber">(3.7)</div></div></div>This reciprocal symmetry mirrors that for the underlying wave field, <span class="InlineEquation" id="IEq58"><img alt="$$p(t;x,x')=p(t;x',x)$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq58.png" style="width:9.57em"/></span> and requires a full bidirectional description. In particular, the directional impulse response is non-reciprocal.</div></section><section class="Section2 RenderAsSection2" id="Sec12"><h3 class="Heading"><span class="HeadingNumber">3.3.5 </span>Band-limitation and the Diffraction Limit</h3><p class="Para" id="Par48">It is important to remember that the bidirectional impulse response is a mathematically convenient intermediate representation only, and cannot be realized physically. The only physically observed quantity is the final rendered audio, <span class="InlineEquation" id="IEq59"><img alt="$$q^{\{l,r\}}(t;x,x')$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq59.png" style="width:5.74em"/></span>. In particular, the BIR representation allows unlimited resolution in time and direction. The source signal, <span class="InlineEquation" id="IEq60"><img alt="$$q'(t)$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq60.png" style="width:2.07em"/></span>, is temporally band-limited for typical sounds, due to aggressive absorption in solid media and air as frequency increases. Similarly, auditory perception is limited to 20 kHz. Band-limitation holds for directional resolution as well because of the <em class="EmphasisTypeItalic ">diffraction limit</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR16" role="doc-biblioref">16</a></span>] which places a fundamental restriction on the angular resolution achievable with a spatially finite radiator or receiver.</p><p class="Para" id="Par49">For a propagating wavelength <span class="InlineEquation" id="IEq61"><img alt="$$\lambda $$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq61.png" style="width:0.94em"/></span>, the diffraction-limited angular resolution scales as <span class="InlineEquation" id="IEq62"><img alt="$$\mathcal {D}/\lambda $$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq62.png" style="width:2.31em"/></span>, where <span class="InlineEquation" id="IEq63"><img alt="$$\mathcal {D}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq63.png" style="width:1.12em"/></span> is the diameter of an enclosing sphere, such as around a radiating object, or the listener’s head and shoulders in the case of HRTFs [<span class="CitationRef"><a epub:type="biblioref" href="#CR105" role="doc-biblioref">105</a></span>]. Therefore, all the convolutions and spherical quadratures in (<span class="InternalRef"><a href="#Equ5">3.5</a></span>) may be performed on a discretization with sufficient sub-wavelength resolution at the highest frequency of interest. Alternatively, it is common to perform time convolutions in frequency domain via the Fast Fourier Transform (FFT) for efficiency. Similarly, spherical harmonics (SH) form an orthonormal linear basis over the sphere and can be used to accelerate the spherical quadrature of function product to an inner product of spherical harmonic (SH) coefficients. An end-to-end auralization system using this approach was shown in [<span class="CitationRef"><a epub:type="biblioref" href="#CR63" role="doc-biblioref">63</a></span>].</p></section></section><section class="Section1 RenderAsSection1" id="Sec13"><h2 class="Heading"><span class="HeadingNumber">3.4 </span>Structure and Perception of the Bidirectional Impulse Response (BIR)</h2><div class="Para" id="Par50">To explain how the theory outlined above can be put into practice, we will first review the physical and perceptual structure of the BIR, followed by a discussion of how auralization systems approximate in various ways.<figure class="Figure" id="Fig3"><div class="MediaObject" id="MO10"><img alt="" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Fig3_HTML.png" style="width:33.95em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.3</span><p class="SimplePara">Structure of the bidirectional impulse response</p><div class="Credit"><p class="SimplePara">(figure adapted from [<span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>])</p></div></div></figcaption></figure>
</div><section class="Section2 RenderAsSection2" id="Sec14"><h3 class="Heading"><span class="HeadingNumber">3.4.1 </span>Physical Structure</h3><p class="Para" id="Par51">The structure of a typical (bidirectional) impulse response may be understood in three phases in time, as illustrated in Fig. <span class="InternalRef"><a href="#Fig3">3.3</a></span>. First, the emitted sound must propagate via the shortest path, potentially diffracting around obstruction edges to reach the listener after some onset delay. This is the initial (or “direct”) sound. The initial sound is followed by early reflections due to scattering and reflection from scene geometry. As sound continues to scatter multiple times from the scene, the temporal arrival density of reflections increases, while the energy of an individual arrival decreases due to absorption at material boundaries and in the air. Over time, with sufficient scattering, the response approaches decaying Gaussian noise, which is referred to as late reverberation. The transition from early reflections to late reverberation is demarcated by the <em class="EmphasisTypeItalic ">mixing time</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR1" role="doc-biblioref">1</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR98" role="doc-biblioref">98</a></span>].</p><p class="Para" id="Par52">As we discuss next, each of these phases has a distinct contribution to the overall spatial perception of a sound. These properties of the human auditory perception play a key role in informing how one might approximate the rendering equation (<span class="InternalRef"><a href="#Equ5">3.5</a></span>) within limited computational resources, while still retaining an immersive auditory experience. A more detailed review of perception of room acoustics can be found in [<span class="CitationRef"><a epub:type="biblioref" href="#CR37" role="doc-biblioref">37</a></span>] and [<span class="CitationRef"><a epub:type="biblioref" href="#CR60" role="doc-biblioref">60</a></span>]. All observations and terms below can be found in these references, unless otherwise noted.</p></section><section class="Section2 RenderAsSection2" id="Sec15"><h3 class="Heading"><span class="HeadingNumber">3.4.2 </span>Initial (“Direct”) Sound</h3><p class="Para" id="Par53">Our perception strongly relies on the initial <span id="ITerm35">sound</span> to localize sound sources, a phenomenon called the <em class="EmphasisTypeItalic ">precedence <span id="ITerm36">effect</span>
</em> [<span class="CitationRef"><a epub:type="biblioref" href="#CR62" role="doc-biblioref">62</a></span>]. Referring to Fig. <span class="InternalRef"><a href="#Fig3">3.3</a></span>, if there is a secondary arrival that is roughly within 1 ms of the initial sound, we perceive a direction intermediate between the two arrival directions, termed <em class="EmphasisTypeItalic ">summing localization</em>, representing the temporal resolution of spatial hearing. Beyond this 1 ms time window, our perceptual system exerts a strongly non-linear suppression effect, so people do not confuse the direction of strong reflections with the true heading of the sound. Sometimes called the <em class="EmphasisTypeItalic ">Haas effect</em>, a later arrival may need to be as much as 10 dB louder than the initial sound to affect the perceived direction significantly. Note that this is not to say that the later arrival is not perceived at all, only that its effect is not to substantially change the localized direction.</p><p class="Para" id="Par54">Consider the case shown in Fig. <span class="InternalRef"><a href="#Fig3">3.3</a></span>, and assume the walls do not substantially transmit sound. The sound shown inside the room would be localized by the listener outside as arriving from the direction of the doorway, rather than the line of sight. Such cues are a natural part of how we navigate to visually occluded events in everyday life. The upshot is that in virtual reality, the initial sound path may be multiply-diffracted and must be modeled with particular care so that the user gets localization cues consistent with the virtual world.</p></section><section class="Section2 RenderAsSection2" id="Sec16"><h3 class="Heading"><span class="HeadingNumber">3.4.3 </span>Early Reflections</h3><p class="Para" id="Par55"><span id="ITerm37">Early reflections</span> directly affect the perception of source properties such as loudness, width, and distance while also informing the listener about surrounding scene geometry such as nearby reflectors. A copy of a sound following the initial arrival is perceptually fused up until a delay called the <em class="EmphasisTypeItalic ">echo <span id="ITerm38">threshold</span>
</em>, beyond which it is perceived as a separate auditory event. The echo threshold varies between 10 ms for impulsive sounds, through 50 ms for speech to 80 ms for orchestral music [<span class="CitationRef"><a epub:type="biblioref" href="#CR62" role="doc-biblioref">62</a></span>, Table 1].</p><p class="Para" id="Par56">The impact of the loudness of early reflections is important in two ways. Firstly, the perception of source distance is known to correlate with the energy ratio between initial sound and remaining response (whose energy mostly comes from early reflections), called the direct-to-reverberant <span id="ITerm39">ratio</span> (DRR) [<span class="CitationRef"><a epub:type="biblioref" href="#CR92" role="doc-biblioref">92</a></span>]. This is often also called the “wet ratio” by audio designers. Secondly, how well one can understand and localize sounds depends on the ratio of the energy of direct sound and early reflection in the first 50 ms to the rest of the response, as measured by <em class="EmphasisTypeItalic "><span id="ITerm40">clarity</span>
</em> (<span class="InlineEquation" id="IEq64"><img alt="$$C_{50}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq64.png" style="width:1.56em"/></span>).</p><p class="Para" id="Par57">The directional distribution of reflections conveys important detail about the size and shape of the local environment around the listener and source. The ratio of reflected energy arriving horizontally and perpendicular to the initial sound is called <em class="EmphasisTypeItalic ">lateral energy fraction</em> and contributes to the perception of spaciousness and affects the <em class="EmphasisTypeItalic ">apparent source width</em>. Further, in VR, strong individual reflections from surfaces close to the listener provide an important <span id="ITerm41">proximity</span> cue [<span class="CitationRef"><a epub:type="biblioref" href="#CR69" role="doc-biblioref">69</a></span>].</p><p class="Para" id="Par58">Thus, an auralization system must model strong initial reflections as well as the aggregate energy and directionality of later reflections up to the first 80 ms to ensure important cues about the sound source and environment are conveyed.</p></section><section class="Section2 RenderAsSection2" id="Sec17"><h3 class="Heading"><span class="HeadingNumber">3.4.4 </span>Late Reverberation</h3><p class="Para" id="Par59">The reverberation time, <span class="InlineEquation" id="IEq65"><img alt="$$T_{60}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq65.png" style="width:1.44em"/></span>, is the time taken by the reverberant energy to decay by 60 dB. Since the <span id="ITerm42">reverberation</span> contains numerous, lengthy paths through the scene, it provides a sense of the overall scene, such as its size. The <span class="InlineEquation" id="IEq66"><img alt="$$T_{60}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq66.png" style="width:1.44em"/></span> is frequency-dependent; the relative decay rate across various frequencies informs the listener about the acoustic materials in a scene and atmospheric absorption.</p><p class="Para" id="Par60">The aggregate directional properties of reverberation affect <em class="EmphasisTypeItalic ">listener <span id="ITerm43">envelopment</span>
</em> which is the perception of being present in a room and immersed in its reverberant field (see Chap. <span class="ExternalRef"><a href="478239_1_En_11_Chapter.xhtml"><span class="RefSource">11</span></a></span> and Sect. <span class="ExternalRef"><a href="478239_1_En_11_Chapter.xhtml"><span class="RefSource">11.​4.​3</span></a></span> for further discussions on related topics). In virtual reality, one may often be present <em class="EmphasisTypeItalic ">outside</em> a room containing sounds and any implausible envelopment becomes especially distracting. For instance, consider the situation in Fig. <span class="InternalRef"><a href="#Fig3">3.3</a></span>—rendering an enveloping room reverberation for the listener will sound wrong, since the expectation would be low envelopment.</p></section></section><section class="Section1 RenderAsSection1" id="Sec18"><h2 class="Heading"><span class="HeadingNumber">3.5 </span>System Design Considerations for VR Auralization</h2><p class="Para" id="Par61">Many types of real-time auralization systems exist today that approximate the rendering equation (<span class="InternalRef"><a href="#Equ5">3.5</a></span>), and in particular, how to evaluate the scene’s sound propagation (i.e., the BIR, <span class="InlineEquation" id="IEq67"><img alt="$$D(t,s,s';x,x')$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq67.png" style="width:6.69em"/></span>) which is typically the most compute-intensive portion. They gain efficiency by making approximations based on the intended application, with a knowledge of the limits of auditory perception.</p><section class="Section2 RenderAsSection2" id="Sec19"><h3 class="Heading"><span class="HeadingNumber">3.5.1 </span>Room Auralization</h3><p class="Para" id="Par62">The roots of auralization research lie in the area of computational modeling of room acoustics, an active area of research with developments dating back at least 50 years [<span class="CitationRef"><a epub:type="biblioref" href="#CR7" role="doc-biblioref">7</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR60" role="doc-biblioref">60</a></span>]. The main objective of these computer models has been to aid in the architectural design of enclosures, such as offices, classrooms, and concert halls. The predictions of these models can then be used by acousticians to propose architectural design changes or acoustic treatments to improve the reverberant properties of a particular room or hall, such as speech intelligibility in a classroom. This requires models that simulate the room’s first reflections and reverberation with perceptual authenticity. The direct path in such applications can often be computed analytically since the line of sight is rarely blocked. We direct the reader to Gade’s book chapter [<span class="CitationRef"><a epub:type="biblioref" href="#CR37" role="doc-biblioref">37</a></span>] on the subject of room acoustics for an excellent summary of the requirements, metrics, and methods in the field from the viewpoint of concert hall design.</p><p class="Para" id="Par63">While initially the computer models could only produce quantitative estimates of room acoustic parameters, with increasing compute power, real-time auralization systems were proposed near the beginning of the millennium [<span class="CitationRef"><a epub:type="biblioref" href="#CR86" role="doc-biblioref">86</a></span>]. As we will discuss in more detail shortly, geometric methods are standard in the area today because they are especially well-suited for modeling a single enclosure where visual occlusion between sounds and listener is not dominant. This holds very well in any hall designed for speech or music. Room auralization is available today in commercial packages such as ODEON [<span class="CitationRef"><a epub:type="biblioref" href="#CR82" role="doc-biblioref">82</a></span>] and CATT [<span class="CitationRef"><a epub:type="biblioref" href="#CR28" role="doc-biblioref">28</a></span>].</p></section><section class="Section2 RenderAsSection2" id="Sec20"><h3 class="Heading"><span class="HeadingNumber">3.5.2 </span>VR Auralization</h3><p class="Para" id="Par64">The concerns of real-time VR auralization are quite distinct along a number of dimensions, which result from going from individual room to a scene that can span entire city blocks with numerous indoor and outdoor areas. This results in a unique set of considerations that we enumerate below, for two reasons. Firstly, they provide a framing for understanding current research in the area and the trade-offs current systems make, which we will discuss in the following sections. Secondly, we hope that the concise listing of practical problems motivates new research in the area, as no system today can meet all these criteria.</p><div class="Para" id="Par65"><div class="OrderedList"><ol><li class="ListItem"><div class="ItemNumber">1.</div><div class="ItemContent"><p class="Para" id="Par66"><strong class="EmphasisTypeBold ">Real time within limited computation. </strong> A VR application’s auralization component can usually only use a single or a few CPU cores for audio simulation at runtime, since resources must be shared with simulating other aspects of the world, such as rigid-body collisions, character animation, and AI path planning. In contrast, owing to the application, in room acoustic auralization one can consume a majority of the resources of a computer including the parallel compute power of modern graphics cards. With power-efficient mobile processors integrated into phones and standalone head-mounted displays, the pressure to minimize computation has only increased.</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">2.</div><div class="ItemContent"><p class="Para" id="Par67"><strong class="EmphasisTypeBold ">Scene complexity and non line of sight. </strong> Room acoustics theory often starts by assuming a single connected space such as a concert hall that has lines of sight from the stage to all listener locations. This allows for a powerful simplification of the sound field as an analytically computable direct sound combined with a diffuse reverberant field. Modern VR systems for building and game acoustics consider the much broader class of all scenes such as a building floor with many rooms, or a street canyon with buildings that may be entered. These are complex <span id="ITerm44">scenes</span> not just in the sense of surface detail but also in that the air volume is topologically complex, with many concavities. As a result, non line of sight cases are common. For instance, hearing sounds in the same room with plausible reverberation can be as important as <em><strong class="EmphasisTypeBoldItalic ">not</strong></em> hearing sounds inside another room, or hearing sounds from unseen sources diffracted around a corner or door.</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">3.</div><div class="ItemContent"><p class="Para" id="Par68"><strong class="EmphasisTypeBold ">Perception. </strong> Physical accuracy is important to VR auralization not as a goal in itself but rather in so far as it impacts sensory immersion. This opens opportunities for fast approximations, and deeply informs practical systems that scale their errors based on the acuity of the human auditory system. This observation underlies the deterministic-statistical decomposition discussed in the next section. Further, in many applications such as games, plausibility can be sufficient as a starting point, while for instance in auralizing building acoustics one might need perceptual authenticity.</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">4.</div><div class="ItemContent"><p class="Para" id="Par69"><strong class="EmphasisTypeBold ">Dynamic sounds. </strong> VR auralization must often support dynamic sound <span id="ITerm45">sources</span> that can translate and rotate. The rendering must respond with low latency and without distracting artifacts, even for fast source motion. This adds significant complexity to a minimum-viable practical system. However, in architectural acoustic systems, static sound sources can be a feasible starting point.</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">5.</div><div class="ItemContent"><p class="Para" id="Par70"><strong class="EmphasisTypeBold ">Dynamic geometry. </strong><span id="ITerm46"/> In many applications, the scene geometry can be changed interactively. This may be while designing a virtual space, in which case an acoustical system for static scenes may re-compute on the updated geometry; depending on the system this can take seconds to hours. The more challenging case is when the geometry is changing in real time. The change might be “locally dynamic”, such as opening a door or moving an obstruction. Since such changes are localized in an otherwise static scene, many systems are able to model such effects. Lastly, the scene may be “globally dynamic”, where there might be unpredictable global changes, such as when a game player creates a building in Minecraft or Fortnite and expects to hear the audio rendering adapt to it in real time—while this has the most practical utility it is also the most challenging case.</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">6.</div><div class="ItemContent"><p class="Para" id="Par71"><strong class="EmphasisTypeBold ">Robustness. </strong> VR requires high robustness given unpredictable user inputs. This means the severity and frequency of large outlying errors may matter more than average error. For instance, as the listener moves quickly through a scene through multiple rooms, the variation in reverberation and diffracted occlusion must stay smooth reliably. This is a tightly restrictive constraint: a technique that has large outlying errors may not be viable in immersive VR regardless of its average error. As an example, an implausible error in calculating occlusion with only 0.1% probability for an experience running at 30 frames per second means distracting the user every 33 s on average. This deteriorates to 3.3 s with 10 sound sources and so on.</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">7.</div><div class="ItemContent"><p class="Para" id="Par72"><strong class="EmphasisTypeBold ">Scalability. </strong> The system should ideally expose compute-quality trade-offs along two axes. Firstly, VR scenes can contain hundreds to thousands of dynamic sound sources, and it is desirable if the signal processing can scale from high-quality rendering of a few sound sources to lower quality (but still plausible) rendering for numerous sound sources. Secondly, the acoustical simulation should also allow methods for reducing quality gracefully as scene size increases. For instance, high-quality propagation modeling of a conference room, up to a rough simulating of a city.</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">8.</div><div class="ItemContent"><p class="Para" id="Par73"><strong class="EmphasisTypeBold ">Automation. </strong> For VR applications, it is preferable to avoid any per-scene manual work, such as geometric scene simplification. Game scenes in particular can span over kilometers with multiple buildings designed iteratively during the production process. This makes manual simplification a major hurdle for practical usage. The auralization system must ideally directly ingest complex scenes with millions of polygons, and perform any necessary simplification while minimizing any human expertise or input, unlike in room auralization.</p></div><div class="ClearBoth"> </div></li><li class="ListItem"><div class="ItemNumber">9.</div><div class="ItemContent"><p class="Para" id="Par74"><strong class="EmphasisTypeBold ">Artistic direction. </strong> VR often requires the final rendering to be controlled by a sound designer. For instance, the reverberation and diffracted occlusion on important dialogue might be reduced to boost speech intelligibility in a game. Or one might want to re-map the dynamic range of the audio rendering with the limits of the audio reproduction system or user comfort in mind. A viable system must provide methods that allow such design intent to be expressed and influence the auralization process appropriately.</p></div><div class="ClearBoth"> </div></li></ol></div>
</div></section></section><section class="Section1 RenderAsSection1" id="Sec21"><h2 class="Heading"><span class="HeadingNumber">3.6 </span>Rendering the BIR: the Deterministic-Statistical Decomposition</h2><p class="Para" id="Par75">A powerful technique employed by most real-time auralization systems is to decompose the BIR as a sum of a deterministic and statistical component. This is deeply informed by acoustical perception (Sect. <span class="InternalRef"><a href="#Sec13">3.4</a></span>) and is key to enabling the computational trade-offs VR auralization must contend with, as described in the prior section. The initial sound and strong early reflections, such as sound heard via a portal or echoes heard from nearby large surfaces, are treated deterministically: that is, simulated and rendered in physical detail, and updated in real time based on the dynamic source and listener pose and scene geometry. Weak early reflections and late reverberation are represented only statistically, ignoring the precise details of each of the amplitudes and delays of thousands of arrivals or more, which are perceived in aggregate.</p><div class="Para" id="Par76">To formalize, the BIR is decomposed as<div class="Equation NumberedEquation" id="Equ8"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\begin{aligned} D(t,s,s';x,x') = D_d(t,s,s';x,x') + D_s(t,s,s';x,x'). \end{aligned}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_Equ8.png" style="width:23.51em"/></div></div> <div class="EquationNumber">(3.8)</div></div></div>Referring to Fig. <span class="InternalRef"><a href="#Fig3">3.3</a></span>, the initial sound and early reflection spikes deemed perceptually salient can be included accurately in <span class="InlineEquation" id="IEq68"><img alt="$$D_d$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq68.png" style="width:1.5em"/></span>. The residual is <span class="InlineEquation" id="IEq69"><img alt="$$D_s$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq69.png" style="width:1.44em"/></span>, which is usually modeled as noise characterized by its perceptually relevant statistical properties.</div><div class="Para" id="Par77">Substituting into the rendering equation (<span class="InternalRef"><a href="#Equ5">3.5</a></span>) and observing linearity, we have<div class="Equation NumberedEquation" id="Equ9"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\begin{aligned} q^{\{l,r\}}(t;x,x') = \sum _{\{d,s\}} q'(t)~*\iint \! D_{\{d,s\}} *S\left( \mathcal { \mathcal {R} }'^{-1}(s'),t\right) *H^{\{l,r\}}\left( \mathcal { \mathcal {R} }^{-1}(s),t\right) \,ds\,ds'\!, \end{aligned}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_Equ9.png" style="width:35.19em"/></div></div> <div class="EquationNumber">(3.9)</div></div></div>so that the input mono signal, <span class="InlineEquation" id="IEq70"><img alt="$$q'(t)$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq70.png" style="width:2.07em"/></span>, is split off as input into separate filtering processes for the two components, whose binaural outputs are summed. This is a fairly standard architecture followed by both research and commercial systems, as the two components may be approximated independently with perception and the particular application in mind. For the remainder of this section, we will assume the BIR components have been computed and focus on the signal processing for rendering. The next section will discuss how this decomposition informs the design of acoustic simulation methods.</div><section class="Section2 RenderAsSection2" id="Sec22"><h3 class="Heading"><span class="HeadingNumber">3.6.1 </span>Deterministic Component, <span class="InlineEquation" id="IEq71"><img alt="$$D_d$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq71.png" style="width:1.5em"/></span></h3><div class="Para" id="Par78">The deterministic component, <span class="InlineEquation" id="IEq72"><img alt="$$D_d$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq72.png" style="width:1.5em"/></span>, is typically represented as a set of <span class="InlineEquation" id="IEq73"><img alt="$$n_d$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq73.png" style="width:1.24em"/></span> peaks:<div class="Equation NumberedEquation" id="Equ10"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\begin{aligned} D_d(t,s,s';x,x') \approx \sum _{i=0}^{n_d-1} a_i(t)*\delta (t-\tau _i)~\delta (s'-s'_i)~\delta (s-s_i). \end{aligned}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_Equ10.png" style="width:26.07em"/></div></div> <div class="EquationNumber">(3.10)</div></div></div>Each term represents an echo of the emitted impulse that arrives at the listener position after a delay of <span class="InlineEquation" id="IEq74"><img alt="$$\tau _i$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq74.png" style="width:0.94em"/></span> from world direction <span class="InlineEquation" id="IEq75"><img alt="$$s_i$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq75.png" style="width:1em"/></span>, having been previously radiated from the source in world direction <span class="InlineEquation" id="IEq76"><img alt="$$s'_i$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq76.png" style="width:1em"/></span> at time <span class="InlineEquation" id="IEq77"><img alt="$$t=0$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq77.png" style="width:2.37em"/></span>. The amplitude filter <span class="InlineEquation" id="IEq78"><img alt="$$a_i(t)$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq78.png" style="width:2.07em"/></span> captures transport effects along the path from edge diffraction, scattering, and frequency-dependent transmission/reflection from scene geometry. Note that the amplitude filter is causal, i.e., <span class="InlineEquation" id="IEq79"><img alt="$$a_i(t) = 0$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq79.png" style="width:3.95em"/></span> for <span class="InlineEquation" id="IEq80"><img alt="$$t&amp;lt;0$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq80.png" style="width:2.37em"/></span>, and by convention <span class="InlineEquation" id="IEq81"><img alt="$$\tau _{i+1}&amp;gt;\tau _i$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq81.png" style="width:4em"/></span>. The parameter <span class="InlineEquation" id="IEq82"><img alt="$$n_d$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq82.png" style="width:1.24em"/></span> is key for trading between rendering quality and computational resources. It is usual to at least treat the initial sound path deterministically (i.e., <span class="InlineEquation" id="IEq83"><img alt="$$n_d\ge 1$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq83.png" style="width:3.06em"/></span>) because of its high importance for localization due to the Precedence Effect. Audio engines will usually designate this (<span class="InlineEquation" id="IEq84"><img alt="$$i=0$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq84.png" style="width:2.31em"/></span>) as the “dry” path with separate design controls due to its perceptual importance.</div><div class="Para" id="Par79">Substituting from (<span class="InternalRef"><a href="#Equ10">3.10</a></span>) into Eq. (<span class="InternalRef"><a href="#Equ9">3.9</a></span>), we get<div class="Equation NumberedEquation" id="Equ11"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\begin{aligned} q^{\{l,r\}}_d(t) = \sum _{i=0}^{n_d-1} q'(t)~*~\delta (t-\tau _i) *a_i(t) *S\left( \mathcal { \mathcal {R} }'^{-1}(s'_i),t\right) \! *H^{\{l,r\}}\left( \mathcal { \mathcal {R} }^{-1}(s_i),t\right) . \end{aligned}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_Equ11.png" style="width:33.63em"/></div></div> <div class="EquationNumber">(3.11)</div></div></div>Thus, each path’s processing is a linear filter chain whose binaural output is summed to render the deterministic component to the listener. Reading the equation from left to right: for each path, take the monophonic source signal and input it to a delay line. Read the delay line at (fractional) delay <span class="InlineEquation" id="IEq85"><img alt="$$\tau _i$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq85.png" style="width:0.94em"/></span> and filter the output based on amplitude filter <span class="InlineEquation" id="IEq86"><img alt="$$a_i$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq86.png" style="width:1.06em"/></span>, then filter it based on the source’s radiation pattern. The lookup via <span class="InlineEquation" id="IEq87"><img alt="$$\mathcal { \mathcal {R} }'^{-1}(s'_i)$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq87.png" style="width:3.75em"/></span> signifies that one must rotate the radiant direction of the path from world space to the local coordinate system of the source’s spherical radiation pattern data.</div><p class="Para" id="Par80">Finally, the last factor makes concrete the modularity shown in Fig. <span class="InternalRef"><a href="#Fig1">3.1</a></span>: the resulting monophonic signal from this prior processing is sent to the spatializer module as arriving from direction <span class="InlineEquation" id="IEq88"><img alt="$$\mathcal { \mathcal {R} }^{-1}(s_i)$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq88.png" style="width:3.57em"/></span> relative to the listener. One is free to substitute any spatializer to separately trade off quality and speed of spatialization versus other costs and priorities for the system. One could even use multiple spatialization techniques, such as high-quality spatialization for the initial path, and lower fidelity for reflections. In a software implementation, the spatializer often acts as a sink for monophonic signals, processing each, mixing their outputs, and sending them to a low-level audio engine for transmission to transducers, thus performing the summation in (<span class="InternalRef"><a href="#Equ11">3.11</a></span>) as well.</p><p class="Para" id="Par81">Similar to the choice of spatializer, the details of all other filtering operations are highly flexible. For the amplitude filter <span class="InlineEquation" id="IEq89"><img alt="$$a_i$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq89.png" style="width:1.06em"/></span>, the simplest realization is to multiply by a scalar for average magnitude over frequencies, thus representing arrivals with idealized Dirac spikes. But for the initial sound filter <span class="InlineEquation" id="IEq90"><img alt="$$a_0$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq90.png" style="width:1em"/></span>, even in a minimalistic setting it is common to apply a low-pass filter to capture the audible muffling of visually occluded sounds. A more accurate implementation accounting for frequency-dependent boundary impedance could use equalization filters in octave bands. For source directivity, it is common to measure and store radiation patterns as third-octave or octave-band data tabulated over the sphere of directions while ignoring phase. Convolution can then be realized via modern fast graphic equalizer algorithms that employ recursive time-domain filters [<span class="CitationRef"><a epub:type="biblioref" href="#CR68" role="doc-biblioref">68</a></span>].</p><p class="Para" id="Par82">The commutative and associative properties of convolution are a powerful tool to optimize signal processing. The ordering of filters in (<span class="InternalRef"><a href="#Equ11">3.11</a></span>) has been chosen to illustrate this. The delay is applied in the very first operation. This makes it so that we only need one single-write-multiple-read delay line shared across all paths. The signal <span class="InlineEquation" id="IEq91"><img alt="$$q'(t)$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq91.png" style="width:2.07em"/></span> is written as input, and each path reads out at delay <span class="InlineEquation" id="IEq92"><img alt="$$\tau _i$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq92.png" style="width:0.94em"/></span>. This is a commonly used optimization. Further, one may then use the associative property to group the factors: <span class="InlineEquation" id="IEq93"><img alt="$$a_i(t)*S\left( \mathcal { \mathcal {R} }'^{-1}(s'_i),t\right) $$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq93.png" style="width:9.44em"/></span>. If both are implemented, say, using an octave-band graphic equalizer, then the per-band amplitudes can be multiplied first and provided to a single instance of the equalizer—a nearly two-fold reduction in equalization compute. These optimizations illustrate the importance of linearity and modularity in the efficient implementation of auralization systems.</p></section><section class="Section2 RenderAsSection2" id="Sec23"><h3 class="Heading"><span class="HeadingNumber">3.6.2 </span>Statistical Component, <span class="InlineEquation" id="IEq94"><img alt="$$D_s$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq94.png" style="width:1.44em"/></span></h3><p class="Para" id="Par83">The central concept for rendering the statistical component, <span class="InlineEquation" id="IEq95"><img alt="$$D_s$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq95.png" style="width:1.44em"/></span>, is to use an analysis-synthesis approach [<span class="CitationRef"><a epub:type="biblioref" href="#CR56" role="doc-biblioref">56</a></span>]. The analysis phase does lossy perceptual coding of the statistical component of the BIR, <span class="InlineEquation" id="IEq96"><img alt="$$D_s$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq96.png" style="width:1.44em"/></span>, to compute <span class="InlineEquation" id="IEq97"><img alt="$$\bar{D}_s$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq97.png" style="width:1.44em"/></span> as the energy envelope of the response summing over time, frequency, and direction. We use the over-bar notation <span class="InlineEquation" id="IEq98"><img alt="$$\bar{f}(\bar{y})$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq98.png" style="width:2.13em"/></span> to indicate that <em class="EmphasisTypeItalic ">y</em> is sub-sampled, and <em class="EmphasisTypeItalic ">f</em>’s corresponding energy is appropriately summed at each sample of <span class="InlineEquation" id="IEq99"><img alt="$$\bar{y}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq99.png" style="width:0.81em"/></span> without loss via some windowing. For instance, if <em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">t</em>) is an impulse response, <span class="InlineEquation" id="IEq100"><img alt="$$\bar{p}(\bar{t})$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq100.png" style="width:1.82em"/></span> indicates the corresponding <em class="EmphasisTypeItalic ">echogram</em>, which is the histogram of <span class="InlineEquation" id="IEq101"><img alt="$$p^2(t)$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq101.png" style="width:2.25em"/></span> sampled at some time-bin centers, <span class="InlineEquation" id="IEq102"><img alt="$$\bar{t}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq102.png" style="width:0.63em"/></span>. This notation is introduced to indicate the reduction in the sampling rate of <em class="EmphasisTypeItalic ">y</em>, and loss of fine structure information in <em class="EmphasisTypeItalic ">f</em> at its original sampling rate, such as phase.</p><p class="Para" id="Par84"><strong class="EmphasisTypeBold ">Parametric reverberation.</strong> During real-time rendering, the description captured in <span class="InlineEquation" id="IEq103"><img alt="$$\bar{D}_s$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq103.png" style="width:1.44em"/></span> can be synthesized using fast <em class="EmphasisTypeItalic ">parametric <span id="ITerm47">reverberation</span>
</em> techniques: the “parameters” being statistical properties that determine <span class="InlineEquation" id="IEq104"><img alt="$$\bar{D}_s$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq104.png" style="width:1.44em"/></span>, as we will discuss. The key advantage is that since the fine structure of the response in time, frequency, and direction is left unspecified, one has vast freedom in choosing efficient techniques. These techniques often rely on recursive time-domain filtering which can potentially make the CPU cost far smaller than applying a few seconds long filter via frequency-domain convolution. The research problem is to make the artificial reverberation sound natural. Among other concerns, the produced reverberation must have realistically high temporal echo density and sound colorless, not introducing perceivable spectral or temporal modulations that cannot be controlled. For further reading, we point readers to the extensive survey in [<span class="CitationRef"><a epub:type="biblioref" href="#CR99" role="doc-biblioref">99</a></span>]. In the following, we focus on how one might characterize <span class="InlineEquation" id="IEq105"><img alt="$$\bar{D}_s$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq105.png" style="width:1.44em"/></span>.</p><p class="Para" id="Par85"><strong class="EmphasisTypeBold ">Energy Decay Relief (EDR).</strong><span id="ITerm48"/> The EDR [<span class="CitationRef"><a epub:type="biblioref" href="#CR56" role="doc-biblioref">56</a></span>] is a central concept for statistical encoding of acoustical responses. Consider a monoaural impulse response, <em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">t</em>). The EDR, <span class="InlineEquation" id="IEq106"><img alt="$$\bar{p}(\bar{t},\bar{\omega })$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq106.png" style="width:3em"/></span>, is computed by performing short-time Fourier analysis on <em class="EmphasisTypeItalic ">p</em>(<em class="EmphasisTypeItalic ">t</em>) to compute how its energy spectral density integrated over perceptual frequency bands with centers <span class="InlineEquation" id="IEq107"><img alt="$$\bar{\omega }$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq107.png" style="width:0.87em"/></span> varies over time-bin centers <span class="InlineEquation" id="IEq108"><img alt="$$\bar{t}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq108.png" style="width:0.63em"/></span>. It can be visualized as a spectrogram. Frequency dependence results from materials of the boundary (e.g., wood tends to be more absorbent at high frequencies compared to concrete) and atmospheric absorption. Frequency band centers are typically spaced by octaves for real-time auralization, and time bins typically have a width of around 10 ms.</p><div class="Para" id="Par86">The reduced sampling rate makes the EDR, <span class="InlineEquation" id="IEq109"><img alt="$$\bar{p}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq109.png" style="width:0.81em"/></span>, already quite compact compared to <em class="EmphasisTypeItalic ">p</em>, which is a highly oscillatory noisy signal at audio sample rates. Further, the EDR is smooth in time: it exhibits slow variation during early reflections (especially if the strong peaks have been separated out already into <span class="InlineEquation" id="IEq110"><img alt="$$D_d$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq110.png" style="width:1.5em"/></span>) followed by monotonic decay during late reverberation. This opens up many avenues for a low-dimensional description with a few parameters. For instance, for a single enclosure, the EDR in each frequency band may be well-approximated by an exponential decay, resulting in a compact description for the late reverberation parameterized by the initial energy, <span class="InlineEquation" id="IEq111"><img alt="$$\bar{p}_0$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq111.png" style="width:1.06em"/></span>, and 60-dB decay time, <span class="InlineEquation" id="IEq112"><img alt="$$T_{60}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq112.png" style="width:1.44em"/></span> in each frequency band:<div class="Equation NumberedEquation" id="Equ12"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\begin{aligned} \bar{p}(\bar{t},\bar{\omega })\approx \bar{p}_0(\bar{\omega }) 10^{-6 \bar{t}/T_{60}(\bar{\omega })}. \end{aligned}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_Equ12.png" style="width:11.88em"/></div></div> <div class="EquationNumber">(3.12)</div></div></div>Apart from substantial further compression, the great advantage of such a parametric description is that it is easy to interpret, allowing artistic direction. Reverberation plugins will typically provide <span class="InlineEquation" id="IEq113"><img alt="$$\bar{p}_0$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq113.png" style="width:1.06em"/></span> as a combination of a broadband “wet gain” and a graphic equalizer, as well as the decay times, <span class="InlineEquation" id="IEq114"><img alt="$$T_{60}(\bar{\omega })$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq114.png" style="width:2.94em"/></span> over frequency bands. For interactive auralization, the artist can exert aesthetic control by the simple means of modifying the reverberation parameters produced from acoustic simulation. For instance, when the player enters a narrow tunnel in VR, footsteps might get a realistic initial power (<span class="InlineEquation" id="IEq115"><img alt="$$\bar{p}_0$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq115.png" style="width:1.06em"/></span>) to convey the constricted space, yet speech might have the wet gain reduced to increase the clarity (<span class="InlineEquation" id="IEq116"><img alt="$$C_{50}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq116.png" style="width:1.56em"/></span>) and improve the intelligibility of dialogue.</div><p class="Para" id="Par87"><strong class="EmphasisTypeBold ">Bidirectional EDR.</strong> For an enclosure where conditions approach ideal diffuse reverberation, the EDR can be a sufficient description. Parametric reverberators will typically ensure that the same EDR is realized at both the ears but that the fine structure is mutually decorrelated, so that the reverberation is perceived by the listener as outside their head. However, in VR applications it becomes important to model the directionality inherent in reverberation because it can become strongly anisotropic. For instance, a visually occluded sound in another room heard through a door will be temporally diffuse, but directionally localized towards the door.</p><p class="Para" id="Par88">The concept of EDR can be extended naturally to the <em class="EmphasisTypeItalic ">bidirectional</em> EDR, <span class="InlineEquation" id="IEq117"><img alt="$$\bar{D_s}(\bar{t},\bar{\omega },\bar{s},\bar{s'};x,x')$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq117.png" style="width:8.19em"/></span>, which adds dependence on direction for both source and listener. It can be constructed and interpreted as follows. Consider a source located at <span class="InlineEquation" id="IEq118"><img alt="$$x'$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq118.png" style="width:1em"/></span> that radiates a Dirac impulse in a beam centered around directional bin center <span class="InlineEquation" id="IEq119"><img alt="$$\bar{s'}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq119.png" style="width:0.94em"/></span>. After propagating through the scene, it is received by the listener at location <em class="EmphasisTypeItalic ">x</em>, who beam-forms in the direction <span class="InlineEquation" id="IEq120"><img alt="$$\bar{s}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq120.png" style="width:0.75em"/></span> and then computes the EDR on the received time-dependent signal. The bidirectional EDR thus captures the frequency-dependent energy decay for all direction-bin pairs <span class="InlineEquation" id="IEq121"><img alt="$${\{}\bar{s},\bar{s'}{\}}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq121.png" style="width:3.12em"/></span>.</p><div class="Para" id="Par89">Invoking the exponential decay model, the bidirectional EDR may be approximated as<div class="Equation NumberedEquation" id="Equ13"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\begin{aligned} \bar{D_s}(\bar{t},\bar{\omega },\bar{s},\bar{s'};x,x') \approx \bar{p}_0(\bar{\omega },\bar{s},\bar{s'};x,x') 10^{-6 \bar{t} /T_{60}(\bar{\omega },\bar{s},\bar{s'};x,x')}. \end{aligned}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_Equ13.png" style="width:24.32em"/></div></div> <div class="EquationNumber">(3.13)</div></div></div>Due to the curse of dimensionality, simulating and rendering the bidirectional EDR can get quite costly despite the simplifications. In practice, one must choose the sampling resolution of all the parameters judiciously depending on the application. An extreme case of this is when we sum over the entire range of a parameter, effectively removing it as a dimension.</div><div class="Para" id="Par90">Let’s consider one example that illustrates the kind of trade-offs offered by statistical modeling in balancing rendering quality and computational complexity. One may profitably compute the <span class="InlineEquation" id="IEq122"><img alt="$$T_{60}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq122.png" style="width:1.44em"/></span> for energy summed over all listener directions <em class="EmphasisTypeItalic ">s</em>, and source directions <span class="InlineEquation" id="IEq123"><img alt="$$s'$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq123.png" style="width:0.94em"/></span>, which amounts to computing the monophonic EDR to derive the reverberation time. In that case, one obtains a simplified hybrid approximation:<div class="Equation NumberedEquation" id="Equ14"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\begin{aligned} \bar{\bar{D_s}}(\bar{t},\bar{\omega },\bar{s},\bar{s'};x,x') \approx \bar{p}_0(\bar{\omega },\bar{s},\bar{s'};x,x') 10^{-6 \bar{t} / T_{60}(\bar{\omega };x,x')}. \end{aligned}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_Equ14.png" style="width:23em"/></div></div> <div class="EquationNumber">(3.14)</div></div></div>The first factor still captures strong anisotropy in reverberant energy, such as reverberation heard by a listener as streaming from a portal, or reverberant power being higher when a human speaker faces a close by reverberant chamber rather than away. In fact, <span class="InlineEquation" id="IEq124"><img alt="$$\bar{p}_0(\bar{\omega },\bar{s},\bar{s'};x,x')$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq124.png" style="width:7.06em"/></span> can be understood as a multiple-input-multiple-output (MIMO) frequency-dependent transfer matrix for incoherent energy between a source and receiver for directional channels sampled via <span class="InlineEquation" id="IEq125"><img alt="$$s'$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq125.png" style="width:0.94em"/></span> and <em class="EmphasisTypeItalic ">s</em>, respectively. The approximation lies in the second factor—directionally varying decay times for a single sound source are not modeled, which may be quite subtle to perceive in many cases.</div></section></section><section class="Section1 RenderAsSection1" id="Sec24"><h2 class="Heading"><span class="HeadingNumber">3.7 </span>Computing the BIR</h2><p class="Para" id="Par91">Acoustic simulation is the key computationally expensive task in modern auralization systems due to the high complexity of today’s virtual scenes. In particular, at every visual frame, for all source and listener pairs with locations <span class="InlineEquation" id="IEq126"><img alt="$$(x,x')$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq126.png" style="width:2.74em"/></span>, the system must compute the BIR <span class="InlineEquation" id="IEq127"><img alt="$$D(t,s,s';x,x')$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq127.png" style="width:6.69em"/></span>, which may then be applied on each source’s audio as discussed in the prior section. There are two distinct ways the problem may be approached: geometric and wave-based methods. In this section, we will discuss the fundamental ideas behind these techniques.</p><section class="Section2 RenderAsSection2" id="Sec25"><h3 class="Heading"><span class="HeadingNumber">3.7.1 </span>Geometric Acoustics (GA)</h3><p class="Para" id="Par92">Geometric <span id="ITerm49">methods</span> approximate sound propagation via the zero-wavelength (infinite frequency) asymptotic limit of the wave equation (<span class="InternalRef"><a href="#Equ1">3.1</a></span>). Borrowing terminology from fluid mechanics, this yields a Lagrangian approach, where packets of energy are tracked explicitly through the scene as they travel along rays and repeatedly scatter into multiple packets in all directions each time they hit the scene boundary. The key strength of geometric methods is speed and flexibility: compared to a full-bandwidth wave simulation, tracing rays can be much cheaper, and it is much easier to incorporate physical phenomena and construct the BIR, assembled by explicitly constructing paths connecting source to listener. Today, these methods are standard in the area of room auralization.</p><p class="Para" id="Par93">Their key challenge falls into two categories. Firstly, one must efficiently search for paths connecting source to listener via complex scenes. Searching costs computation. Doing too little can under-sample the response, causing audible jumps in the rendering. Secondly, diffraction at audible wavelengths must be considered explicitly (since it is not present by default) to ensure plausibility. Both must be incorporated while balancing smooth rendering for moving sources and listener against the CPU cost of geometric analysis inherent in path search.</p><p class="Para" id="Par94">Below, we briefly elaborate on the general design of GA systems and practical implications for VR auralization, and refer the reader to Savioja and Svensson’s excellent survey on the recent developments in GA techniques [<span class="CitationRef"><a epub:type="biblioref" href="#CR87" role="doc-biblioref">87</a></span>].</p><p class="Para" id="Par95"><strong class="EmphasisTypeBold ">Simplified geometry.</strong> Due to the zero-wavelength approximation, geometric methods remain sensitive to geometric detail indefinitely below audible wavelengths. For instance, if one directly used a visual mesh for GA simulation, a coffee mug can create a strong audible echo if the source and listener are connected by a specular reflection path hitting the cup. Such specular glints are observed for light, but not sound with its much longer wavelength. So, it becomes important to build an equivalent simplified acoustical model of the scene which captures only large facets, combined with coefficients that summarize scattering due to diffraction. For instance, the seating area in a concert hall might be replaced with an enclosing box with an equivalent scattering coefficient. This process requires the user to have a degree of acoustical expertise, and inaccuracies can result without carefully specified geometry and boundary data [<span class="CitationRef"><a epub:type="biblioref" href="#CR21" role="doc-biblioref">21</a></span>]. However, for VR auralization, automation is highly desirable, with some recent work along these lines [<span class="CitationRef"><a epub:type="biblioref" href="#CR88" role="doc-biblioref">88</a></span>].</p><p class="Para" id="Par96"><strong class="EmphasisTypeBold ">Deterministic-statistical decomposition.</strong> Geometric methods directly incorporate the deterministic-statistical decomposition in the simulation process to reduce CPU burden. In particular, the two components <span class="InlineEquation" id="IEq128"><img alt="$$D_d$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq128.png" style="width:1.5em"/></span> and <span class="InlineEquation" id="IEq129"><img alt="$$D_s$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq129.png" style="width:1.44em"/></span> are typically computed and rendered separately and then mixed in the final rendering to balance quality and speed.</p><p class="Para" id="Par97">GA methods perform a deterministic path search only up to a certain number of bounces on the scene boundary, called the <em class="EmphasisTypeItalic ">reflection order</em>. This is a key parameter for GA systems because it has a sensitive impact on both performance and rendering quality, varying by system and application. Typically, the user can specify this parameter, which then implicitly determines the number of deterministic peaks rendered, <span class="InlineEquation" id="IEq130"><img alt="$$n_d$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq130.png" style="width:1.24em"/></span>, in (<span class="InternalRef"><a href="#Equ10">3.10</a></span>). To accelerate path search, early methods [<span class="CitationRef"><a epub:type="biblioref" href="#CR86" role="doc-biblioref">86</a></span>] proposed using the image source method [<span class="CitationRef"><a epub:type="biblioref" href="#CR7" role="doc-biblioref">7</a></span>], which is well-suited for single enclosures but scales exponentially with reflection order and does not account for edge diffraction.</p><p class="Para" id="Par98">Following work on beam <span id="ITerm50">tracing</span>, [<span class="CitationRef"><a epub:type="biblioref" href="#CR36" role="doc-biblioref">36</a></span>] showed that in multi-room scenes, precomputing a beam-tree data structure can at once control the exponential scaling and also incorporate edge diffraction which is crucial for plausibility in such densely occluded scenes. The system introduced precomputation as a powerful technique for reducing runtime acoustics computation, which most modern systems employ at least to some degree.</p><p class="Para" id="Par99">A key general concept employed in the beam tracing work in [<span class="CitationRef"><a epub:type="biblioref" href="#CR36" role="doc-biblioref">36</a></span>] is the <em class="EmphasisTypeItalic ">room-portal <span id="ITerm51">decomposition</span>
</em>: an indoor scene with many rooms is approximately decomposed into a set of Simplicial convex shapes that represent room volume, connected by flat portals representing doors. This is a frequently used method in GA systems, as it allows efficient deterministic path search on the discrete graph formed by rooms as nodes and portals as connecting edges. However, room-portal decomposition does not generalize to outdoor or mixed scenes, which is a key limitation that recent research is focusing on to allow fast deterministic search of high-order diffraction paths [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR88" role="doc-biblioref">88</a></span>].</p><p class="Para" id="Par100">Techniques developed for light transport in the computer <span id="ITerm52">graphics</span> community are a great fit for computing the statistical component owing to its phase incoherence. Many methods are possible, such as those based on radiosity [<span class="CitationRef"><a epub:type="biblioref" href="#CR8" role="doc-biblioref">8</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR93" role="doc-biblioref">93</a></span>]. Stochastic path tracing is a standard method in both graphics and acoustics communities today, used originally by DIVA [<span class="CitationRef"><a epub:type="biblioref" href="#CR86" role="doc-biblioref">86</a></span>] and in modern systems like RAVEN [<span class="CitationRef"><a epub:type="biblioref" href="#CR90" role="doc-biblioref">90</a></span>]. More recent improvements use bidirectional path tracing [<span class="CitationRef"><a epub:type="biblioref" href="#CR24" role="doc-biblioref">24</a></span>], which directly exploits the bidirectional reciprocity principle (<span class="InternalRef"><a href="#Equ7">3.7</a></span>) to accelerate computation.</p><p class="Para" id="Par101">GA methods cannot construct the fine structure of the reverberant portion of the response, but as we discussed in Sect. <span class="InternalRef"><a href="#Sec23">3.6.2</a></span>, it is often sufficient to build the bidirectional energy decay relief, <span class="InlineEquation" id="IEq131"><img alt="$$\bar{D_s}(\bar{t},\bar{\omega },\bar{s},\bar{s'};x,x')$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq131.png" style="width:8.19em"/></span>, or some lower dimensional approximation ignoring directionality. With path tracing techniques, this is directly accomplished by accumulating into a histogram indexed on all the function parameters—each path represents an energy packet that accumulates into its corresponding histogram bin. The key parameter trading quality and cost is the number of paths sampled so that the energy value in each histogram bin is sufficiently converged.</p><p class="Para" id="Par102">With simplified scenes admitting a room-portal decomposition one can expect robust convergence, or even use approximations that avoid path tracing altogether [<span class="CitationRef"><a epub:type="biblioref" href="#CR94" role="doc-biblioref">94</a></span>], but for path tracing in complex VR scenes, the required number of paths for a converged histogram can vary significantly based on source and listener locations, <span class="InlineEquation" id="IEq132"><img alt="$$\{x,x'\}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq132.png" style="width:3.38em"/></span>. For instance, if they are connected only through a few narrow apertures in the scene, it can be hard to find connecting paths despite extensive random searching. There is precedence for such issues in computer graphics as well [<span class="CitationRef"><a epub:type="biblioref" href="#CR101" role="doc-biblioref">101</a></span>], representing a frontier for new research with systematic convergence studies, as initiated in [<span class="CitationRef"><a epub:type="biblioref" href="#CR24" role="doc-biblioref">24</a></span>].</p></section><section class="Section2 RenderAsSection2" id="Sec26"><h3 class="Heading"><span class="HeadingNumber">3.7.2 </span>Wave Acoustics (WA)</h3><p class="Para" id="Par103">Wave <span id="ITerm53">acoustic</span>
<span id="ITerm54"/> methods take an Eulerian approach: space time is discretized onto a fictitious background, such as a uniform discrete grid, and then one updates pressure amplitude in each cell at each time-step. Paths are not constructed explicitly, so as energy scatters in various directions from scene surfaces, the amount of information tracked does not change. Thus, arbitrary combinations of diffraction and scattering are naturally captured by wave methods. By running a simulation with a source located at <span class="InlineEquation" id="IEq133"><img alt="$$x'$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq133.png" style="width:1em"/></span>, a discrete approximation of Green’s function <span class="InlineEquation" id="IEq134"><img alt="$$p(t,x;x')$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq134.png" style="width:4.19em"/></span> is directly produced by running a volumetric simulation for a sufficient duration. The BIR <span class="InlineEquation" id="IEq135"><img alt="$$D(t;s,s',x,x')$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq135.png" style="width:6.69em"/></span> may then be computed via accurate plane-wave decomposition in a volume centered at the source and listener location  [<span class="CitationRef"><a epub:type="biblioref" href="#CR2" role="doc-biblioref">2</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR91" role="doc-biblioref">91</a></span>] or via the much faster approximation using instantaneous flux density [<span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>], first applied to audio coding in [<span class="CitationRef"><a epub:type="biblioref" href="#CR74" role="doc-biblioref">74</a></span>].</p><p class="Para" id="Par104"><strong class="EmphasisTypeBold ">Numerical solvers.</strong> The main challenge of wave methods is their computational cost. Since wave solvers directly resolve the detailed wave field by discretizing space and time, their cost scales as the fourth power of the maximum simulated frequency and third power of the scene diameter, due to Nyquist criteria as outlined in Sect. <span class="InternalRef"><a href="#Sec3">3.2.1</a></span>. This made them outright infeasible for most practical uses until the last decade, apart from low-frequency modal simulations up to a few hundred Hertz. However, they have seen a resurgence of interest over the last decade, with many kinds of solvers being actively researched today for auralization, such as spectral methods [<span class="CitationRef"><a epub:type="biblioref" href="#CR52" role="doc-biblioref">52</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR77" role="doc-biblioref">77</a></span>], finite difference methods [<span class="CitationRef"><a epub:type="biblioref" href="#CR49" role="doc-biblioref">49</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR85" role="doc-biblioref">85</a></span>], and the finite element method [<span class="CitationRef"><a epub:type="biblioref" href="#CR71" role="doc-biblioref">71</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR103" role="doc-biblioref">103</a></span>]. Alongside the progress in numerical methods, the increased computational power of CPUs and graphics processors, as well as the availability of increased RAM, now allows simulations of practical cases of interest, such as concert halls, up to mid-frequencies (1 kHz and beyond). This is still short of complete audible bandwidth, and it is common to use approximate extrapolation beyond the band-limit frequency. The compute times remain suitable only for off-line computation, ranging in a few hours. The availability of commodity cloud computation has further aided the wider applicability of wave methods despite the cost.</p><p class="Para" id="Par105"><strong class="EmphasisTypeBold ">Precomputation and static scenes.</strong> The idea of precomputation has been central to the increasing application of wave methods in VR auralization. Real-time auralization with wave methods was first shown to be viable for complex scenes in [<span class="CitationRef"><a epub:type="biblioref" href="#CR80" role="doc-biblioref">80</a></span>]. The method performs multiple simulations off-line and the resulting (monophonic) impulse responses are encoded and stored in a file. At runtime, this file is loaded, and the sampled acoustical data are spatially interpolated for a dynamic source and listener which informs spatialization of the source audio. This overall architecture is followed by most wave-based auralization methods.</p><p class="Para" id="Par106">The disadvantage of <span id="ITerm55">precomputation</span> is that it is limited to static scenes. However, it has the great benefit that the fidelity of acoustical simulation becomes decoupled from runtime CPU usage. One may perform a detailed simulation directly on complex scene geometry ensuring robust results at runtime. These trade-offs are highly analogous to “light baking” which is a common feature of game engines today: expensive global illumination is simulated beforehand on static scenes to ensure fast runtime rendering. Similar to developments in lighting, one can conceivably incorporate local dynamism such as additional occlusion from portals [<span class="CitationRef"><a epub:type="biblioref" href="#CR76" role="doc-biblioref">76</a></span>] or moving objects [<span class="CitationRef"><a epub:type="biblioref" href="#CR84" role="doc-biblioref">84</a></span>] in the future.</p><p class="Para" id="Par107"><strong class="EmphasisTypeBold ">Parametric encoding.</strong> The key research challenge introduced by precomputation is that the BIR field <span class="InlineEquation" id="IEq136"><img alt="$$D(t,s,s',x,x')$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq136.png" style="width:6.69em"/></span> is 11-dimensional and highly oscillatory. Capturing it in detail can easily take an impractical amount of storage. Spatial audio <span id="ITerm56">coding</span> methods such as DirAC [<span class="CitationRef"><a epub:type="biblioref" href="#CR73" role="doc-biblioref">73</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR74" role="doc-biblioref">74</a></span>]<span id="ITerm57"/> demonstrate a path forward, in that they extract and render perceptual properties from directional audio recordings rather than trying to re-create the physical sound field. This in turn is similar in spirit to audio coding methods such as MP3 where precise waveform reconstruction is eschewed in favor of controllable trade-offs between perceived quality and compressed size.</p><p class="Para" id="Par108">These observations have motivated a new thread of auralization research on wave-based parametric methods [<span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR78" role="doc-biblioref">78</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR79" role="doc-biblioref">79</a></span>] that combine precomputed wave acoustics with compact, perceptual coding of the resulting BIR fields. Such methods are practical enough today to be employed in many gaming applications. The deterministic-statistical decomposition plays a crucial role in this encoding stage, as we will elaborate in Sect. <span class="InternalRef"><a href="#Sec31">3.8.4</a></span> when we discuss [<span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>] in more detail.</p><p class="Para" id="Par109"><strong class="EmphasisTypeBold ">Physical encoding.</strong> In a parallel thread, there has been work on methods that directly approximate and convolve the complete BIR without involving perceptual coding. The equivalent source method was proposed in [<span class="CitationRef"><a epub:type="biblioref" href="#CR63" role="doc-biblioref">63</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>], at the expense of restricting to scenes that are a sparse set of exterior-scattering building facades. More recent methods for high-quality building auralization have been developed, which sample and interpolate BIRs for dynamic rendering  [<span class="CitationRef"><a epub:type="biblioref" href="#CR55" role="doc-biblioref">55</a></span>]. The advantage is that no inherent assumptions are made about the perception or the structure of the BIR, but in turn, such systems tend to be more expensive and current technology is limited to static sound sources.</p></section></section><section class="Section1 RenderAsSection1" id="Sec27"><h2 class="Heading"><span class="HeadingNumber">3.8 </span>Auralization Systems</h2><p class="Para" id="Par110">In this section, we will discuss a few illustrative example systems in more detail. We emphasize that this should not be interpreted as a representative survey. Instead, our aim is to illustrate how the design of practical systems can vary widely depending on the intended application, chosen algorithms, and in particular how systems choose to prioritize a subset of the design constraints (Sect. <span class="InternalRef"><a href="#Sec18">3.5</a></span>). <span id="ITerm58">Most</span> of these systems are available for download and experimentation.</p><section class="Section2 RenderAsSection2" id="Sec28"><h3 class="Heading"><span class="HeadingNumber">3.8.1 </span>Room Acoustics for Virtual Environments (RAVEN)</h3><p class="Para" id="Par111">RAVEN [<span class="CitationRef"><a epub:type="biblioref" href="#CR90" role="doc-biblioref">90</a></span>] is a research system built from the ground up aiming for perceptually authentic and real-time auralization in VR. The computational budget is thus on the high side, such as all the resources of a single or few networked computers. This is in line with the intended application: for an acoustician evaluating a planned design, it is more important to hear a result with reliable predictive value, and the precise amount of computation does not matter as long as it is real time. RAVEN is a great example of the archetypal decisions involved in the end-to-end design of modern real-time geometric systems.</p><p class="Para" id="Par112">A key assumption in the system is that the scene is a typical building floor. Many decisions and efficiencies flow naturally. Chiefly, one can employ the room-portal decomposition as discussed in Sect. <span class="InternalRef"><a href="#Sec25">3.7.1</a></span>. Local scene dynamism is also allowed by the system, such as opening or closing doors, with limited precomputation on the scene geometry. However, like most geometric acoustic systems, the scene geometry has to be manually simplified with acoustical expertise to achieve the simplified cells required by rooms and portals. Flexible signal processing that can include artistic design need not be considered, since the application is physical prediction.</p><p class="Para" id="Par113">RAVEN models diffraction on both the deterministic and statistical components of the BIR. The former uses the image source method, with reflection orders up to 3 for real-time evaluation. Edge sources are introduced to account for diffraction paths that, e.g., first undergo a bounce from a flat surface and then diffract around a portal edge. Capturing such effects is especially important for smooth results on dynamic source and listener motion, which RAVEN carefully models.</p><p class="Para" id="Par114">The statistical component uses stochastic ray <span id="ITerm59">tracing</span> with improved convergence using the “diffuse rain” technique [<span class="CitationRef"><a epub:type="biblioref" href="#CR90" role="doc-biblioref">90</a></span>]. To model diffraction for reverberation, a probabilistic scheme is used [<span class="CitationRef"><a epub:type="biblioref" href="#CR95" role="doc-biblioref">95</a></span>] that deflects rays that pass close enough to scene edges. Since the precise reconstruction of the reverberant characteristics is of central importance in architectural acoustics, RAVEN models the complete bidirectional energy decay relief, as illustrated in [<span class="CitationRef"><a epub:type="biblioref" href="#CR90" role="doc-biblioref">90</a></span>, Fig. 5.19].</p></section><section class="Section2 RenderAsSection2" id="Sec29"><h3 class="Heading"><span class="HeadingNumber">3.8.2 </span>Wwise Spatial Audio</h3><p class="Para" id="Par115">Audiokinetic’s Wwise [<span class="CitationRef"><a epub:type="biblioref" href="#CR9" role="doc-biblioref">9</a></span>] is a commonly employed audio engine in video games, alongside many other audio design applications. Wwise provides both geometric acoustical simulation and HRTF spatialization using either object-based or spherical-harmonic processing (Sect. <span class="InternalRef"><a href="#Sec6">3.2.4</a></span>). The system stands in illustrative contrast to RAVEN, showing how different application needs can deeply shape technical choices of auralization systems. A detailed description of ideas and motivation can be found in the series of white papers [<span class="CitationRef"><a epub:type="biblioref" href="#CR23" role="doc-biblioref">23</a></span>].</p><p class="Para" id="Par116">Gaming applications require very low CPU utilization (fraction of a single core) without requiring physical accuracy. But one needs to approximate carefully. The rendering must stay perceptually believable, such as smooth acoustic changes on fast source motion or visual occlusion. Minimizing precomputation is desirable for reducing artist iteration times. Finally, the ability of artists to interpret the acoustic simulation and design the rendered output is paramount.</p><p class="Para" id="Par117">To meet these goals, Wwise also starts with a deterministic-statistical decomposition. Like most geometric systems, the user must provide a simplified audio geometry for the scene, which is the bulk of the work. Once this is done, the system responds interactively without precomputation. The initial sound is derived based on an explicit path search on simplified geometry at runtime, with reflections modeled via image <span id="ITerm60">sources</span> up to some user-controlled reflection order (usually ~3 for efficiency).</p><p class="Para" id="Par118">Importantly, rather than estimating diffraction losses based on physical approximations such as the Uniform Theory of Diffraction [<span class="CitationRef"><a epub:type="biblioref" href="#CR59" role="doc-biblioref">59</a></span>] that cost CPU, the system exposes an abstract “diffraction coefficient” that varies smoothly as the sound source, and corresponding image sources transition between visual occlusion and visibility. This ameliorates the key perceptual deficit of audible loudness jumps that result when diffraction is ignored. The audio designer can draw a function in the user interface to map the diffraction coefficient to loudness attenuation. This design underlines how practical systems balance CPU cost, plausible rendering, and artistic control. Note how just reducing accuracy to gain CPU is not the path taken: instead, one must carefully understand which physical behaviors must be preserved to not violate our (stringent) sensory expectations, such as that sound fields rarely show a sudden audible variation on small movement in everyday life.</p><p class="Para" id="Par119">For modeling the statistical component, the system avoids costly stochastic ray tracing in favor of reverberation flow modeled on a room-portal <span id="ITerm61">decomposition</span> of the simplified scene. The design is in the vein of [<span class="CitationRef"><a epub:type="biblioref" href="#CR94" role="doc-biblioref">94</a></span>], with diffuse energy flow on a graph composed of rooms as nodes and portals as edges. However, in keeping with the primary goal of audio design, the user is free to choose or parametrically design individual filters for each room, while the system ensures that the net result correctly accumulates reverberation and spatializes it as streaming to the listener from (potentially) multiple portals. Again, plausibility, performance, and design are prioritized over adherence to accuracy, keeping in mind the primary use case of scalable rendering for games and VR.</p></section><section class="Section2 RenderAsSection2" id="Sec30"><h3 class="Heading"><span class="HeadingNumber">3.8.3 </span>Steam Audio and Resonance Audio</h3><p class="Para" id="Par120">Steam Audio [<span class="CitationRef"><a epub:type="biblioref" href="#CR100" role="doc-biblioref">100</a></span>] and Resonance Audio [<span class="CitationRef"><a epub:type="biblioref" href="#CR46" role="doc-biblioref">46</a></span>] are geometric acoustics systems also designed for gaming and VR applications with similar considerations as Wwise Spatial Audio. They both offer HRTF spatialization combined with geometric acoustics modeling; however, diffraction is ignored. A distinctive aspect of Steam Audio is the capability to <span id="ITerm62">precompute</span> room reverberation filters (i.e., the statistical component) directly from scene geometry without requiring any simplification, auralized dynamically based on listener location. Resonance Audio on the other hand primarily focuses on highly efficient spatialization [<span class="CitationRef"><a epub:type="biblioref" href="#CR47" role="doc-biblioref">47</a></span>] that scales down to mobile devices for numerous sources, using up to third-order spherical harmonics. In fact, Resonance Audio can be used as a plugin within the Wwise audio engine to perform spatialization, illustrating the utility of the modular design of auralization systems (Sect. <span class="InternalRef"><a href="#Sec2">3.2</a></span>).</p></section><section class="Section2 RenderAsSection2" id="Sec31"><h3 class="Heading"><span class="HeadingNumber">3.8.4 </span>Project Acoustics (PA)</h3><div class="Para" id="Par121">We now consider a wave-based system, Project Acoustics [<span class="CitationRef"><a epub:type="biblioref" href="#CR66" role="doc-biblioref">66</a></span>], which has shown practical viability for gaming [<span class="CitationRef"><a epub:type="biblioref" href="#CR81" role="doc-biblioref">81</a></span>] and VR [<span class="CitationRef"><a epub:type="biblioref" href="#CR45" role="doc-biblioref">45</a></span>] experiences recently. We summarize its key design ideas here; technical details can be found in  [<span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR78" role="doc-biblioref">78</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR79" role="doc-biblioref">79</a></span>]. As is typical for wave acoustics systems (Sect. <span class="InternalRef"><a href="#Sec26">3.7.2</a></span>), costly simulation is performed in a <span id="ITerm63">precomputation</span> stage, shown on the left of Fig. <span class="InternalRef"><a href="#Fig4">3.4</a></span>. Many simulations are performed in parallel that collectively sample and compress the entire BIR field <span class="InlineEquation" id="IEq137"><img alt="$$D(t,s,s',x,x')$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq137.png" style="width:6.69em"/></span> into an acoustic dataset. With today’s commodity cloud computing resources, complete game scenes may be processed in less than an hour.<figure class="Figure" id="Fig4"><div class="MediaObject" id="MO18"><img alt="" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Fig4_HTML.png" style="width:33.98em"/></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig. 3.4</span><p class="SimplePara">High-level architecture of Project Acoustics’ wave-based parametric auralization</p></div></figcaption></figure>
</div><p class="Para" id="Par122">The bidirectional reciprocity principle (<span class="InternalRef"><a href="#Equ7">3.7</a></span>) plays an important role. The listener location, <em class="EmphasisTypeItalic ">x</em>, is typically restricted in motion to head height above the ground, thus varying in two dimensions rather than three, such as the floors of a building. Potential listener locations are sampled in the lowered dimension adapting to local geometry [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>]. Note that source locations, <span class="InlineEquation" id="IEq138"><img alt="$$x'$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq138.png" style="width:1em"/></span>, may still vary in three dimensions. Then, a series of 3D wave simulations are performed with each potential <em class="EmphasisTypeItalic ">listener</em> location acting as <em class="EmphasisTypeItalic ">source</em> during simulation. The reduction in BIR field dimension by one yields an order-of-magnitude reduction in data size.</p><p class="Para" id="Par123">Project Acoustics’ main idea is to employ lossy perceptual encoding on the BIR field to bring it within practical storage budgets of a few hundred MB. The deterministic-statistical decomposition is employed at this stage. The initial arrival time and direction are encoded explicitly to ensure the correct localization of the sound, and the rest of the response is encoded statistically (i.e., <span class="InlineEquation" id="IEq139"><img alt="$$n_d=1$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq139.png" style="width:3em"/></span> referring to Sect. <span class="InternalRef"><a href="#Sec22">3.6.1</a></span>). An example simulation snapshot is shown in Fig. <span class="InternalRef"><a href="#Fig4">3.4</a></span> with the corresponding initial path encoding visualized on the right. Color shows frequency-averaged loudness, and arrows show the localized direction at the listener location, x, with the source location <span class="InlineEquation" id="IEq140"><img alt="$$x'$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq140.png" style="width:1em"/></span> varying over the image. For instance, any source inside the room would be localized by the listener as arriving from the door, so the arrows inside the room consistently point in the door-to-listener direction. The perceptual parameters vary smoothly over space, mirroring our everyday experience, allowing further compression via entropy coding [<span class="CitationRef"><a epub:type="biblioref" href="#CR78" role="doc-biblioref">78</a></span>].</p><div class="Para" id="Par124">The statistical component simplifies (<span class="InternalRef"><a href="#Equ14">3.14</a></span>) further to average over all simulated frequencies, approximating the bidirectional energy decay relief as<div class="Equation" id="Equ15"><div class="EquationWrapper"><div class="EquationContent"><div class="MediaObject"><img alt="$$\begin{aligned} \bar{\bar{D_s}}(\bar{t},\bar{\omega },\bar{s},\bar{s'};x,x') \approx \bar{p}_0(\bar{s},\bar{s'};x,x') 10^{-6\bar{t}/T_60(x,x')}. \end{aligned}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_Equ15.png" style="width:21.26em"/></div></div></div></div>The directions <span class="InlineEquation" id="IEq141"><img alt="$${\{}\bar{s},\bar{s'}{\}}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq141.png" style="width:3.12em"/></span> sample the six signed Cartesian directions, thus discretizing <span class="InlineEquation" id="IEq142"><img alt="$$\bar{p}_0$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq142.png" style="width:1.06em"/></span> to a <span class="InlineEquation" id="IEq143"><img alt="$$6\times 6$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq143.png" style="width:2.44em"/></span> “reflections transfer” matrix that compactly approximates directional reverberation, alongside a single <span class="InlineEquation" id="IEq144"><img alt="$$T_{60}$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq144.png" style="width:1.44em"/></span> value across direction and frequency. Visualizations of the reflections transfer matrix can be found in [<span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>] that illustrate how it captures anisotropic effects like directional reverberation from portals or nearby reverberant chambers.</div><p class="Para" id="Par125">One can observe that this encoding is quite simplified and can be expected to only plausibly reproduce the simulated BIR field. The choices result from the system’s goal: capturing key geometry-dependent audio cues within a compact storage budget—too large a size simply obviates practical use. For instance, one could encode much more detailed information such as numerous (<span class="InlineEquation" id="IEq145"><img alt="$$n_d\sim 20\!-\!50$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq145.png" style="width:5.5em"/></span>) individual reflection peaks [<span class="CitationRef"><a epub:type="biblioref" href="#CR80" role="doc-biblioref">80</a></span>] but that is far too costly, in turn motivating recent research on how one might trade between number of encoded peaks (<span class="InlineEquation" id="IEq146"><img alt="$$n_d$$" src="../images/478239_1_En_3_Chapter/478239_1_En_3_Chapter_TeX_IEq146.png" style="width:1.24em"/></span>) and perceived authenticity [<span class="CitationRef"><a epub:type="biblioref" href="#CR18" role="doc-biblioref">18</a></span>].</p><p class="Para" id="Par126">Generally speaking, precomputed systems shift the trade-off from quality-versus-CPU as with runtime propagation simulation to quality-versus-storage (Sects. <span class="InternalRef"><a href="#Sec28">3.8.1</a></span> and <span class="InternalRef"><a href="#Sec29">3.8.2</a></span>). This holds regardless of whether the precomputation is geometric (Steam Audio) or wave-based (Project Acoustics). Precomputation can introduce limitations such as slower artist turnaround times and static scenes, but in return significantly lowers the barrier to viability whenever the available CPU is severely restricted, which is the case for gaming applications or untethered VR platforms.</p><p class="Para" id="Par127">Wave simulation forces precomputation in today’s systems due to its high computational cost, but its advantage compared to geometric methods is that complex visual scene geometry is processed directly, without requiring any manual simplification. Further, arbitrary order of diffraction around detailed geometry in general scenes (trees, buildings, chairs, etc.) is modeled, which avoids the risk of not sampling a salient path. In sum, one pays a high, fixed precomputation cost largely insensitive to scene complexity, and if that is feasible, obtains robust results directly from visual geometry with a low CPU cost.</p><p class="Para" id="Par128">As discussed in Sect. <span class="InternalRef"><a href="#Sec23">3.6.2</a></span>, parametric approaches enable intuitive controls for sound designers, which is of crucial importance in gaming applications, as we also saw in the design of the Wwise Spatial Audio system. In the case of PA, the parameters are looked up at each source-listener location pair at runtime (right of Fig. <span class="InternalRef"><a href="#Fig4">3.4</a></span>), and it becomes possible for the artist to specify dynamic aesthetic <em class="EmphasisTypeItalic ">modifications</em> of the physically-based baseline produced by simulation [<span class="CitationRef"><a epub:type="biblioref" href="#CR44" role="doc-biblioref">44</a></span>]. The sounds and modified acoustic parameters can then be sent to any efficient parametric reverberation and spatialization sub-system for rendering the binaural output.</p></section></section><section class="Section1 RenderAsSection1" id="Sec32"><h2 class="Heading"><span class="HeadingNumber">3.9 </span>Summary and Outlook</h2><p class="Para" id="Par129">Creating an immersive and interactive sonic experience for virtual reality applications requires auralizing complex 3D scenes robustly and within tight real-time constraints. To meet these requirements, real-time systems follow a modular approach of dividing the problem into sound production, propagation, and spatialization. These can be mathematically formulated via the source directivity function, bidirectional impulse responses (BIR), and head-related transfer functions (HRTFs), respectively, leading to a general framework. Human auditory perception of acoustic responses deeply informs most systems, motivating optimizations such as the deterministic-statistical decomposition of the BIR.</p><p class="Para" id="Par130">We discussed many design considerations that inform the design of practical systems. We illustrated with a few auralization systems how the application requirements shape design choices, ranging from perceptual authenticity in architectural acoustics, to game engines where believability, audio design, and CPU usage take central priority. With more development, one can hope for auralization systems in the future that are capable of scaling their quality-compute trade-offs to span all applications of VR auralization. Such a convergent evolution would be in line with current trends in visual rendering where off-line photo-realistic rendering techniques and real-time game techniques are becoming increasingly unified [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>].</p><p class="Para" id="Par131">Looking to the future, real-time auralization faces two major research challenges: scalability and scene dynamics. Game and VR scenes are trending toward completely open worlds where entire cities are modeled at once, spanning tens of kilometers, with numerous sound sources, where very few assumptions can be made about the scene’s geometry or complexity. Similar considerations hold for engineering prediction of outdoor acoustics, such as noise levels in a city. We need real-time techniques that can scale to such challenging scenarios within CPU budgets, perhaps by analogy with level-of-detail techniques used in graphics. Scene dynamism is a related challenge. Many current game engines allow the users to make global changes to immersive 3D worlds in real time. Dynamic techniques are required that can model, for instance, the diffraction loss around a just-created wall within tolerable latency. Progress in this direction has only just begun [<span class="CitationRef"><a epub:type="biblioref" href="#CR35" role="doc-biblioref">35</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR75" role="doc-biblioref">75</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR83" role="doc-biblioref">83</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR84" role="doc-biblioref">84</a></span>].</p><p class="Para" id="Par132">The open challenge for the future is to build real-time auralization systems that can gracefully scale from plausible to accurate audio rendering for complex, dynamic, city-scale scenes depending on available computational resources. There is much to be done, and many undiscovered, foundational ideas remain.</p></section><div class="License LicenseSubType-cc-by"><a href="https://creativecommons.org/licenses/by/4.0"><img alt="Creative Commons" src="../css/cc-by.png"/></a><p class="SimplePara"><strong class="EmphasisTypeBold ">Open Access</strong> This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (<span class="ExternalRef"><a href="http://creativecommons.org/licenses/by/4.0/"><span class="RefSource">http://​creativecommons.​org/​licenses/​by/​4.​0/​</span></a></span>), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p><p class="SimplePara">The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p></div><aside aria-labelledby="Bib1Heading" class="Bibliography" id="Bib1"><div epub:type="bibliography" role="doc-bibliography"><div class="Heading" id="Bib1Heading">References</div><ol class="BibliographyWrapper"><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">1.</div><div class="CitationContent" id="CR1">Abel, J. S., Huang, P.: A Simple, Robust Measure of Reverberation Echo Density in Audio Engineering Society Convention 121 (2006).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">2.</div><div class="CitationContent" id="CR2">Ahrens, J.: Analytic Methods of Sound Field Synthesis (T-Labs Series in Telecommunication Services) Two thousand, twelfth (Springer, 2014).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">3.</div><div class="CitationContent" id="CR3">Ajdler, T., Sbaiz, L.,Vetterli, M.: The Plenacoustic Function and Its Sampling. Signal Processing, IEEE Transactions on <strong class="EmphasisTypeBold ">54</strong>, 3790–3804 (2006).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">4.</div><div class="CitationContent" id="CR4">Albert, D. G., Liu, L.: The Effect of Buildings on Acoustic Pulse Propagation in an Urban Environment. The Journal of the Acoustical Society of America <strong class="EmphasisTypeBold ">127</strong>, 1335–1346 (2010).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">5.</div><div class="CitationContent" id="CR5">Algazi, V. R., Duda, R. O., Thompson, D. M., Avendano, C.: The cipic hrtf database in Proceedings of the 2001 IEEE Workshop on the Applications of Signal Processing to Audio and Acoustics (Cat. No. 01TH8575) (2001), 99–102.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">6.</div><div class="CitationContent" id="CR6">Allen, A., Raghuvanshi, N.: Aerophones in Flatland: Interactive Wave Simulation of Wind Instruments. ACM Trans. Graph. <strong class="EmphasisTypeBold ">34</strong> (2015).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">7.</div><div class="CitationContent" id="CR7">Allen, J. B., Berkley, D. A.: Image Method for Efficiently Simulating Small- Room Acoustics. J. Acoust. Soc. Am <strong class="EmphasisTypeBold ">65</strong>, 943–950 (1979).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">8.</div><div class="CitationContent" id="CR8">Antani, L., Chandak, A., Taylor, M., Manocha, D.: Direct-to-Indirect Acoustic Radiance Transfer. IEEE Transactions on Visualization and Computer Graphics <strong class="EmphasisTypeBold ">18</strong>, 261–269 (2012).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">9.</div><div class="CitationContent" id="CR9">AudioKinetic Inc.: Wwise <span class="ExternalRef"><a href="https://www.audiokinetic.com/products/wwise/"><span class="RefSource">https://​www.​audiokinetic.​com/​products/​wwise/​</span></a></span>. 2018.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">10.</div><div class="CitationContent" id="CR10">Avni, A. et al.: Spatial perception of sound fields recorded by spherical microphone arrays with varying spatial resolution. The Journal of the Acoustical Society of America <strong class="EmphasisTypeBold ">133</strong>, 2711–2721 (2013).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">11.</div><div class="CitationContent" id="CR11">Ben-Hur, Z., Brinkmann, F., Sheaffer, J., Weinzierl, S., Rafaely, B.: Spectral equalization in binaural signals represented by order-truncated spherical harmonics. The Journal of the Acoustical Society of America <strong class="EmphasisTypeBold ">141</strong>, 4087–4096 (2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">12.</div><div class="CitationContent" id="CR12">Bilbao, S.: Numerical Sound Synthesis: Finite Difference Schemes and Simulation in Musical Acoustics First (Wiley, 2009).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">13.</div><div class="CitationContent" id="CR13">Bilbao, S., Hamilton, B.: Directional Sources inWave-Based Acoustic Simulation. IEEE/ACM Transactions on Audio, Speech, and Language Processing <strong class="EmphasisTypeBold ">27</strong>, 415–428 (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">14.</div><div class="CitationContent" id="CR14">Bilbao, S. et al.: Physical Modeling, Algorithms, and Sound Synthesis: The NESS Project. Computer Music Journal <strong class="EmphasisTypeBold ">43</strong>, 15–30 (2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">15.</div><div class="CitationContent" id="CR15">Bilinski, P., Ahrens, J., Thomas, M. R., Tashev, I. J., Platt, J. C.: HRTF magnitude synthesis via sparse representation of anthropometric features in 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (2014), 4468–4472.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">16.</div><div class="CitationContent" id="CR16">Born, M.,Wolf, E.: Principles of Optics: 60th Anniversary Edition 7th edition. English (Cambridge University Press, Cambridge, 2019).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1017/9781108769914"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">17.</div><div class="CitationContent" id="CR17">Breebaart, J. et al.: Spatial Audio Object Coding (SAOC) - The Upcoming MPEG Standard on Parametric Object Based Audio Coding English. In (Audio Engineering Society, 2008).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">18.</div><div class="CitationContent" id="CR18">Brinkmann, F., Gamper, H., Raghuvanshi, N., Tashev, I.: Towards Encoding Perceptually Salient Early Reflections for Parametric SpatialAudio Rendering English. in Audio Engineering Society Convention 148 (Audio Engineering Society, 2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">19.</div><div class="CitationContent" id="CR19">Brinkmann, F., Weinzierl, S.: Comparison of Head-Related Transfer Functions Pre-Processing Techniques for Spherical Harmonics Decomposition. English (2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">20.</div><div class="CitationContent" id="CR20">Brinkmann, F. et al.: A Cross-Evaluated Database of Measured and Simulated HRTFs Including 3D Head Meshes, Anthropometric Features, and Headphone Impulse Responses. en. Journal of the Audio Engineering Society <strong class="EmphasisTypeBold ">67</strong>, 705–718 (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">21.</div><div class="CitationContent" id="CR21">Brinkmann, F. et al.: A Round Robin on Room Acoustical Simulation and Auralization. J. Acoustical Soc. of Am. (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">22.</div><div class="CitationContent" id="CR22">Brungart, D. S.,Kordik, A. J., Simpson, B. D.: Effects of Headtracker Latency in Virtual Audio Displays. en. J. Audio Eng. Soc. <strong class="EmphasisTypeBold ">54</strong>, 13 (2006).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">23.</div><div class="CitationContent" id="CR23">Buffoni, L.-X.: A Wwise Approach to Spatial Audio (Blog Series) https://​blog.​audiokinetic.​com/​a-wwise-approach-to-spatial-audiopart-1/​.​ 2020.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">24.</div><div class="CitationContent" id="CR24">Cao, C., Ren, Z., Schissler, C., Manocha, D., Zhou, K.: Interactive Sound Propagation with Bidirectional Path Tracing. ACM Transactions on Graphics (TOG) <strong class="EmphasisTypeBold ">35</strong>, 180 (2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">25.</div><div class="CitationContent" id="CR25">Chaitanya, C. R. A., Snyder, J. M., Godin, K., Nowrouzezahrai, D., Raghuvanshi, N.: Adaptive Sampling for Sound Propagation. IEEE Trans. on Vis. Comp. Graphics <strong class="EmphasisTypeBold ">25</strong>, 1846–1854 (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">26.</div><div class="CitationContent" id="CR26">Chaitanya, C. R. A. et al.: Directional Sources and Listeners in Interactive Sound Propagation Using ReciprocalWave Field Coding. ACM Transactions on Graphics (SIGGRAPH 2020) <strong class="EmphasisTypeBold ">39</strong> (2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">27.</div><div class="CitationContent" id="CR27">Cooper, C. M., Abel, J. S.: Digital Simulation of "Brassiness" and Amplitude- Dependent Propagation Speed in Wind Instruments in Proc. 13th Int. Conf. on Digital Audio Effects (DAFx-10) (2010), 1–6.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">28.</div><div class="CitationContent" id="CR28">Dalenbäck, B.-I.: CATT-Acoustic Software https://​www.​catt.​se/​.​2021.​</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">29.</div><div class="CitationContent" id="CR29">Davis, L. S. et al.: High Order Spatial Audio Capture and Its Binaural Head-Tracked Playback Over Headphones with HRTF Cues English. In Audio Engineering Society Convention 119 (Audio Engineering Society, 2005).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">30.</div><div class="CitationContent" id="CR30">Dobashi,Y.,Yamamoto, T., Nishita, T.: Real-TimeRendering ofAerodynamic Sound Using Sound Textures Based on Computational Fluid Dynamics. ACM Trans. Graph. <strong class="EmphasisTypeBold ">22</strong>, 732–740 (2003).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">31.</div><div class="CitationContent" id="CR31">Dobashi, Y., Yamamoto, T., Nishita, T.: Synthesizing Sound from Turbulent Field Using Sound Textures for Interactive Fluid Simulation. Computer Graphics Forum (Proc. EUROGRAPHICS 2004) <strong class="EmphasisTypeBold ">23</strong>, 539–546 (2004).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">32.</div><div class="CitationContent" id="CR32">Embrechts, J.-J.: Review on the Applications of Directional Impulse Responses in Room Acoustics in Actes Du CFA 2016 (2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">33.</div><div class="CitationContent" id="CR33">Epic Games: Unreal Engine 5 Documentation https://​docs.​unrealengine.​com/​5.​0/​en-US/​RenderingFeature​s/​Lumen/​.​ 2020.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">34.</div><div class="CitationContent" id="CR34">Erraji, A., Stienen, J., Vorländer, M.: The Image Edge Model. en. Acta Acustica <strong class="EmphasisTypeBold ">5</strong>, 17 (2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">35.</div><div class="CitationContent" id="CR35">Fan, Z., Vineet, V., Gamper, H., Raghuvanshi, N.: Fast Acoustic Scattering Using Convolutional Neural Networks in ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (2020), 171–175.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">36.</div><div class="CitationContent" id="CR36">Funkhouser, T. et al.: A Beam Tracing Method for Interactive Architectural Acoustics. The Journal of the Acoustical Society of America <strong class="EmphasisTypeBold ">115</strong>, 739–756 (2004).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">37.</div><div class="CitationContent" id="CR37">Gade, A. in Springer Handbook of Acoustics (ed Rossing, T.) Two thousand, seventh. Chap. 9 (Springer, 2007).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">38.</div><div class="CitationContent" id="CR38">Gamper, H., Johnston, D., Tashev, I. J.: Interaural time delay personalisation using incomplete head scans in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (2017), 461–465.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">39.</div><div class="CitationContent" id="CR39">Gardner, B., Martin, K., et al.: HRFT Measurements of a KEMAR Dummyhead Microphone (Vision and Modeling Group, Media Laboratory, Massachusetts Institute of Technology, 1994).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">40.</div><div class="CitationContent" id="CR40">Gardner, W. G., Martin, K. D.: HRTF measurements of a KEMAR. The Journal of the Acoustical Society of America <strong class="EmphasisTypeBold ">97</strong>, 3907–3908 (1995).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">41.</div><div class="CitationContent" id="CR41">Geronazzo, M., Spagnol, S., Bedin, A.,Avanzini, F.: Enhancing vertical localization with image-guided selection of non-individual head-related transfer functions in 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (2014), 4463–4467.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">42.</div><div class="CitationContent" id="CR42">Gerzon, M. A.: Ambisonics in Multichannel Broadcasting and Video. J. Audio Eng. Soc. <strong class="EmphasisTypeBold ">33</strong>, 859–871 (1985).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">43.</div><div class="CitationContent" id="CR43">Gerzon, M. A.: Periphony: With-Height Sound Reproduction. J. Audio Eng. Soc <strong class="EmphasisTypeBold ">21</strong>, 2–10 (1973).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">44.</div><div class="CitationContent" id="CR44">Godin, K., Gamper, H., Raghuvanshi, N.: Aesthetic Modification of Room Impulse Responses for Interactive Auralization in AES International Conference on Immersive and Interactive Audio (Audio Engineering Society, 2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">45.</div><div class="CitationContent" id="CR45">Godin, K. W., Rohrer, R., Snyder, J., Raghuvanshi, N.: Wave Acoustics in a Mixed Reality Shell in AES Conf. on Audio for Virt. and Augmented Reality (AVAR) (2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">46.</div><div class="CitationContent" id="CR46">Google Inc.: Resonance Audio https://​developers.​google.​com/​resonance-audio/​.​ 2018.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">47.</div><div class="CitationContent" id="CR47">Gorzel, M. et al.: Efficient Encoding and Decoding of Binaural Sound with Resonance Audio in AES International Conference on Immersive and Interactive Audio (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">48.</div><div class="CitationContent" id="CR48">Guezenoc, C., Seguier, R.: HRTF individualization: A survey. arXiv preprint <span class="ExternalRef"><a href="http://arxiv.org/abs/2003.06183"><span class="RefSource">arXiv:​2003.​06183</span></a></span> (2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">49.</div><div class="CitationContent" id="CR49">Hamilton, B., Bilbao, S.: FDTD Methods for 3-D RoomAcoustics Simulation With High-Order Accuracy in Space and Time. IEEE/ACM Transactions on Audio, Speech, and Language Processing <strong class="EmphasisTypeBold ">25</strong> (2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">50.</div><div class="CitationContent" id="CR50">He, J., Ranjan, R., Gan, W.-S.: Fast continuous HRTF acquisition with unconstrained movements of human subjects in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (2016), 321–325.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">51.</div><div class="CitationContent" id="CR51">Hold, C., Gamper, H., Pulkki, V., Raghuvanshi, N., Tashev, I.: Improving Binaural Ambisonics Decoding by Spherical Harmonics Domain Tapering and Coloration Compensation in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">52.</div><div class="CitationContent" id="CR52">Hornikx, M., Forssén, J.: Modelling of Sound Propagation to Three- Dimensional Urban Courtyards Using the Extended Fourier PSTD Method. Applied Acoustics <strong class="EmphasisTypeBold ">72</strong>, 665–676 (2011).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">53.</div><div class="CitationContent" id="CR53">Howe, M. S.: Theory of Vortex Sound 1st edition. English (Cambridge University Press, New York, 2002).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1017/CBO9780511755491"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">54.</div><div class="CitationContent" id="CR54">Hughes, J. et al.: Computer Graphics: Principles and Practice 3rd edition. English (Addison-Wesley Professional, Upper Saddle River, New Jersey, 2013).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">55.</div><div class="CitationContent" id="CR55">Jörgensson, F. K. P.: Wave-Based Virtual Acoustics English. PhD thesis (Technical University of Denmark, 2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">56.</div><div class="CitationContent" id="CR56">Jot, J.-M.: An Analysis/Synthesis Approach to Real-Time Artificial Reverberation in [Proceedings] ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech, and Signal Processing <strong class="EmphasisTypeBold ">2</strong> (1992), 221–224.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">57.</div><div class="CitationContent" id="CR57">Kajiya, J. T.: The Rendering Equation in Proceedings of the 13th Annual Conference on Computer Graphics and Interactive Techniques <strong class="EmphasisTypeBold ">20</strong> (ACM, New York, NY, USA, 1986), 143–150.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">58.</div><div class="CitationContent" id="CR58">Katz, B. F.: Boundary element method calculation of individual head-related transfer function. I. Rigid model calculation. The Journal of the Acoustical Society of America <strong class="EmphasisTypeBold ">110</strong>, 2440–2448 (2001).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">59.</div><div class="CitationContent" id="CR59">Kouyoumjian, R., Pathak, P.: A Uniform Geometrical Theory of Diffraction for an Edge in a Perfectly Conducting Surface. Proceedings of the IEEE <strong class="EmphasisTypeBold ">62</strong>, 1448–1461 (1974).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">60.</div><div class="CitationContent" id="CR60">Kuttruff, H.: Room Acoustics Fourth (Taylor &amp; Francis, 2000).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">61.</div><div class="CitationContent" id="CR61">Li, S., Tobbala, A., Peissig, J.: Towards Mobile 3D HRTF Measurement English. in (Audio Engineering Society, 2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">62.</div><div class="CitationContent" id="CR62">Litovsky, R. Y., Colburn, S. H., Yost, W. A., Guzman, S. J.: The Precedence Effect. The Journal of the Acoustical Society of America <strong class="EmphasisTypeBold ">106</strong>, 1633–1654 (1999).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1121/1.427914"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">63.</div><div class="CitationContent" id="CR63">Mehra, R., Antani, L., Kim, S., Manocha, D.: Source and Listener Directivity for Interactive Wave-Based Sound Propagation. IEEE Transactions on Visualization and Computer Graphics <strong class="EmphasisTypeBold ">20</strong>, 495–503 (2014).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1109/TVCG.2014.38"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">64.</div><div class="CitationContent" id="CR64">Mehra, R. et al.: Wave-Based Sound Propagation in Large Open Scenes Using an Equivalent Source Formulation. ACM Trans. Graph. <strong class="EmphasisTypeBold ">32</strong> (2013).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">65.</div><div class="CitationContent" id="CR65">Meshram, A. et al.: P-HRTF: Efficient personalized HRTF computation for high-fidelity spatial sound in 2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) (2014), 53–61.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">66.</div><div class="CitationContent" id="CR66">Microsoft Corp.: Project Acoustics https://​aka.​ms/​acoustics.​ 2018.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">67.</div><div class="CitationContent" id="CR67">Noisternig, M., Sontacchi, A., Musil, T., Hóldrich, R.: A 3D ambisonic based binaural sound reproduction system in Audio Engineering Society Conference: 24th International Conference: Multichannel Audio, The New Reality (2003).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">68.</div><div class="CitationContent" id="CR68">Oliver, R. J., Jot, J.-M.: Efficient Multi-Band DigitalAudio Graphic Equalizer with Accurate Frequency Response Control in Audio Engineering Society Convention 139 (2015).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">69.</div><div class="CitationContent" id="CR69">Paasonen, J., Karapetyan, A., Plogsties, J., Pulkki, V.: Proximity of Surfaces - Acoustic and Perceptual Effects. J. Audio Eng. Soc <strong class="EmphasisTypeBold ">65</strong>, 997–1004 (2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">70.</div><div class="CitationContent" id="CR70">Pierce, A. D.: Acoustics: An Introduction to Its Physical Principles and Applications (Acoustical Society of America, 1989).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">71.</div><div class="CitationContent" id="CR71">Pind, F. et al.: Time Domain Room Acoustic Simulations Using the Spectral Element Method. The Journal of the Acoustical Society of America <strong class="EmphasisTypeBold ">145</strong>, 3299–3310 (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">72.</div><div class="CitationContent" id="CR72">Pulkki, V.: Virtual Sound Source Positioning Using Vector Base Amplitude Panning. English. Journal of the Audio Engineering Society <strong class="EmphasisTypeBold ">45</strong>, 456–466 (1997).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">73.</div><div class="CitationContent" id="CR73">Pulkki, V.: Spatial Sound Reproduction with Directional Audio Coding. J. Audio Eng. Soc. (2007).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">74.</div><div class="CitationContent" id="CR74">Pulkki,V., Merimaa, J.: Spatial ImpulseResponseRendering II:Reproduction of Diffuse Sound and Listening Tests. J. Aud. Eng. Soc. <strong class="EmphasisTypeBold ">54</strong>, 3–20 (2006).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">75.</div><div class="CitationContent" id="CR75">Pulkki, V., Svensson, U. P.: Machine-Learning-Based Estimation and Rendering of Scattering in Virtual Reality. J. Acoust. Soc. Am. <strong class="EmphasisTypeBold ">145</strong>, 2664–2676 (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">76.</div><div class="CitationContent" id="CR76">Raghuvanshi, N.: Dynamic Portal Occlusion for Precomputed Interactive Sound Propagation. <span class="ExternalRef"><a href="http://arxiv.org/abs/2107.11548"><span class="RefSource">arXiv:​2107.​11548</span></a></span> [cs, eess] (2021).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">77.</div><div class="CitationContent" id="CR77">Raghuvanshi, N., Narain, R., Lin, M. C.: Efficient and Accurate Sound Propagation Using Adaptive Rectangular Decomposition. IEEE Transactions on Visualization and Computer Graphics <strong class="EmphasisTypeBold ">15</strong>, 789–801 (2009).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">78.</div><div class="CitationContent" id="CR78">Raghuvanshi, N., Snyder, J.: ParametricWave Field Coding for Precomputed Sound Propagation. ACM Transactions on Graphics (TOG) - Proceedings of ACM SIGGRAPH 2014 <strong class="EmphasisTypeBold ">33</strong> (2014).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">79.</div><div class="CitationContent" id="CR79">Raghuvanshi, N., Snyder, J.: Parametric Directional Coding for Precomputed Sound Propagation. ACM Trans. Graph. (2018).<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a href="https://doi.org/10.1145/3197517.3201339"><span><span>Crossref</span></span></a></span></span></div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">80.</div><div class="CitationContent" id="CR80">Raghuvanshi, N., Snyder, J., Mehra, R., Lin, M. C., Govindaraju, N. K.: PrecomputedWave Simulation for Real-Time Sound Propagation of Dynamic Sources in Complex Scenes. ACM Transactions on Graphics <strong class="EmphasisTypeBold ">29</strong> (2010).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">81.</div><div class="CitationContent" id="CR81">Raghuvanshi, N., Tennant, J., Snyder, J.: Triton: Practical Pre-Computed Sound Propagation for Games and Virtual Reality. The Journal of the Acoustical Society of America <strong class="EmphasisTypeBold ">141</strong>, 3455–3455 (2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">82.</div><div class="CitationContent" id="CR82">Rindel, J. H., Christensen, C. L.: The Use of Colors, Animations and Auralizations in Room Acoustics in Internoise 2013 (2013).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">83.</div><div class="CitationContent" id="CR83">Rosen, M., Godin, K. W., Raghuvanshi, N.: Interactive Sound Propagation for Dynamic Scenes Using 2D Wave Simulation. en. Computer Graphics Forum <strong class="EmphasisTypeBold ">39</strong>, 39–46 (2020).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">84.</div><div class="CitationContent" id="CR84">Rungta, A., Schissler, C.,Rewkowski,N., Mehra, R., Manocha, D.: Diffraction Kernels for Interactive Sound Propagation in Dynamic Environments. IEEE Transactions on Visualization and Computer Graphics <strong class="EmphasisTypeBold ">24</strong>, 1613–1622 (2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">85.</div><div class="CitationContent" id="CR85">Savioja, L.: Real-Time 3D Finite-Difference Time-Domain Simulation of Mid-Frequency Room Acoustics in 13th International Conference on Digital Audio Effects (2010).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">86.</div><div class="CitationContent" id="CR86">Savioja, L., Huaniemi, J., Lokki, T.,Vaananen, R.: Creating InteractiveVirtual Acoustic Environments. J. Audio Eng. Soc. (1999).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">87.</div><div class="CitationContent" id="CR87">Savioja, L., Svensson, U. P.: Overview of Geometrical Room Acoustic Modeling Techniques. The Journal of the Acoustical Society of America <strong class="EmphasisTypeBold ">138</strong>, 708–730 (2015).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">88.</div><div class="CitationContent" id="CR88">Schissler, C., Mehra, R., Manocha, D.: High-Order Diffraction and Diffuse Reflections for Interactive Sound Propagation in Large Environments. ACM Transactions on Graphics (TOG) <strong class="EmphasisTypeBold ">33</strong>, 39 (2014).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">89.</div><div class="CitationContent" id="CR89">Schonstein, D., Katz, B. F.: HRTF selection for binaural synthesis from a database using morphological parameters in International Congress on Acoustics (ICA) (2010).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">90.</div><div class="CitationContent" id="CR90">Schröder, D.: Physically Based Real-Time Auralization of Interactive Virtual Environments (Logos Verlag, 2011).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">91.</div><div class="CitationContent" id="CR91">Sheaffer, J., Van Walstijn, M., Rafaely, B., Kowalczyk, K.: Binaural Reproduction of Finite Difference Simulations Using Spherical Array Processing. IEEE/ACM Trans. Audio, Speech and Lang. Proc. <strong class="EmphasisTypeBold ">23</strong>, 2125–2135 (2015).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">92.</div><div class="CitationContent" id="CR92">Shinn-Cunningham, B. G.: Distance cues for virtual auditory space in Proceedings of the IEEE-PCM <strong class="EmphasisTypeBold ">2000</strong> (2000), 227–230.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">93.</div><div class="CitationContent" id="CR93">Siltanen, S., Lokki, T., Kiminki, S., Savioja, L.: The Room Acoustic Rendering Equation. J. Acoust. Soc. Am. (2007).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">94.</div><div class="CitationContent" id="CR94">Stavrakis, E., Tsingos, N., Calamia, P.: Topological Sound Propagation with Reverberation Graphs. Acta Acustica/Acustica - the Journal of the European Acoustics Association (EAA) (2008).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">95.</div><div class="CitationContent" id="CR95">Stephenson, U. M., Svensson, U. P.: An Improved Energetic Approach to Diffraction Based on theUncertainty Principle in 19th Int. Cong. onAcoustics (ICA) (2007).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">96.</div><div class="CitationContent" id="CR96">Takala, T., Hahn, J.: Sound Rendering. SIGGRAPH Comput. Graph. <strong class="EmphasisTypeBold ">26</strong>, 211–220 (1992).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">97.</div><div class="CitationContent" id="CR97">Theis, T. N., Wong, H.-S. P.: The end of Moore’s law: A new beginning for information technology. Computing in Science &amp; Engineering <strong class="EmphasisTypeBold ">19</strong>, 41–50 (2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">98.</div><div class="CitationContent" id="CR98">Tukuljac, H. P. et al.: A Sparsity Measure for Echo Density Growth in General Environments in ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (2019), 1–5.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">99.</div><div class="CitationContent" id="CR99">Valimaki, V., Parker, J. D., Savioja, L., Smith, J. O., Abel, J. S.: Fifty Years of Artificial Reverberation. IEEE Transactions on Audio, Speech, and Language Processing <strong class="EmphasisTypeBold ">20</strong>, 1421–1448 (2012).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">100.</div><div class="CitationContent" id="CR100">Valve Corporation: Steam Audio ().</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">101.</div><div class="CitationContent" id="CR101">Veach, E., Guibas, L. J.: Metropolis Light Transport in Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques (ACM Press/Addison-Wesley Publishing Co., USA, 1997), 65–76.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">102.</div><div class="CitationContent" id="CR102">Vorländer, M.: Auralization: Fundamentals of Acoustics, Modelling, Simulation, Algorithms andAcousticVirtualReality (RWTHedition) First (Springer, 2007).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">103.</div><div class="CitationContent" id="CR103">Wang, H., Sihar, I., Pagán Muñoz, R., Hornikx, M.: Room Acoustics Modelling in the Time-Domain with the Nodal Discontinuous Galerkin Method. The Journal of the Acoustical Society of America <strong class="EmphasisTypeBold ">145</strong>, 2650–2663 (2019).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">104.</div><div class="CitationContent" id="CR104">Wang, J.-H., Qu, A., Langlois, T. R., James, D. L.: TowardWave-based Sound Synthesis for Computer Animation. ACM Trans. Graph. <strong class="EmphasisTypeBold ">37</strong>, 109:1–109:16 (2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">105.</div><div class="CitationContent" id="CR105">Zhang, W., Abhayapala, T. D., Kennedy, R. A., Duraiswami, R.: Insights into Head-Related Transfer Function: Spatial Dimensionality and Continuous Representation. The Journal of the Acoustical Society of America <strong class="EmphasisTypeBold ">127</strong>, 2347–2357 (2010).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">106.</div><div class="CitationContent" id="CR106">Zotkin, D., Hwang, J., Duraiswaini, R., Davis, L. S.: HRTF personalization using anthropometric measurements in 2003 IEEEWorkshop on Applications of Signal Processing to Audio and Acoustics (IEEE Cat. No. 03TH8684) (2003), 157–160.</div></li></ol></div></aside></div></div></body></html>