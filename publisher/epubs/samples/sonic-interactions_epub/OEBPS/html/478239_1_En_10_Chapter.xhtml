<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops"><head><title>Audio in Multisensory Interactions: From Experiments to Experiences</title><meta content="text/html; charset=utf-8" http-equiv="content-type"/><link href="../css/springer_epub.css" rel="styleSheet" type="text/css"/></head><body><div epub:type="chapter" role="doc-chapter"><div class="ChapterContextInformation"><div class="ContextInformation" id="Chap10"><div class="ChapterCopyright">© The Author(s) 2023</div><span class="ContextInformationAuthorEditorNames">M. Geronazzo, S. Serafin<span class="CollaboratorDesignation"> (eds.)</span></span><span class="ContextInformationBookTitles"><span class="BookTitle">Sonic Interactions in Virtual Environments</span></span><span class="ContextInformationSeries"><span class="SeriesTitle" lang="en">Human–Computer Interaction Series</span></span><span class="ChapterDOI"><a href="https://doi.org/10.1007/978-3-031-04021-4_10">https://doi.org/10.1007/978-3-031-04021-4_10</a></span></div></div><!--Begin Abstract--><div class="MainTitleSection"><h1 class="ChapterTitle" lang="en">10. Audio in Multisensory Interactions: From Experiments to Experiences</h1></div><div class="AuthorGroup"><div class="AuthorNames"><span class="Author"><span class="AuthorName">Stefania Serafin</span><sup><a href="#Aff34">1</a> <a aria-label="Contact information for this author" href="#ContactOfAuthor1"><span class="ContactIcon"> </span></a></sup></span></div><div class="Affiliations"><div class="Affiliation" id="Aff34"><span class="AffiliationNumber">(1)</span><div class="AffiliationText">Department of Architecture, Design, and Media Technology, Aalborg University Copenhagen, Copenhagen, Denmark</div></div><div class="ClearBoth"> </div></div><div class="Contacts"><div class="Contact" id="ContactOfAuthor1"><div class="ContactIcon"> </div><div class="ContactAuthorLine"><span class="AuthorName">Stefania Serafin</span></div><div class="ContactAdditionalLine"><span class="ContactType">Email: </span><a href="mailto:sts@create.aau.dk">sts@create.aau.dk</a></div></div></div></div><section class="Abstract" id="Abs1" lang="en" role="doc-abstract"><h2 class="Heading">Abstract</h2><p class="Para" id="Par1">In the real and virtual world, we usually experience sounds in combination with at least an additional modality, such as vision, touch or proprioception. Understanding how sound enhances, substitutes or modifies the way we perceive and interact with the world is an important element when designing interactive multimodal experiences. In this chapter, we present an overview of sound in a multimodal context, ranging from basic experiments in multimodal perception to more advanced interactive experiences in virtual reality.</p></section><!--End Abstract--><div class="Fulltext"><section class="Section1 RenderAsSection1" id="Sec1"><h2 class="Heading"><span class="HeadingNumber">10.1 </span>Introduction</h2><p class="Para" id="Par2">This book examines the role of sound in virtual environments (VEs). However, most of our interactions with both the physical and virtual worlds appear through a combination of different sensory modalities. Auditory feedback is often the consequence of an action produced by touch and is presented in the form of a combination of auditory, haptic and visual feedback. Let us consider for example the simple action of <span id="ITerm1">walking</span>: The auditory feedback is given by the sound produced by the shoes interacting with the floor, the visual feedback is the surrounding environment, and the haptic feedback is the feeling of the surface one is stepping on. It is important that these different sensory modalities are perceived in synchronization, in order to experience <span id="ITerm2">a</span> coherent action.</p><p class="Para" id="Par3">Since sound can be perceived from all directions, it is ideal for providing information when the eyes are otherwise occupied. This could be a situation where someone’s visual <span id="ITerm3">attention</span>should be entirely devoted to a specific task such as driving or a surgeon operating on a patient [<span class="CitationRef"><a epub:type="biblioref" href="#CR46" role="doc-biblioref">46</a></span>]. Another notable property of the human auditory system is its sensitivity to the temporal aspects of sound [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>]. In many instances, response times for auditory stimuli are faster than those for visual stimuli [<span class="CitationRef"><a epub:type="biblioref" href="#CR55" role="doc-biblioref">55</a></span>]. Furthermore, given the higher temporal resolution of the auditory system compared to the visual system, people can resolve subtle temporal dynamics in sounds more readily than in visual stimuli; thus the rendering of data into sound may manifest periodic or other temporal information that is not easily perceivable in visualizations [<span class="CitationRef"><a epub:type="biblioref" href="#CR17" role="doc-biblioref">17</a></span>]. Moreover, the ears are capable of decomposing complex auditory scenes [<span class="CitationRef"><a epub:type="biblioref" href="#CR3" role="doc-biblioref">3</a></span>] and selectively attending to certain sources, as seen, for example, in the cocktail party problem [<span class="CitationRef"><a epub:type="biblioref" href="#CR7" role="doc-biblioref">7</a></span>]<span id="ITerm4"/>. Audition, then, may be the most appropriate modality for simple and intuitive (see [<span class="CitationRef"><a epub:type="biblioref" href="#CR9" role="doc-biblioref">9</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR37" role="doc-biblioref">37</a></span>]) information display when data have complex patterns, express meaningful changes in time, or require immediate action.</p><div class="Para" id="Par4">In this chapter, an overview is presented of how knowledge of human perception and cognition can be helpful in the design of multimodal systems where interactive sonic feedback plays an important role. Table <span class="InternalRef"><a href="#Tab1">10.1</a></span> presents a typology of different kinds of cross- modal interactions, adapted from [<span class="CitationRef"><a epub:type="biblioref" href="#CR2" role="doc-biblioref">2</a></span>]<span id="ITerm5"/>.<div class="Table" id="Tab1"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table 10.1</span><p class="SimplePara">Typology of different kinds of cross-modal interactions</p></div></div><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; border-left: 0.5pt solid ; border-right: 0.5pt solid ; "><colgroup><col class="tcol1 align-left"/><col class="tcol2 align-left"/><col class="tcol3 align-left"/></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Cross-modal interaction</p></th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Description</p></th><th style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Example</p></th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Amodal mapping</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Use of VEs or other representational system to map abstract or amodal information (e.g., time, amount, etc.) to some continuous or discrete sensory cue</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">The use of colour mapping and relative size in graphics and scientific visualization (e.g., colour, size, depth, etc.)</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Cross-modal mapping</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Use of a VE to map one or more dimensions of a sensory stimulus to another sensory channel</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">An oscilloscope</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Intersensory biases</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Stimuli from two or more sensory channels may represent discrepant/conflicting information</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Ventriloquism effect [<span class="CitationRef"><a epub:type="biblioref" href="#CR24" role="doc-biblioref">24</a></span>]</p></td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Cross-modal enhancement</p></td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Stimuli from one sensory channel enhance or alter the perceptual interpretation of stimulation from another sensory channel</p></td><td style="border-bottom: 0.5pt solid ; text-align: left;"><p class="SimplePara">Increased perceived visual fidelity of display as a result of increased auditory fidelity</p></td></tr><tr><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Cross-modal transfers or illusions</p></td><td style="border-right: 0.5pt solid ; text-align: left;"><p class="SimplePara">Stimulation in one sensory channel leads to the illusion of stimulation in another sensory channel</p></td><td style="text-align: left;"><p class="SimplePara">Synesthesia</p></td></tr></tbody></table></div>
</div><p class="Para" id="Par5">Sonic feedback can interact with visual or haptic feedback in different ways. As an example, cross-modal mapping represents the situation where one or more dimensions of a sound are mapped to visual or haptic feedback: A beeping sound combined with a flashing light. In cross-modal mapping, there is no specific interaction between the two modalities, but simply a function that connects some parameters of one modality to the parameters of another.</p><p class="Para" id="Par6"><span id="ITerm6">Intersensory biases</span> become important where audition and a second modality provide conflicting cues. In the following section, several examples of intersensory biases will be provided. In most of these situations, the user tries to perceptually integrate the conflicting information. This conflict might lead to a bias towards a stronger modality. One classic example is the <span id="ITerm7">ventriloquism effect</span> [<span class="CitationRef"><a epub:type="biblioref" href="#CR24" role="doc-biblioref">24</a></span>], which illustrates the dominance of visual over auditory information when spatially discrepant audio and visual cues are experienced as co-localized at the location of the visual cue.</p><p class="Para" id="Par7">The name clearly derives from the ventriloquists, who are able to give the impression that the speaking voice is originated from the dummy they are holding, as opposed as from the person herself. This effect is commonly used in cinemas and home theatres where, although the sound physically originates at the speakers, it appears as if coming from the moving images on screen. The ventriloquist effect occurs because the visual estimates of location are typically more accurate than the auditory estimates of location, and therefore the overall perception of location is largely determined by vision. This phenomenon is also known as visual capture [<span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>]. Another classic example is the <span id="ITerm8">Colavita effect</span> [<span class="CitationRef"><a epub:type="biblioref" href="#CR8" role="doc-biblioref">8</a></span>]. In the original experiment, Colavita presented participants with an auditory (tone) or visual (light) stimulus, to which they were instructed to respond by pressing the tone key or light key respectively. When presented with bimodal stimuli, the visual dominance effect refers to the phenomenon where participants respond more often to the visual component.</p><p class="Para" id="Par8"><span id="ITerm9">Vision</span> is indeed the dominant sense in many circumstances. On one hand, visual dominance over hearing and other sensory modalities has been frequently demonstrated (e.g., [<span class="CitationRef"><a epub:type="biblioref" href="#CR45" role="doc-biblioref">45</a></span>]), and a neural basis has been posited for visual dominance in processing audiovisual objects (e.g., [<span class="CitationRef"><a epub:type="biblioref" href="#CR48" role="doc-biblioref">48</a></span>]). <span id="ITerm10">Cross-modal enhancement</span> refers to stimuli from one sensory channel enhancing or altering the perceptual interpretation of stimuli from another sensory channel. As an example, three studies presented in [<span class="CitationRef"><a epub:type="biblioref" href="#CR57" role="doc-biblioref">57</a></span>] show how high-quality auditory displays coupled with high-quality visual displays increase the quality perception of the visual displays relative to the evaluation of the visual display alone. Moreover, the same study shows how low-quality auditory displays coupled with high-quality visual displays decrease the perception of quality of the auditory displays relative to the evaluation of the auditory display alone. These studies were performed by manipulating the pixel resolution of the visual display and Gaussian white noise level, and by manipulating the sampling frequency of the auditory display and Gaussian white noise level. These findings strongly suggest that the quality of realism in an audiovisual display must be a function of both auditory and visual display fidelities and their interactions. Cross-modal enhancements can occur even when extra-modal input does not provide information directly meaningful for the task. An early study by Stein asked subjects to rate the intensity of a beam of light. Their findings showed that the test subjects believed the light to be brighter when it was accompanied by a brief, broadband auditory stimulus than when it was presented alone. The auditory stimulus produced more enhancement for lower visual intensities, regardless of the relative location of the auditory cue source.</p><p class="Para" id="Par9"><span id="ITerm11">Cross-modal transfers</span> or illusions are situations where stimulation in one sensory channel leads to the illusion of stimulation in another sensory channel. An example of this is <span id="ITerm12">synesthesia</span>, which in the audio-visual domain is expressed as the ability to see a colour while hearing a sound. When considering inter-sensory discrepancies, Welch and Warren propose a modality appropriateness hypothesis [<span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>] that suggests that various sensory modalities are differentially well-suited to the perception of different events. Generally, it is supposed that vision is more appropriate for the perception of spatial location than audition, with touch sited somewhere in between. Audition is most appropriate for the perception of temporally structured events. Touch is more appropriate than audition for the perception of texture, where vision and touch may be about equally appropriate for the perception of textures. The appropriateness is a consequence of the different temporal and spatial resolution of the auditory, haptic and visual systems. Moreover, especially when it is combined with touch stimulation, sound increases the sense of immersion [<span class="CitationRef"><a epub:type="biblioref" href="#CR63" role="doc-biblioref">63</a></span>].</p><p class="Para" id="Par10">Apart from the way that the different senses can interact, the auditory channel also presents some advantages when compared to other modalities. For example, humans have a complete sphere of receptivity around the head, while visual feedback has a limited spatial region in terms of field-of-view and field-of-regard. Because auditory information is primarily temporal, the temporal resolution of the auditory system is more precise. We can discriminate between a single click and a pair of clicks when the gap is only a few tens of microseconds [<span class="CitationRef"><a epub:type="biblioref" href="#CR30" role="doc-biblioref">30</a></span>]. Perception of temporal changes in the visual modality is much poorer, and the fastest visible flicker rate in normal conditions is about 40–50 Hz [<span class="CitationRef"><a epub:type="biblioref" href="#CR4" role="doc-biblioref">4</a></span>]. In <span id="ITerm13">multi-sensory interaction</span>, therefore, audio tends to elicit the shortest response time [<span class="CitationRef"><a epub:type="biblioref" href="#CR33" role="doc-biblioref">33</a></span>].</p><p class="Para" id="Par11">In contrast, the maximum spatial resolution (contrast sensitivity) of the human eye is approximately 1/30 degrees, a much finer resolution than that of the auditory system, which is approximately 1 degree. Humans are sensitive to sounds arriving from anywhere within the environment whereas the visual field is limited to the frontal hemisphere, with good resolution limited specifically to the foveal region. Therefore, while the spatial resolution of the auditory modality is cruder, it can serve as a cue to events occurring outside the visual field-of-view.</p><p class="Para" id="Par12">In the rest of this chapter, we provide an overview of the interaction between audition and vision and between audition and touch, together with guidelines on how such knowledge can be used in the design of interactive sonic systems. By understanding how we naturally interact in a world where several sensorial stimuli are provided, we can apply this understanding to the design of sonic interactive systems. Research on multisensory perception and cognition can provide us with important guidelines on how to design virtual environments where interactive sound plays an important role. Through technical advancements such as mobile technologies and 3D interfaces, it has become possible to design systems that have similar natural multimodal properties as the physical world. These future interfaces understand human multimodal communication and can actively anticipate and act in line with human capabilities and limitations. A large challenge for the near future is the development of such natural multimodal interfaces, something that requires the active participation of industry, technology, and the human sciences.</p></section><section class="Section1 RenderAsSection1" id="Sec2"><h2 class="Heading"><span class="HeadingNumber">10.2 </span>Audio-Visual Interactions</h2><p class="Para" id="Par13">Research into multimodal interaction between audition and other modalities has primarily focused on the interaction between audition and <span id="ITerm14">vision</span>. This choice is naturally due to the fact that audition and vision are the most dominant modalities in the human perceptual system [<span class="CitationRef"><a epub:type="biblioref" href="#CR29" role="doc-biblioref">29</a></span>]. A well-known multimodal phenomenon is the <span id="ITerm15">McGurk effect</span> [<span class="CitationRef"><a epub:type="biblioref" href="#CR38" role="doc-biblioref">38</a></span>]. The McGurk effect is an example of how vision alters speech perception; for instance, the sound “ba” is perceived as “da” when viewed with the lip movements for “ga”. Notice that in this case, the percept is different from both the visual and auditory stimuli, so this is an example of intersensory bias, as described in the previous section.</p><p class="Para" id="Par14">The different experiments described until now show a dominance of vision over audition, when conflicting cues are provided. However, this is not always the case. As an example, in [<span class="CitationRef"><a epub:type="biblioref" href="#CR53" role="doc-biblioref">53</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR54" role="doc-biblioref">54</a></span>], a visual illusion induced by sound is described. When a single visual flash is accompanied by multiple auditory beeps, the single flash is perceived as multiple flashes. These results were obtained by flashing a uniform white disk for a variable number of times, 50 milliseconds apart, on a black background. Flashes were accompanied by a variable number of beeps, each spaced 57 milliseconds apart. Observers were asked to judge how many visual flashes were presented on each trial. The trials were randomized and each stimulus combination was run five times on eight naive observers. Surprisingly, observers consistently and incorrectly reported seeing multiple flashes whenever a single flash was accompanied by more than one beep [<span class="CitationRef"><a epub:type="biblioref" href="#CR53" role="doc-biblioref">53</a></span>]. This experiment is known as sound-induced flash illusion. A follow-up experiment investigated whether the illusory flashes could be perceived independently at different spatial locations [<span class="CitationRef"><a epub:type="biblioref" href="#CR26" role="doc-biblioref">26</a></span>]. Two bars were displayed at two locations, creating an apparent motion. All subjects reported that an illusory bar was perceived with the second beep at a location between the real bars. This is analogous to the cutaneous rabbit perceptual illusion, where trains of successive cutaneous pulses delivered at a few widely separated locations produce sensations at many in-between points [<span class="CitationRef"><a epub:type="biblioref" href="#CR19" role="doc-biblioref">19</a></span>]. As a matter of fact, perception of time, wherein auditory estimates are typically more accurate, is dominated by hearing.</p><p class="Para" id="Par15">Another experiment explored whether two objects appear to bounce of each other or simply cross, if observers hear a beep when the objects could be in contact. In this particular case, a desktop computer displayed two identical objects moving towards each others. The display was ambiguous to provide two different interpretations after the objects met: They could either bounce off each other or cross. Since collisions usually produce a characteristic impact sound, introducing such sound when objects met promoted the perception of bouncing versus crossing. This experiment is usually known as motion-bounce illusion [<span class="CitationRef"><a epub:type="biblioref" href="#CR51" role="doc-biblioref">51</a></span>]. In a subsequent study, Sekuler and Sekuler found that any transient sound temporally aligned with the would-be collision increased the likelihood of a bounce percept [<span class="CitationRef"><a epub:type="biblioref" href="#CR50" role="doc-biblioref">50</a></span>]. This includes a pause, a flash of light on the screen, or a sudden disappearance of the discs. Auditory dominance has also been found in other examples with respect to time-based abilities such as precise temporal processing [<span class="CitationRef"><a epub:type="biblioref" href="#CR47" role="doc-biblioref">47</a></span>], temporal localization [<span class="CitationRef"><a epub:type="biblioref" href="#CR5" role="doc-biblioref">5</a></span>], and estimation of time durations [<span class="CitationRef"><a epub:type="biblioref" href="#CR43" role="doc-biblioref">43</a></span>]. Lipscomb and Kendall [<span class="CitationRef"><a epub:type="biblioref" href="#CR34" role="doc-biblioref">34</a></span>] provide another example of auditory dominance in a multimedia context (film). These researchers found that variation in participant semantic differential ratings was influenced more by the musical component than by the visual element. Particularly interesting in its implications for processing multisensory experiences is [<span class="CitationRef"><a epub:type="biblioref" href="#CR22" role="doc-biblioref">22</a></span>] pointing to the disappearance of <span id="ITerm16">visual dominancewhen</span> a visual signal is presented simultaneously with an auditory and haptic signal (i.e., as a tri-sensory combination). The authors concluded that while vision can dominate both the auditory and the haptic sensory modalities, this is limited to bi-sensory combinations in which the visual signal is combined with another single stimulus.</p><p class="Para" id="Par16">More recent investigations examined the role of ecological auditory feedback in affecting multimodal perception of visual <span id="ITerm17">content</span>. As an example, in a study presented in [<span class="CitationRef"><a epub:type="biblioref" href="#CR15" role="doc-biblioref">15</a></span>], the combined perceptual effect of visual and auditory information on the perception of a moving object’s trajectory was investigated. Inspired by the experimental paradigm presented in [<span class="CitationRef"><a epub:type="biblioref" href="#CR27" role="doc-biblioref">27</a></span>], the visual stimuli consisted of a perspective rendering of a ball moving in a three-dimensional box. Each video was paired with one of three sound conditions: Silence, the sound of a ball <span id="ITerm18">rolling</span>, or the sound of a ball hitting the ground. It was found that the sound condition influenced whether observers were more likely to perceive the ball as rolling back in depth on the floor of the box or jumping in the frontal <span id="ITerm19">plane</span>.</p><p class="Para" id="Par17">Another interesting study related to the role of auditory cues in the perception of visual stimuli is the one presented in [<span class="CitationRef"><a epub:type="biblioref" href="#CR60" role="doc-biblioref">60</a></span>]. Two psychophysical studies were conducted to test whether visual sensitivity to point-light depictions of human gait reflects the action specific co-occurrence of visual and auditory cues typically produced by walking people. To perform the experiment, visual walking patterns were captured using a <span id="ITerm20">motion capture</span>system, and a between-subject experimental procedure was adopted. Specifically, subjects were randomly exposed to one of the three experimental conditions: No sound, <span id="ITerm21">footstep sounds</span>, or a pure tone at 1000 Hz, which represented a control case. Visual sensitivity to coherent human gait was measured by asking subjects if they could detect a person walking or not. Such sensitivity was greatest in the presence of temporally coincident and action-consistent sounds, in this case, the sound of footsteps. Visual sensitivity to human gait with coincident sounds that were not action-consistent, in this case the pure tone, was significantly lower and did not significantly differ from visual sensitivity to gaits presented without sound.</p><p class="Para" id="Par18">As an additional interaction between audition and vision, sound can help the user search for an object within a cluttered, continuously changing environment. It has been shown that a simple auditory pip drastically decreases search times for a synchronized visual object that is normally very difficult to find. This is known as the pip and pop effect [<span class="CitationRef"><a epub:type="biblioref" href="#CR62" role="doc-biblioref">62</a></span>]. Visual feedback can also affect several aspects of a musical performance, although in this chapter affective and emotional aspects of a musical performance are not considered. As an example, Schutz and Lipscomb report an audio-visual illusion in which an expert musician’s gestures affect the perceived duration of a note without changing its acoustic length [<span class="CitationRef"><a epub:type="biblioref" href="#CR49" role="doc-biblioref">49</a></span>]. To demonstrate this, they recorded a world-renowned marimba player performing single notes on a marimba using long and short gestures. They paired both types of sounds with both types of gestures, resulting in a combination of natural (i.e., congruent gesture-note pairs) and hybrid (i.e., incongruent gesture-note pairs) stimuli. They informed participants that some auditory and visual components had been mismatched, and asked them to judge tone duration based on the auditory component alone. Despite these instructions, the participants’ duration ratings were strongly influenced by visual gesture information. As a matter of fact, notes were rated as longer when paired with long gestures than when paired with short gestures. These results are somehow puzzling, since they contradict the view that judgments of tone duration are relatively immune from visual influence [<span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>], that is, in temporal tasks visual influence on audition is negligible. However, the results are not based on information quality, but rather on perceived causality, given that visual influence in this paradigm is dependent on the presence of an ecologically plausible audiovisual relationship.</p><p class="Para" id="Par19">Indeed, it is also possible to consider the characteristics of vision and audition to predict which modality will prevail when conflicting information is provided. In this direction, [<span class="CitationRef"><a epub:type="biblioref" href="#CR31" role="doc-biblioref">31</a></span>] introduced the notion of auditory and visual objects. They describe the different characteristics of audition and vision, claiming that a primary source of information for vision is a surface, while a secondary source of information is the location and colour of sources. On the other hand, a primary source of information for audition is a source and a secondary source of information is a surface.</p><p class="Para" id="Par20">In [<span class="CitationRef"><a epub:type="biblioref" href="#CR16" role="doc-biblioref">16</a></span>], a theory is suggested on how our brain merges the different sources of information coming from the different modalities, specifically audition, vision, and touch. The first is what is called sensory combination, which means the maximization of information delivered from the different sensory modalities. The second strategy is called sensory integration, which means the reduction of variance in the sensory estimate to increase its reliability. Sensory combination describes interactions between sensory signals that are not redundant. By contrast, sensory integration describes interactions between redundant signals. Ernst and coworkers [<span class="CitationRef"><a epub:type="biblioref" href="#CR16" role="doc-biblioref">16</a></span>] describe the integration of sensory information as a bottom-up process.</p><p class="Para" id="Par21">The modality precision, also called <span id="ITerm22">modality appropriateness hypothesis</span>, by [<span class="CitationRef"><a epub:type="biblioref" href="#CR64" role="doc-biblioref">64</a></span>], is often cited when trying to explain which modality dominates under what circumstances. This hypothesis states that discrepancies are always resolved in favour of the more precise or more appropriate modality. In spatial <span id="ITerm23">tasks</span>, for example, the visual modality usually dominates, because it is the most precise at determining spatial information. However, according to [<span class="CitationRef"><a epub:type="biblioref" href="#CR16" role="doc-biblioref">16</a></span>], this terminology is misleading because it is not the modality itself or the stimulus that dominates. Rather, the dominance is determined by the estimate and how reliably it can be derived within a specific modality from a given stimulus.</p><p class="Para" id="Par22">A major design dilemma involves the extent to which audio interfaces should maintain the conventions of visual interfaces [<span class="CitationRef"><a epub:type="biblioref" href="#CR40" role="doc-biblioref">40</a></span>]. Indeed, most attempts at auditory display seek to emulate or translate elements of visual interfaces to the auditory modality. While retrofitting visual interfaces with sound can offer some consistencies across modalities, the constraints of this approach may hinder the design of auditory interfaces. While visual objects exist primarily in space, auditory stimuli occur in time. A more appropriate approach to auditory interface design, therefore, may require designers to focus more strictly on auditory capabilities. Such interfaces may present the items and objects of the interface in a fast, linear fashion over time rather than attempting to provide auditory versions of the spatial relationships found in visual interfaces.</p></section><section class="Section1 RenderAsSection1" id="Sec3"><h2 class="Heading"><span class="HeadingNumber">10.3 </span>Embodied Interactions</h2><p class="Para" id="Par23">The experiments described until now assume a passive observer, in the sense that a subject is exposed to a fixed sequence of audiovisual stimuli and is asked to report on the resulting perceptual experience. When a subject is interacting with the stimuli provided, a tight sensory motor coupling is enabled, that is an important characteristic of embodied perception. According to <span id="ITerm24">embodiment</span>theory, a person and the environment form a pair in which the two parts are coupled and determine each other. The term <em class="EmphasisTypeItalic ">embodied</em> highlights two points: First, cognition depends upon the kinds of experience that are generated from specific sensorimotor capacities. Second, these individual sensorimotor capacities are themselves embedded in a biological, psychological, and cultural context [<span class="CitationRef"><a epub:type="biblioref" href="#CR14" role="doc-biblioref">14</a></span>]<span id="ITerm25"/>.</p><p class="Para" id="Par24">The notion of embodied interaction is based on the view that meanings are present in the actions that people engage in while interacting with objects, with other people, and with the environment in general. Embodied interfaces try to exploit the phenomenological attitude of looking at the direct experience, and let the meanings and structures emerge as experienced phenomena. Embodiment is not a property of artefacts but rather a property of how actions are performed with or through the artefacts.</p><p class="Para" id="Par25">The central role of our body in perception, cognition and interaction, has been previously addressed by philosophers (e.g., [<span class="CitationRef"><a epub:type="biblioref" href="#CR39" role="doc-biblioref">39</a></span>]), psychologists (e.g., [<span class="CitationRef"><a epub:type="biblioref" href="#CR41" role="doc-biblioref">41</a></span>]) and neuroscientists (e.g., [<span class="CitationRef"><a epub:type="biblioref" href="#CR10" role="doc-biblioref">10</a></span>]). A rather recent approach to the understanding of the design process, especially in its early stages, has been to focus on the role of multimodality and the contribution of non-verbal channels as key means of communication, <span id="ITerm26">kinaesthetic thinking</span>, and more generally of doing design [<span class="CitationRef"><a epub:type="biblioref" href="#CR59" role="doc-biblioref">59</a></span>]. Audio-haptic interactions, described in the following section, also require a continuous action-feedback loop between a person and the environment, an important characteristic of embodied perception. Another approach, called embodied sound design, proposes to place the bodily experience (i.e., communication of sonic concepts through vocal and gestural imitations) at the centre of the sound creation process [<span class="CitationRef"><a epub:type="biblioref" href="#CR12" role="doc-biblioref">12</a></span>].</p><p class="Para" id="Par26">The role of the body in HCI has overall recently gained more attention, and interested readers can refer to the book by Hook [<span class="CitationRef"><a epub:type="biblioref" href="#CR23" role="doc-biblioref">23</a></span>] and to Chap. <span class="ExternalRef"><a href="478239_1_En_7_Chapter.xhtml"><span class="RefSource">7</span></a></span> in this volume.</p></section><section class="Section1 RenderAsSection1" id="Sec4"><h2 class="Heading"><span class="HeadingNumber">10.4 </span>Audio-Haptic Interactions</h2><p class="Para" id="Par27">Although the investigation of audio-haptic interactions has not received as much attention as audiovisual interactions, it is certainly an interesting field of research, especially considering the tight connections existing between the sense of touch and <span id="ITerm27">audition</span>. As a matter of fact, both audition and touch are sensitive to the very same kind of physical property, that is, mechanical pressure in the form of oscillations. The tight correlation between the information content (oscillatory patterns) being conveyed in the two senses can potentially support interactions of an integrative nature at a variety of levels along the sensory pathways. Auditory cues are normally elicited when one touches everyday objects, and these sounds often convey useful informational regarding the nature of the objects [<span class="CitationRef"><a epub:type="biblioref" href="#CR18" role="doc-biblioref">18</a></span>]. The feeling of skin dryness or moistness that arises when we rub our hands against each other is subjectively referred to the friction forces at the epidermis. Yet, it has been demonstrated that acoustic information also participates in this bodily sensation, because altering the sound arising from the hand rubbing action changes our sensation of dryness or moistness at the skin. This phenomenon is known as the <span id="ITerm28">parchment-skin illusion</span> [<span class="CitationRef"><a epub:type="biblioref" href="#CR25" role="doc-biblioref">25</a></span>].</p><p class="Para" id="Par28">The parchment-skin illusion is an example of how interactive auditory feedback can affect subjects’ haptic sensation. Specifically, in the experiment demonstrating the rubber-skin illusion, subjects were asked to sit with a microphone close to their hands, and then to rub their hands against each other. The sound of hands rubbing was captured by a microphone; they were then manipulated in real time, and played back through headphones. The sound was modified by attenuating the overall amplitude and by amplifying the high frequencies. Subjects were asked to rate the haptic sensation in their palms as a function of the different auditory cues provided, in a scale ranging from very moist to very dry. Results show that the provided auditory feedback significantly affected the perception of the skin’s dryness. This study was extended in [<span class="CitationRef"><a epub:type="biblioref" href="#CR20" role="doc-biblioref">20</a></span>], by using a more rigorous psychophysical testing procedure. Results reported a similar increase in smooth-dry scale correlated to changes in auditory feedback, but not in the roughness judgments per se. However, both studies provide convincing empirical evidence demonstrating the modulatory effect of auditory cues on people’s haptic perception of a variety of different surfaces. A similar experiment was performed combining auditory cues with haptic cues at the tongue. Specifically, subjects were asked to chew on potato chips, and the sound produced was again captured and manipulated in real time. Results show that the perception of potato chips’ crispness was affected by the auditory feedback provided [<span class="CitationRef"><a epub:type="biblioref" href="#CR56" role="doc-biblioref">56</a></span>]. A surprising audio-haptic bodily illusion that demonstrates human observers rapidly update their assumptions about the material qualities of their body is the <span id="ITerm29">marble hand illusion</span> [<span class="CitationRef"><a epub:type="biblioref" href="#CR52" role="doc-biblioref">52</a></span>]. By repeatedly gently hitting participants’ hand while progressively replacing the natural sound of the hammer against the skin with the sound of a hammer hitting a piece of marble, it was possible to induce an illusory misperception of the material properties of the hand. After 5 min, the hand started feeling stiffer, heavier, harder, less sensitive, and unnatural, and showed enhanced galvanic skin response to threatening stimuli. This bodily illusion demonstrates that the experience of the material of our body can be quickly updated through multisensory integration. Another interesting example where sounds again affect body perception is shown in [<span class="CitationRef"><a epub:type="biblioref" href="#CR58" role="doc-biblioref">58</a></span>]. Here, the illusion is applied to footstep sounds. By digitally varying sounds produced by walking, it is possible to vary one’s perception of weight.</p><p class="Para" id="Par29">Lately, artificial cues are appearing in audiohaptic interfaces, allowing us to carefully control the variations to the provided feedback and the resulting perceived effects on exposed subjects [<span class="CitationRef"><a epub:type="biblioref" href="#CR13" role="doc-biblioref">13</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR42" role="doc-biblioref">42</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR61" role="doc-biblioref">61</a></span>]. Artificial auditory cues have also been used in the context of sensory substitution, for artificial sensibility at the hands using hearing as a replacement for loss of sensation [<span class="CitationRef"><a epub:type="biblioref" href="#CR35" role="doc-biblioref">35</a></span>]. In this particular study, microphones placed at the fingertips captured and amplified the friction sound obtained when rubbing hard surfaces.</p><p class="Para" id="Par30">In [<span class="CitationRef"><a epub:type="biblioref" href="#CR28" role="doc-biblioref">28</a></span>], a nice investigation on the interaction between auditory and haptic cues in the near space is presented. The authors show an interesting illusion of how sounds delivered through headphones, presented near to the head induces an haptic experience. The left ear of a dummy head was stroked with a paintbrush and the sound was recorded. The sound was then presented to the participants who felt a tickling sensation when the sound was presented near to the head, but not when it was presented distant from the head.</p><p class="Para" id="Par31">Another kind of dynamic sonic objecthood is that obtained through data physicalization, which is the 3D rendering of a dataset in the form of a solid physical object. Although there is a long history of physicalization, this area of research has become increasingly interesting through the facilitation of 3D printing technology. Physicalizations allow the user to hold and manipulate a dataset in their hands, providing an embodied experience that allows rich naturalistic and intuitive interactions such as multi-finger touch, tapping, pressing, squeezing, scraping, and rotating [<span class="CitationRef"><a epub:type="biblioref" href="#CR36" role="doc-biblioref">36</a></span>].</p><p class="Para" id="Par32">Physical manipulation produces acoustic effects that are influenced by the material properties, shape, forces, modes of interaction and events over time. The idea that sound could be a way to augment data physicalization has been explored through acoustic sonifications in which the 3D printed dataset is super-imposed on the form of a sounding object, such as a bell or a singing bowl [<span class="CitationRef"><a epub:type="biblioref" href="#CR1" role="doc-biblioref">1</a></span>]. Since acoustic vibrations are strongly influenced by 3D form, the sound that is produced is influenced by the dataset that is used to shape the sounding object. On a similar vein, the design of musical instruments has also inspired the design of new interfaces for human-computer interaction. As stated by Jaron Lanier, musical instruments are the best user interfaces (see [<span class="CitationRef"><a epub:type="biblioref" href="#CR1" role="doc-biblioref">1</a></span>]), and we can learn to design new interfaces by looking at musical <span id="ITerm30">instruments</span>. An example is the work of [<span class="CitationRef"><a epub:type="biblioref" href="#CR32" role="doc-biblioref">32</a></span>], where structural elements along the speaker-microphone pathway characteristically alter the acoustic output. Moreover, Chap. <span class="ExternalRef"><a href="478239_1_En_12_Chapter.xhtml"><span class="RefSource">12</span></a></span> proposes several case studies in the context of musical haptics.</p><p class="Para" id="Par33">In designing multimodal environments, several elements need to be taken into consideration. However, technology imposes some limitations, especially when the ultimate goal is to simulate systems that react in realtime. This issue is nicely addressed by Pai, who describes a tradeoff between accuracy and <span id="ITerm31">responsiveness</span>, a crucial difference between models for science and models for interaction (see [<span class="CitationRef"><a epub:type="biblioref" href="#CR44" role="doc-biblioref">44</a></span>]). Specifically, computations about the physical world are always approximations. In general, it is possible to improve accuracy by constructing more detailed models and performing more precise measurements, but this increased accuracy comes at the cost of latency, i.e., the elapsed time before an answer is obtained. For multisensory models, it is also essential to ensure synchronization of time between different sensory modalities [<span class="CitationRef"><a epub:type="biblioref" href="#CR44" role="doc-biblioref">44</a></span>]. groups all of these temporal considerations, such as latency and synchronization, into a single category called responsiveness. The question then becomes how to balance accuracy and responsiveness. The choice between accuracy and responsiveness depends also on the final goal of the multimodal system design. Often, scientists are more concerned with accuracy, so responsiveness is only a soft constraint based on available resources. On the other hand, for interaction designers, responsiveness is an essential parameter that must be satisfied.</p></section><section class="Section1 RenderAsSection1" id="Sec5"><h2 class="Heading"><span class="HeadingNumber">10.5 </span>Conclusions</h2><p class="Para" id="Par34">This chapter has provided an overview of several experiments whose goals were to achieve a better understanding of how the human auditory system is connected to visual and haptic channels. A better understanding of multimodal perception can have several applications. As an example, systems based on sensory substitution help people lacking a certain sensorial modality by replacing it with another sensorial modality. Moreover, cross-modal enhancement allows reduced stimuli in one sensorial modality to be augmented by a stronger stimulation in another modality.</p><p class="Para" id="Par35">Contemporary advances in hardware and software technology allow us to experiment in several ways with technologies for multimodal interaction design, building for example, haptic illusions with equipment available in a typical hardware store [<span class="CitationRef"><a epub:type="biblioref" href="#CR21" role="doc-biblioref">21</a></span>] or easily experimenting with sketching and rapid prototyping [<span class="CitationRef"><a epub:type="biblioref" href="#CR6" role="doc-biblioref">6</a></span>, <span class="CitationRef"><a epub:type="biblioref" href="#CR11" role="doc-biblioref">11</a></span>]. These advances in technology create several possibilities for discovering novel cross-modal illusions and interactions between the senses, especially when a collaboration between cognitive psychologists and interaction designers is facilitated. A research challenge is not only to understand how humans process information coming from different senses, but also how information in a multimodal system should be distributed to different modalities in order to obtain the best user experience.</p><p class="Para" id="Par36">As an example, in a multi-modal system such as a system for controlling an haptic display, seeing a visual display and listening to interactive auditory display, it is important to determine which synchronicities are more important. At one extreme, a completely disjointed distribution of information over several modalities can offer the highest bandwidth, but the user may be confused in connecting the modalities and one modality might mask another and distract the user by focusing attention on events that might not be important. At the other extreme, a completely redundant distribution of information is known to increase the cognitive load and is not guaranteed to increase user performance.</p><p class="Para" id="Par37">Beyond the research on multimodal stimuli processing, studies are needed on the processing of multimodal stimuli that are connected via interaction. We would expect that the human brain and sensory system have been optimized to cope with a certain mixture of redundant information, and that information displays are better the more they follow this natural distribution. Overall, the more we achieve a better understanding of the ways humans interact with the everyday world, the more we can obtain inspiration for the design of effective natural multimodal interfaces.</p></section><div class="License LicenseSubType-cc-by"><a href="https://creativecommons.org/licenses/by/4.0"><img alt="Creative Commons" src="../css/cc-by.png"/></a><p class="SimplePara"><strong class="EmphasisTypeBold ">Open Access</strong> This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (<span class="ExternalRef"><a href="http://creativecommons.org/licenses/by/4.0/"><span class="RefSource">http://​creativecommons.​org/​licenses/​by/​4.​0/​</span></a></span>), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p><p class="SimplePara">The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p></div><aside aria-labelledby="Bib1Heading" class="Bibliography" id="Bib1"><div epub:type="bibliography" role="doc-bibliography"><div class="Heading" id="Bib1Heading">References</div><ol class="BibliographyWrapper"><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">1.</div><div class="CitationContent" id="CR1">Barrass, S.: Diagnosing blood pressure with Acoustic Sonification singing bowls. International Journal of Human-Computer Studies <strong class="EmphasisTypeBold ">85</strong>, 68–71 (2016).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">2.</div><div class="CitationContent" id="CR2">Biocca, F., Kim, J., Choi, Y.: Visual touch in virtual environments: An exploratory study of presence, multimodal interfaces, and cross-modal sensory illusions. Presence: Teleoperators &amp; Virtual Environments <strong class="EmphasisTypeBold ">10</strong>, 247–265 (2001).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">3.</div><div class="CitationContent" id="CR3">Bregman, A. S.: Auditory scene analysis: The perceptual organization of sound (MIT press, 1994).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">4.</div><div class="CitationContent" id="CR4">Bruce, V., Green, P. R., Georgeson, M. A.: Visual perception: Physiology, psychology, &amp; ecology (Psychology Press, 2003).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">5.</div><div class="CitationContent" id="CR5">Burr, D., Banks, M. S., Morrone, M. C.: Auditory dominance over vision in the perception of interval duration. Experimental Brain Research <strong class="EmphasisTypeBold ">198</strong>, 49 (2009).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">6.</div><div class="CitationContent" id="CR6">Buxton, B.: Sketching user experiences: getting the design right and the right design (Morgan kaufmann, 2010).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">7.</div><div class="CitationContent" id="CR7">Cherry, E. C.: Some experiments on the recognition of speech, with one and with two ears. The Journal of the acoustical society of America <strong class="EmphasisTypeBold ">25</strong>, 975–979 (1953).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">8.</div><div class="CitationContent" id="CR8">Colavita, F. B.: Human sensory dominance. Perception &amp; Psychophysics <strong class="EmphasisTypeBold ">16</strong>, 409–412 (1974).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">9.</div><div class="CitationContent" id="CR9">Connell, B. R.: The principles of universal design, version 2.0. <span class="ExternalRef"><a href="http://www.design.ncsu.edu/cud/univ/design/princ/overview.htm"><span class="RefSource">http://​www.​design.​ncsu.​edu/​cud/​univ/​design/​princ/​overview.​htm</span></a></span> (1997).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">10.</div><div class="CitationContent" id="CR10">Damasio, A. R.: Descartes’ error (Random House, 2006).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">11.</div><div class="CitationContent" id="CR11">Delle Monache, S., Polotti, P., Rocchesso, D.: A toolkit for explorations in sonic interaction design in Proc. Int. Conf. Audio Mostly (AM2010) (Piteå, 2010), 1–7.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">12.</div><div class="CitationContent" id="CR12">Delle Monache, S. et al.: Embodied sound design. International Journal of Human-Computer Studies <strong class="EmphasisTypeBold ">118</strong>, 47–59 (2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">13.</div><div class="CitationContent" id="CR13">DiFilippo, D., Pai, D. K.: The AHI: An audio and haptic interface for contact interactions in Proceedings of the 13th annual ACM symposium on User interface software and technology (2000), 149–158.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">14.</div><div class="CitationContent" id="CR14">Dourish, P.: Where the action is: the foundations of embodied interaction (MIT press, 2004).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">15.</div><div class="CitationContent" id="CR15">Ecker, A. J., Heller, L. M.: Auditory Visual Interactions in the Perception of a Ball’s Path. Perception <strong class="EmphasisTypeBold ">34</strong>, 59–75 (2005).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">16.</div><div class="CitationContent" id="CR16">Ernst, M. O., Bülthoff, H. H.: Merging the senses into a robust percept. Trends in cognitive sciences <strong class="EmphasisTypeBold ">8</strong>, 162–169 (2004).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">17.</div><div class="CitationContent" id="CR17">Flowers, J. H., Buhman, D. C., Turnage, K. D.: Data sonification from the desktop: Should sound be part of standard data analysis software? ACM Transactions on Applied Perception (TAP) <strong class="EmphasisTypeBold ">2</strong>, 467–472 (2005).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">18.</div><div class="CitationContent" id="CR18">Gaver,W.: What in the world do we hear?: An ecological approach to auditory event perception. Ecological Psychology <strong class="EmphasisTypeBold ">5</strong>, 1–29 (1993).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">19.</div><div class="CitationContent" id="CR19">Geldard, F. A., Sherrick, C. E.: The cutaneous" rabbit": A perceptual illusion. Science <strong class="EmphasisTypeBold ">178</strong>, 178–179 (1972).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">20.</div><div class="CitationContent" id="CR20">Guest, S., Catmur, C., Lloyd, D., Spence, C.: Audiotactile interactions in roughness perception. Experimental Brain Research <strong class="EmphasisTypeBold ">146</strong>, 161–171 (2002).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">21.</div><div class="CitationContent" id="CR21">Hayward, V.: A brief taxonomy of tactile illusions and demonstrations that can be done in a hardware store. Brain research bulletin <strong class="EmphasisTypeBold ">75</strong>, 742–752 (2008).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">22.</div><div class="CitationContent" id="CR22">Hecht, D., Reiner, M.: Sensory dominance in combinations of audio, visual and haptic stimuli. Experimental brain research <strong class="EmphasisTypeBold ">193</strong>, 307–314 (2009).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">23.</div><div class="CitationContent" id="CR23">Höök, K.: Designing with the body: Somaesthetic interaction design (MIT Press, 2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">24.</div><div class="CitationContent" id="CR24">Jack, C. E., Thurlow,W. R.: Effects of degree of visual association and angle of displacement on the ?ventriloquism? effect. Perceptual and motor skills <strong class="EmphasisTypeBold ">37</strong>, 967–979 (1973).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">25.</div><div class="CitationContent" id="CR25">Jousmäki, V., Hari, R.: Parchment-skin illusion: sound-biased touch. Current biology <strong class="EmphasisTypeBold ">8</strong>, R190–R191 (1998).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">26.</div><div class="CitationContent" id="CR26">Kamitani, Y., Shimojo, S.: Sound-induced visual ?rabbit? Journal of vision <strong class="EmphasisTypeBold ">1</strong>, 478–478 (2001).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">27.</div><div class="CitationContent" id="CR27">Kersten, D., Mamassian, P., Knill, D. C.: Moving cast shadows induce apparent motion in depth. Perception <strong class="EmphasisTypeBold ">26</strong>, 171–192 (1997).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">28.</div><div class="CitationContent" id="CR28">Kitagawa, N., Zampini, M., Spence, C.: Audiotactile interactions in near and far space. Experimental Brain Research <strong class="EmphasisTypeBold ">166</strong>, 528–537 (2005).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">29.</div><div class="CitationContent" id="CR29">Kohlrausch, A., van de Par, S.: Auditory-visual interaction: from fundamental research in cognitive psychology to (possible) applications in Human Vision and Electronic Imaging IV <strong class="EmphasisTypeBold ">3644</strong> (1999), 34–44.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">30.</div><div class="CitationContent" id="CR30">Krumbholz, K., Patterson, R., Seither-Preisler, A., Lammertmann, C., Lütkenhöner, B.: Neuromagnetic evidence for a pitch processing center in Heschl?s gyrus. Cerebral Cortex <strong class="EmphasisTypeBold ">13</strong>, 765–772 (2003).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">31.</div><div class="CitationContent" id="CR31">Kubovy, M., Van Valkenburg, D.: Auditory and visual objects. Cognition <strong class="EmphasisTypeBold ">80</strong>, 97–126 (2001).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">32.</div><div class="CitationContent" id="CR32">Laput, G., Brockmeyer, E., Hudson, S. E., Harrison, C.: Acoustruments: Passive, acoustically-driven, interactive controls for handheld devices in Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (2015), 2161–2170.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">33.</div><div class="CitationContent" id="CR33">Li, T., Wang, D., Peng, C., Yu, C., Zhang, Y.: Speed-accuracy tradeoff of fingertip force control with visual/audio/haptic feedback. International Journal of Human-Computer Studies <strong class="EmphasisTypeBold ">110</strong>, 33–44 (2018).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">34.</div><div class="CitationContent" id="CR34">Lipscomb, S. D., Kendall, R. A.: Perceptual judgement of the relationship between musical and visual components in film. Psychomusicology: A Journal of Research in Music Cognition <strong class="EmphasisTypeBold ">13</strong>, 60 (1994).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">35.</div><div class="CitationContent" id="CR35">Lundborg, G., Rosén, B., Lindberg, S.: Hearing as substitution for sensation: a new principle for artificial sensibility. The Journal of hand surgery <strong class="EmphasisTypeBold ">24</strong>, 219–224 (1999).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">36.</div><div class="CitationContent" id="CR36">Lupton, D.: Feeling your data: Touch and making sense of personal digital data. New Media &amp; Society <strong class="EmphasisTypeBold ">19</strong>, 1599–1614 (2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">37.</div><div class="CitationContent" id="CR37">Mcguire, J. M., Scott, S. S., Shaw, S. F.: Universal design and its applications in educational environments. Remedial and special education <strong class="EmphasisTypeBold ">27</strong>, 166–175 (2006).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">38.</div><div class="CitationContent" id="CR38">McGurk, H., MacDonald, J.: Hearing lips and seeing voices. Nature <strong class="EmphasisTypeBold ">264</strong>, 746 (1976).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">39.</div><div class="CitationContent" id="CR39">Merleau-Ponty, M.: Phenomenology of perception Routledge. UK.[France, 1945] (1962).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">40.</div><div class="CitationContent" id="CR40">Mynatt, E. D., Edwards, W. K.: Mapping GUIs to auditory interfaces in Proceedings of the 5th annual ACM symposium on User interface software and technology (1992), 61–70.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">41.</div><div class="CitationContent" id="CR41">Niedenthal, P. M., Barsalou, L. W., Winkielman, P., Krauth-Gruber, S., Ric, F.: Embodiment in attitudes, social perception, and emotion. Personality and social psychology review <strong class="EmphasisTypeBold ">9</strong>, 184–211 (2005).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">42.</div><div class="CitationContent" id="CR42">Nordahl, R. et al.: Preliminary experiment combining virtual reality haptic shoes and audio synthesis in International Conference on Human Haptic Sensing and Touch Enabled Computer Applications (2010), 123–129.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">43.</div><div class="CitationContent" id="CR43">Ortega, L., Guzman-Martinez, E., Grabowecky, M., Suzuki, S.: Auditory dominance in time perception. Journal of Vision <strong class="EmphasisTypeBold ">9</strong>, 1086–1086 (2009).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">44.</div><div class="CitationContent" id="CR44">Pai, D. K.: Multisensory interaction: Real and virtual in Robotics Research. The Eleventh International Symposium (2005), 489–498.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">45.</div><div class="CitationContent" id="CR45">Posner, M. I., Nissen, M. J., Klein, R. M.: Visual dominance: an informationprocessing account of its origins and significance. Psychological review <strong class="EmphasisTypeBold ">83</strong>, 157 (1976).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">46.</div><div class="CitationContent" id="CR46">Recarte, M. A., Nunes, L. M.: Mental workload while driving: effects on visual search, discrimination, and decision making. Journal of experimental psychology: Applied <strong class="EmphasisTypeBold ">9</strong>, 119 (2003).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">47.</div><div class="CitationContent" id="CR47">Repp, B. H., Penel, A.: Auditory dominance in temporal processing: new evidence from synchronization with simultaneous visual and auditory sequences. Journal of Experimental Psychology: Human Perception and Performance <strong class="EmphasisTypeBold ">28</strong>, 1085 (2002).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">48.</div><div class="CitationContent" id="CR48">Schmid, C., Büchel, C., Rose, M.: The neural basis of visual dominance in the context of audio-visual object processing. NeuroImage <strong class="EmphasisTypeBold ">55</strong>, 304–311 (2011).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">49.</div><div class="CitationContent" id="CR49">Schutz, M., Lipscomb, S.: Hearing gestures, seeing music: Vision influences perceived tone duration. Perception <strong class="EmphasisTypeBold ">36</strong>, 888–897 (2007).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">50.</div><div class="CitationContent" id="CR50">Sekuler, A. B., Sekuler, R.: Collisions between moving visual targets: what controls alternative ways of seeing an ambiguous display? Perception <strong class="EmphasisTypeBold ">28</strong>, 415–432 (1999).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">51.</div><div class="CitationContent" id="CR51">Sekuler, R.: Sound alters visual motion perception. Nature <strong class="EmphasisTypeBold ">385</strong>, 308 (1997).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">52.</div><div class="CitationContent" id="CR52">Senna, I., Maravita, A., Bolognini,N., Parise, C.V.: The marble-hand illusion. PloS one <strong class="EmphasisTypeBold ">9</strong> (2014).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">53.</div><div class="CitationContent" id="CR53">Shams, L., Kamitani, Y., Shimojo, S.: Illusions: What you see is what you hear. Nature <strong class="EmphasisTypeBold ">408</strong>, 788 (2000).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">54.</div><div class="CitationContent" id="CR54">Shams, L., Kamitani, Y., Shimojo, S.: Visual illusion induced by sound. Cognitive Brain Research <strong class="EmphasisTypeBold ">14</strong>, 147–152 (2002).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">55.</div><div class="CitationContent" id="CR55">Spence, C., Driver, J.: Audiovisual links in exogenous covert spatial orienting. Perception &amp; psychophysics <strong class="EmphasisTypeBold ">59</strong>, 1–22 (1997).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">56.</div><div class="CitationContent" id="CR56">Spence, C., Zampini, M.: Auditory contributions to multisensory product perception. Acta Acustica united with Acustica <strong class="EmphasisTypeBold ">92</strong>, 1009–1025 (2006).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">57.</div><div class="CitationContent" id="CR57">Storms, R. L., Zyda, M. J.: Interactions in perceived quality of auditoryvisual displays. Presence: Teleoperators &amp; Virtual Environments <strong class="EmphasisTypeBold ">9</strong>, 557–580 (2000).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">58.</div><div class="CitationContent" id="CR58">Tajadura-Jiménez, A. et al.: As light as your footsteps: altering walking sounds to change perceived body weight, emotional state and gait in Proc. ACM Conf. on Human Factors in Computing Systems (Seoul, Apr. 2015), 2943–2952.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">59.</div><div class="CitationContent" id="CR59">Tholander, J., Karlgren, K., Ramberg, R., Sökjer, P.: Where all the interaction is: sketching in interaction design as an embodied practice in Proceedings of the 7th ACM conference on Designing interactive systems (2008), 445–454.</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">60.</div><div class="CitationContent" id="CR60">Thomas, J. P., Shiffrar, M.: I can see you better if I can hear you coming: Action-consistent sounds facilitate the visual detection of human gait. Journal of vision <strong class="EmphasisTypeBold ">10</strong>, 14–14 (2010).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">61.</div><div class="CitationContent" id="CR61">Van den Doel, K., Pai, D. K.: The sounds of physical shapes. Presence <strong class="EmphasisTypeBold ">7</strong>, 382–395 (1998).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">62.</div><div class="CitationContent" id="CR62">Van der Burg, E., Olivers, C. N., Bronkhorst, A. W., Theeuwes, J.: Pip and pop: nonspatial auditory signals improve spatial visual search. Journal of Experimental Psychology: Human Perception and Performance <strong class="EmphasisTypeBold ">34</strong>, 1053 (2008).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">63.</div><div class="CitationContent" id="CR63">Vi, C. T., Ablart, D., Gatti, E., Velasco, C., Obrist, M.: Not just seeing, but also feeling art: Mid-air haptic experiences integrated in a multisensory art exhibition. International Journal of Human-Computer Studies <strong class="EmphasisTypeBold ">108</strong>, 1–14 (2017).</div></li><li class="Citation" epub:type="biblioentry" role="doc-biblioentry"><div class="CitationNumber">64.</div><div class="CitationContent" id="CR64">Welch, R. B., Warren, D. H.: Immediate perceptual response to intersensory discrepancy. Psychological bulletin <strong class="EmphasisTypeBold ">88</strong>, 638 (1980).</div></li></ol></div></aside></div></div></body></html>